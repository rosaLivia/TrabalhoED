
[Página 1]
LLM-Based Misconfiguration Detection for AWS Serverless Computing JINFENG WEN, Beijing University of Posts and Telecommunications, China ZHENPENG CHEN, Nanyang Technological University, Singapore FEDERICA SARRO, University College London, United Kingdom ZIXI ZHU, Beijing University of Posts and Telecommunications, China YI LIU, Advanced Institute of Big Data, China HAODI PING, Beijing University of Technology, China SHANGGUANG WANG, Beijing University of Posts and Telecommunications, China Serverless computing is an emerging cloud computing paradigm that enables developers to build applications at the function level, known as serverless applications. Amazon Web Services (AWS), the leading provider in this domain, provides the Serverless Application Model (AWS SAM), the most widely adopted configuration schema for configuring and managing serverless applications through a specified file. However, misconfigurations pose a significant challenge in serverless development. Traditional data-driven techniques, which learn configuration patterns from historical data to identify anomalies, may struggle with serverless applications because the complexity of serverless configurations hinders pattern recognition, and it is challenging to gather complete datasets that cover all possible configurations. Recent advancements in Large Language Models (LLMs) have shown promise in tackling various software engineering tasks. Leveraging vast amounts of publicly available data during pre-training, LLMs can have the potential to assist in identifying and explaining misconfigurations in serverless applications. In this paper, we introduce SlsDetector , the first framework leveraging LLMs to detect misconfigurations in serverless applications. SlsDetector utilizes effective prompt engineering with zero-shot learning to identify configuration issues. It designs multi-dimensional constraints specifically tailored to the configuration charac- teristics of serverless applications and leverages the Chain of Thought technique to enhance LLMs inferences, alongside generating customized structured responses. We evaluate SlsDetector on a curated dataset of 110 configuration files, which includes correct configurations, real-world misconfigurations, and intentionally injected errors. Our results show that SlsDetector , based on ChatGPT-4o (one of the most representative LLMs), achieves a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%, outperforming state-of-the-art data-driven approaches by 53.82, 17.40, and 49.72 percentage points, respectively. Furthermore, we investigate the generalization capability of SlsDetector by applying recent LLMs, including Llama 3.1 (405B) Instruct Turbo and Gemini 1.5 Pro, with results showing consistently high effectiveness across these models. CCS Concepts: •Software and its engineering →Software configuration management and version control systems ;•Computer systems organization →Cloud computing . Authors’ Contact Information: Jinfeng Wen, Beijing University of Posts and Telecommunications, Beijing, China, jinfeng. wen@bupt.edu.cn; Zhenpeng Chen, zhenpeng.chen@ntu.edu.sg, Nanyang Technological University, Singapore, Singapore; Federica Sarro, f.sarro@ucl.ac.uk, University College London, London, United Kingdom; Zixi Zhu, zhuzx816@163.com, Beijing University of Posts and Telecommunications, Beijing, China; Yi Liu, liuyi14@pku.edu.cn, Advanced Institute of Big Data, Beijing, China; Haodi Ping, haodi.ping@bjut.edu.cn, Beijing University of Technology, Beijing, China; Shangguang Wang, sgwang@bupt.edu.cn, Beijing University of Posts and Telecommunications, Beijing, China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM XXXX-XXXX/2024/11-ART https://doi.org/XXXXXXX.XXXXXXX , Vol. 1, No. 1, Article . Publication date: November 2024.arXiv:2411.00642v1 [cs.SE] 1 Nov 2024

[Página 2]
2 Jinfeng Wen et al. ACM Reference Format: Jinfeng Wen, Zhenpeng Chen, Federica Sarro, Zixi Zhu, Yi Liu, Haodi Ping, and Shangguang Wang. 2024. LLM-Based Misconfiguration Detection for AWS Serverless Computing. 1, 1 (November 2024), 21 pages. https://doi.org/XXXXXXX.XXXXXXX 1 INTRODUCTION Serverless computing is an emerging cloud computing paradigm that allows developers to build and run applications, known as serverless applications , without managing underlying infrastructure tasks [ 50]. It has been widely adopted across diverse application domains [ 18,45,58], attracting growing interest from research communities, such as Software Engineering (SE) [ 50] and Sys- tems [ 40], and from industry. To support the development and execution of serverless applications, leading cloud providers have introduced serverless platforms. Among these providers, Amazon Web Services (AWS) stands out as the leader in serverless computing [32, 50, 51]. Serverless computing supports two primary service models: Function-as-a-Service (FaaS) and Backend-as-a-Service (BaaS) [ 25,30]. FaaS allows developers to build applications as small, event- driven functions (i.e., serverless functions), while BaaS provides ready-to-use cloud services such as storage (e.g., AWS S3 [ 2]), database, and API gateway management. This collaboration between FaaS and BaaS enables developers to efficiently create serverless applications. To configure and manage functions and required cloud resources for serverless applications, AWS provides the Serverless Application Model (AWS SAM) [ 5], the most widely adopted configuration schema in the serverless computing practice [ 1,7,15]. It can streamline the development process and reduce complexities associated with resource management in serverless applications. However, misconfigurations have emerged as a major challenge in serverless application de- velopment [ 38,46,51]. Misconfigurations in serverless computing can lead to significant security vulnerabilities and operational issues. For example, as reported [ 16], a coronavirus testing company exposed over 50,000 patients’ scanned IDs and thousands of COVID-19 test results due to a miscon- figuration in an AWS S3 bucket, used in conjunction with serverless applications. Similarly, another company experienced a data breach affecting 4.9 million customers due to API misconfigurations within serverless environments [ 12]. These examples underscore that misconfigurations are not isolated incidents but represent systemic issues that pose significant risks to serverless applications, suggesting the urgent need for effective misconfiguration detection for serverless computing. Misconfigurations have become one of the major causes of system software failures [ 55]. Despite the promise of existing data-driven methods for misconfiguration detection in other scenarios [ 43, 44,61,62], they have low effectiveness to serverless computing. Data-driven approaches, which rely on anomaly detection or pattern recognition based on training data, suffer from limitations such as incomplete or incorrect datasets [ 61,62]. Additional strategies that incorporate extensive knowledge, such as predefined templates and official documentation, lack flexibility and adaptability. These problems make the data-driven approach not enough to detect configuration problems of serverless applications. Moreover, serverless application configurations involve intricate structures, including domain-specific languages, complex dependency relationships, and nested objects across over 800 cloud resource types, which further complicates their detection. Recent advancements in Large Language Models (LLMs) offer a promising new approach to this problem. LLMs have demonstrated significant success in various SE tasks, such as code summa- rization [ 17], program repair [ 27], unit test generation [ 60], and log parsing [ 53]. Trained on vast amounts of publicly available data, LLMs can potentially understand and recognize configuration patterns, best practices, and common pitfalls in serverless application configurations. This makes LLMs well-suited for detecting potential misconfigurations in serverless applications. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 3]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 3 In this paper, we introduce SlsDetector , the first LLM-based framework specifically designed to detect misconfigurations in serverless applications. By leveraging advanced prompt engineering in conjunction with zero-shot learning, which requires no prior examples, SlsDetector efficiently identifies configuration problems with minimal effort. This marks a significant advancement in detecting misconfigurations within serverless environments. SlsDetector accepts a configuration file of the serverless application as input and outputs detected misconfigurations along with detailed explanations for each issue. To achieve this, SlsDetector features a prompt generation component that dynamically integrates the configuration file, task description, multi-dimensional constraints, and customized responses. Multi-dimensional constraints are tailored to the specific characteristics of serverless applications, taking into account resource types, configuration entries, values, and different levels of dependencies. This context-aware design provides targeted guidance for detection. Furthermore, SlsDetector employs the Chain of Thought technique [ 10,22], a reasoning strategy that enhances the problem-solving process, into these constraints. The customized response provides the content demand and format demand of LLM outputs, ensuring that responses are not only structured but also actionable answers aligned with detailed explanations. To evaluate SlsDetector , we curate a dataset of 110 configuration files, including 26 correctly configured files, 58 with real-world misconfigurations, and 26 with injected errors. Our results show that SlsDetector , based on ChatGPT-4o (one of the most representative LLMs known for outstanding performance), achieves a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%. It outperforms the state-of-the-art data-driven approach by 53.82, 17.40, and 49.72 percentage points in precision, recall, and F1-score, respectively. We further explore the generalization capability of SlsDetector on other representative LLMs, including Llama 3.1 (405B) Instruct Turbo and Gemini 1.5 Pro. The results show that SlsDetector consistently achieves high effectiveness across these models. In summary, this paper makes the following contributions: •We present SlsDetector , the first LLM-based approach specifically designed for detecting miscon- figurations in serverless computing. •We conduct an empirical study using our benchmark dataset to evaluate the effectiveness of our misconfiguration detection approach, demonstrating that it outperforms baseline methods. 2 BACKGROUND 2.1 Serverless Computing Applications developed within the serverless computing paradigm are referred to as serverless applications. These applications are built around event-driven serverless functions, which represent the core business logic. They are complemented by associated cloud services that facilitate the integration of backend functionalities. This combination streamlines the development process [ 25, 50]. During the development and deployment of serverless applications, developers define essential execution settings. These settings include the runtime environment, memory allocation, timeout duration, predefined event triggers, and required cloud resources for the serverless applications. 2.2 Serverless Application Configurations: AWS SAM Developers leverage specified configuration files, such as YAML files, to define the execution settings of serverless applications. In serverless computing, serverless functions are inherently event-driven, meaning that the relationships between functions and predefined events are not explicitly detailed in the application code. Instead, these relationships are succinctly captured in the configuration file, which automates infrastructure provisioning. Thus, the configuration file plays a crucial role in the development process of serverless applications. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 4]
4 Jinfeng Wen et al. Among mainstream serverless platforms, AWS Lambda employs a widely used configuration schema [ 1,15] known as the AWS Serverless Application Model (AWS SAM) [ 5]. AWS SAM enables developers to easily reuse proven configurations, streamlining the development and deployment of serverless applications. In contrast, other platforms, such as Google Cloud Functions [ 14] and Microsoft Azure Functions [ 9], lack a formal configuration schema. They rely on command-line interfaces or platform consoles to manually manage key settings and required resources. This manual way lacks standardization and the availability of configuration datasets for analysis. Given AWS Lambda’s widespread use and the advantages offered by AWS SAM’s configuration schema, our paper focuses on analyzing the configurations of serverless applications built using AWS SAM. AWS SAM uses a YAML-based configuration file format with specialized template specifications. It builds upon and extends AWS CloudFormation [ 3], which is primarily used for provisioning and configuring non-serverless cloud resources. AWS SAM introduces a syntax specifically designed for defining and managing both serverless infrastructure (spanning nine categories [ 6]) and non- serverless infrastructure (covering over 800 categories [4]). Serverless application configurations are complex and exhibit unique characteristics. Unlike the simple “flat” key-value pair format commonly seen in prior configuration studies [ 20,43,46,49,54], serverless application configurations feature intricate structures, including objects, lists, maps, and nested elements. Each cloud resource type is represented by custom-named objects, which contain specific configuration entries and their corresponding values. These values can be strings, lists, maps, or even nested objects representing other cloud resources. Additionally, serverless configurations introduce resource types specific to serverless environments (e.g., “AWS::Serverless::Function”) and attributes unique to serverless applications (e.g., Handler ,MemorySize ,Timeout ). This exhibits that AWS SAM YAML files function as domain-specific languages within the serverless computing domain, increasing the complexity of configurations. 2.3 An Example of the Configuration File We provide a real-world configuration file example [ 13] from GitHub, a widely used platform for studying developer issues [ 33,34], as shown in Fig. 1. In this example, the developer created a serverless function that responds to events from the AWS S3 storage service [ 2]. However, this configuration failed during deployment. Resolving this issue required nearly 20 rounds of communication involving 26 people and spanned almost five years before a correct solution was found. The root cause was the unsupported Condition entry mistakenly added on line 24. This example underscores the critical need for an effective approach to detect misconfigurations in serverless applications early. Such an approach would quickly pinpoint potential issues, reducing the time, effort, and communication overhead required to troubleshoot and resolve misconfigurations. We explain this configuration file example. The content is mainly structured into two sections: Transform (line 2) and Resources (lines 15-39). The Transform section identifies the file as an AWS SAM template with the value “AWS::Serverless-2016-10-31.” The Resources section defines the required execution settings through resource types . The “AWS::Serverless::Function” resource type (named “BucketEventConsumer”) aims to configure a serverless function, while “AWS::S3::Bucket” (named “SomeBucket”) represents an AWS S3 bucket, a non-serverless resource that frequently interacts with serverless functions. The “BucketEventConsumer” object includes configuration entries such as the handler function (line 19), runtime environment (line 20), code location (line 21), and a predefined event (lines 22-33). These entries are allocated values that conform to the constraints. For example, Runtime is set to “python3.6” (line 20). In this example, the event source is set to S3 (line 25) using a nested object. The function is triggered when an S3 object is created (lines 27-28) that meets the filter rule specified as key-value pairs (lines 31-33). Specifically, the , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 5]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 5 1 AWSTemplateFormatVersion : '2010 −09−09' 2 Transform : AWS : : S e r v e r l e s s −2016 −10−31 3 D e s c r i p t i o n : Lambda t h a t r e s p o n d s t o S3 e v e n t s 4 P a r a m e t e r s : 5 P r e E x i s t i n g B u c k e t : 6 D e s c r i p t i o n : " Does an e x i s t i n g b u ck e t e x i s t ( not managed by c l o u d f o r m a t i o n ) " 7 Type : S t r i n g 8 D e f a u l t : 'no' 9 AllowedValues : 10 − 'yes' 11 − 'no' 12 C o n s t r a i n t D e s c r i p t i o n : must s p e c i f y yes or no . 13 C o n d i t i o n s : 14 NeedsSomeBucket : ! E q u a l s [ ! Ref P r e E x i s t i n g B u c k e t , 'no'] 15 R e s o u r c e s : 16 BucketEventConsumer : 17 Type : AWS : : S e r v e r l e s s : : F u n c t i o n 18 P r o p e r t i e s : 19 Handler : BucketEventConsumer . main . lambda_handler 20 Runtime : python3 . 6 21 CodeUri : bundle . z i p 22 Events : 23 CreateMetaEvent : 24 # C o n d i t i o n : NeedsSomeBucket 25 Type : S3 26 P r o p e r t i e s : 27 Bucket : ! Ref SomeBucket 28 Events : " s3 : O b j e c t C r e a t e d : ∗" 29 F i l t e r : 30 S3Key : 31 R u l e s : 32 −Name : s u f f i x 33 Value : meta . j s o n 34 SomeBucket : 35 C o n d i t i o n : NeedsSomeBucket 36 Type : AWS : : S3 : : Bucket 37 P r o p e r t i e s : 38 BucketName : 'some −bucket −somewhere ' 39 D e l e t i o n P o l i c y : R e t a i n Fig. 1. An configuration file example of serverless applications. function is invoked when a file in the “SomeBucket” S3 bucket ends with “meta.json” (lines 32-33). Name from line 32 and Value from line 33 need to appear together, indicating entry dependencies . Line 27 illustrates a relationship between Bucket value and the “AWS::S3::Bucket” resource in line 34, showing that the value dependencies of one configuration value depend on other values. In addition to the core sections, other parts of the configuration file are also important. The AWSTemplateFormatVersion section (line 1) specifies the template’s capabilities, with the current valid format version being “2010-09-09” [ 8]. The Description section (line 3) provides a textual description of the template. The Parameters section (lines 4-12) defines values that are passed to the template at runtime. The “PreExistingBucket” parameter accepts either “yes” or “no” as values. The Conditions section (lines 13-14) controls resource creation or property assignment based on the value of a parameter. The “NeedsSomeBucket” condition checks if the “PreExistingBucket” parameter is set to “no”. If true, the condition evaluates to true, otherwise, it evaluates to false. 2.4 LLMs for SE The application of LLMs to downstream tasks has become a significant area of research in SE [ 17,27, 53,60]. Recent studies [ 41,60] have demonstrated the potential of prompt engineering to achieve impressive performance across various tasks. Prompt engineering offers a flexible and resource- efficient way to utilize LLMs by adapting models to specific task requirements through carefully designed prompts. In contrast, the fine-tuning method involves updating the model’s parameters using specific downstream datasets. However, it often requires substantial computational resources and access to high-quality data, limiting its practicality in other contexts. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 6]
6 Jinfeng Wen et al. AConfigurationFiletobeDetectedTaskDescriptionMulti-dimensionalConstraints (CoT)CustomizedResponse LLMDetectedResults Resource TypeConstraintEntryConstraintValueConstraintEntryDependencyConstraintValueDependencyConstraint InputüResource Type ErrorsüEntry ErrorsüEntry Value ErrorsüEntry Dependency ErrorsüValue Dependency ErrorsOutput ContentDemandFormatDemand Prompt Generation Fig. 2. The overview of our approach SlsDetector . This paper aims to capitalize on the strengths of prompt engineering by developing specialized prompts to detect misconfigurations in serverless applications. This ensures effective detection while avoiding the computation and data demands associated with fine-tuning methods. 3 OUR APPROACH: SLSDETECTOR We present SlsDetector , an LLM-based framework designed to detect misconfigurations in serverless applications. SlsDetector takes a configuration file of the serverless application to be detected as input and outputs structured results, providing a list of detected misconfigurations along with detailed explanations for each issue. The framework is designed to be adaptable, supporting various LLMs. In the following sections, we provide an overview of SlsDetector and outline its component. 3.1 Overview Fig. 2 shows an overview of SlsDetector . It converts a misconfiguration detection request into a meticulously constructed prompt for LLMs. We employ zero-shot learning to minimize reliance on external sample configurations. This technique, which requires no prior examples, is a popular optimization technique [ 31,52,60]. While many studies [ 41,53,57] have utilized few-shot learning to improve effectiveness by learning from examples during inference, it relies heavily on the quality and selection of labeled samples. In contrast, zero-shot learning avoids the cost and effort associated with sample collection and curation, making it the preferred technique for our framework. InSlsDetector , we design a prompt generation component to construct a tailored prompt focused on the objective of detecting misconfiguration in serverless applications. This prompt is structured into four parts, where multi-dimensional constraints are core of SlsDetector and highly context- aware, shown in Fig. 2. Once the prompt is constructed, it is sent to the LLM, which generates the final output. Next, we introduce the prompt generation component in detail. 3.2 Prompt Generation We present the prompt content generated by the prompt generation component, which includes: (i) the configuration file to be analyzed, (ii) a task description for the LLMs, (iii) detailed multi- dimensional constraints, and (iv) a customized response. Fig. 3 illustrates our prompt structure. 3.2.1 Task Description. The task description includes the following elements: (i) a role-playing instruction designed to enhance the LLM’s ability to detect misconfigurations, which is a common prompt optimization technique [ 24,60]; and (ii) a task description instruction. In our scenario, the role is designed as “You are an expert at writing AWS SAM configurations for serverless applications”, while the task description asks, “Are there any misconfigurations in the above configuration file?”. These elements are carefully crafted to clearly outline the tasks the LLM needs to complete within the assigned role. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 7]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 7 nRole: You are an expert at writing AWS SAM configurations for serverless applications.nQuestion: Are there any misconfigurations in the above configuration file?ØPlease consider the following constraints in a category-by-categorymanner.ü[Resource Type Constraint]Configuration FileTaskDescriptionMulti-dimensionalConstraints1.CheckwhethertheresourcetypeiscurrentlysupportedbyAWSSAM,searchthefollowingURL1tocompareallsupportedAWSresourceslisted,notingthelettercase.2.Followthestepsbelowforastep-by-stepcheck.Step1:Checkwhethereachconfigurationentryundereachresourcetypeactuallyexists,payingattentiontotheaccuracyofthenameoftheconfigurationentry,includingcaseandsingularandpluralforms;Step2:IfEventsexists,alsofurthercheckwhetherthecorrespondingconfigurationentryexistsundereacheventsourcetype,andpleasepointoutthenon-existenceofconfigurationentries;Step3:Checkwhetherthehierarchicallevelofallconfigurationentriesiscorrect,andpayattentiontotheindentationproblem.3.Checkthatthevaluetype,constraints,andsupportedvaluesoftheconfigurationentryarecorrect,thatthevaluerepresentationisaccurate,andthatthevaluecannotbedefinedasnull.4.Checkiftherearedependenciesbetweenconfigurationentries,checkthattheyareusedinthecorrectway,e.g.Refandthatthereferencedresourcetypesarecorrect,andthattherelevantrequiredreferencedefinitionsaregiven.FurthercheckwhichconfigurationentriesareorarenotrequiredunderthePackageTypetype.5.Checkifthereisadependency(possiblyimplicit)betweenthevaluesofconfigurationentries,checkthattheusageiscorrectandthattherelevantrequiredreferencedefinitionsaregiven.ü[EntryConstraint]ü[ValueConstraint]ü[EntryDependencyConstraint]ü[ValueDependencyConstraint]Customized ResponsenPlease summarize the misconfigurations that are absolutely certain. They are categorized as [Resource Type Errors], [Configuration Entry Errors], [Configuration Entry Value Errors], [Entry Dependency Errors], [Value Dependency Errors] (if present).nAnswer format (You MUSTfollow this): Detected errors are written between<START> and <END> tags: Fig. 3. The prompt structure of SlsDetector . 3.2.2 Multi-dimensional Constraints. Multi-dimensional constraints are designed based on the configuration characteristics of serverless applications. As introduced in Section 2.3, the constituent elements of a configuration file are diverse and encompass the following aspects: •Resource Types : Serverless application configurations are primarily centered around defining resource types (e.g., lines 17 and 36 in Fig. 1). Resource types are core to establishing application execution settings. For instance, custom names such as “BucketEventConsumer” (line 16) are assigned to objects tied to specific resource types, such as “AWS::Serverless::Function”. Moreover, resource type names are case-sensitive. •Configuration Entries : Each resource type specifies diverse execution parameters, including lan- guage runtime and required resources for predefined events. These parameters are represented by configuration entries, such as Runtime on line 20 and Events on line 22 in Fig. 1. •Values of Configuration Entries : Each configuration entry is assigned specific values, often gov- erned by varied constraints. For example, the Runtime entry (line 20) has a set of allowed languages, e.g., “python3.6” and “nodejs16.x”, while the Bucket entry (line 27) accepts only referenced objects. •Entry Dependencies : Certain configuration entries depend on others. For example, Name from line 32 and Value from line 33 need to appear together in Fig. 1. These relationships are implicit and generally discovered by consulting documentation. •Value Dependencies : Some values of configuration entries are interdependent across different resource types. For instance, the RestApiId entry for API event triggers depends on the object name value corresponding to the “AWS::Serverless::Api” resource. This shows how values can be linked across different resource types, showing extensive value dependencies. Such dependencies are common in configurations due to the collaboration between FaaS and BaaS. Based on these configuration characteristics, we design five dimensions of constraints (i.e., multi-dimensional constraints) to enhance the LLM’s ability to identify serverless application misconfigurations: resource type constraint ,entry constraint ,value constraint ,entry de- pendency constraint , and value dependency constraint . Fig. 3 shows their details. Before explaining constraints, we introduce the Chain of Thought (CoT) technique [ 10,22,37]. CoT is a reasoning strategy to guide the problem-solving process toward more accurate and logical conclusions. This technique breaks down complex tasks into smaller, manageable steps. A CoT- based prompt includes several intermediate natural language reasoning steps that describe how to , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 8]
8 Jinfeng Wen et al. solve the task step by step. Based on the principle of this technique, we design our CoT strategy for detecting misconfigurations of serverless applications by guiding LLMs to consider constraints in a “category-by-category” manner. Forresource type constraint , we describe it as follows: “Check whether the resource type is currently supported by AWS SAM, search the following URL1to compare all supported AWS resources listed, noting the letter case.” By providing a direct link to the official documentation, we enable SlsDetector to effectively identify and compare resource type names, with a particular focus on case sensitivity, a critical aspect in AWS SAM configurations. Forentry constraint , we design a three-step validation process to ensure the correctness of configuration entries. The first step checks the correctness of each entry in relation to its corresponding resource type. The second step checks the correctness of event-related entries. The third step ensures that all configuration entries follow the correct hierarchical structure. SlsDetector applies these checks using the CoT technique, following a “step-by-step” process. The three steps are as follows. Step 1: SlsDetector checks that each configuration entry exists under its respective resource type. This includes checking the entry’s name for accuracy, paying particular attention to case sensitivity, and the use of singular or plural forms. Step 2: For event-related entries, SlsDetector checks that configuration entries corresponding to each event source type are present. If any non-existent entries are given, SlsDetector flags them for review. Step 3: SlsDetector checks the correct hierarchical structure of all configuration entries, with special attention to indentation. Misplaced or improperly indented entries may lead to errors, as they will not be recognized under the expected resource type. This three-step validation process allows SlsDetector to systematically detect errors, ensuring comprehensive and accurate checks for configuration entries. Forvalue constraint , we describe it as follows: “Check that the value type, constraints, and supported values of the configuration entry are correct, that the value representation is accurate, and that the value cannot be defined as null”. These constraints consider various aspects such as the correct data type, valid value ranges, and proper value formatting, ensuring that all values adhere to the required specifications. Forentry dependency constraint , we describe it as: “Check if there are dependencies between configuration entries, check that they are used in the correct way”. We also provide specific guidelines for validating dependencies, such as checking the accuracy of referenced resource types, ensuring required reference definitions are present, and confirming that required function entries are properly configured. Forvalue dependency constraint , we specify it as: “Check if there is a dependency (possibly implicit) between the values of configuration entries, check that the usage is correct and that the relevant required reference definitions are given”. This constraint ensures that value dependency checks are comprehensive across the configuration, helping to maintain consistency and correctness in how values interact and depend on each other within the configurations. 3.2.3 Customized Response. We customize the LLMs’ output by specifying both the content and format requirements for the responses, ensuring their effectiveness and relevance. For the content demand, we aim to avoid receiving vague or uncertain answers that fail to explicitly identify configuration errors. To achieve it, we instruct the model with the directive: “Please summarize the misconfigurations that are absolutely certain”. This ensures that only clear, deterministic errors are returned. Additionally, when applicable, we categorize the detected misconfigurations into specific groups, including “Resource Type Errors,” “Configuration Entry Errors,” “Configuration Entry Value Errors,” “Entry Dependency Errors,” and “Value Dependency Errors”. 1Supported resource types: https://docs.aws.amazon.com/serverlessrepo/latest/devguide/list-supported-resources.html , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 9]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 9 Table 1. The explanation of TP, FP, TN, and FN in our scenario. TP A misconfigured parameter correctly identified as misconfigured FP A correctly configured parameter mistakenly flagged as misconfigured TN A correctly configured parameter accurately recognized as valid FN A misconfigured parameter that is overlooked or incorrectly classified as valid For the format demand, to eliminate redundant content that does not reveal specific misconfigu- rations from the raw output, we use delimiters: “<START>” and “<END>”, to mark the required portion of the response. In SlsDetector , the desired output is enclosed within these markers, for example: “<START> Resource Type Errors: ..., Value Dependency Errors: ... <END>”. This structured way ensures that only the relevant content is captured. During post-processing, SlsDetector employs regular expressions to extract the information between these markers efficiently. Although the model might generate additional text beyond the expected response, the use of locators allows for the seamless extraction of relevant content while discarding unnecessary text. 4 EXPERIMENTAL EVALUATION To evaluate the effectiveness of SlsDetector in identifying misconfigurations within serverless applications, we present four research questions (Section 4.1). To answer these questions, we detail the evaluation metrics (Section 4.2), baselines for comparison (Section 4.3), evaluation dataset (Section 4.4), and experimental settings (Section 4.5). 4.1 Research Questions •RQ1: How does the effectiveness of SlsDetector compared to traditional data-driven methods? •RQ2: How effective is SlsDetector without considering our multi-dimensional constraints? •RQ3: How does the non-determinism of LLMs influence the effectiveness of SlsDetector ? •RQ4: How does the generalization capability of SlsDetector when using different LLMs? 4.2 Evaluation Metrics We use𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ,𝑟𝑒𝑐𝑎𝑙𝑙 , and𝐹1-𝑠𝑐𝑜𝑟𝑒 as evaluation metrics to compare SlsDetector against the baseline methods at the configuration parameter level, i.e., configuration entries or values. We check whether the detection approach can accurately determine the validity of each configuration parameter within the configuration file. 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 measures the proportion of correctly identified misconfigured parameters among all parameters flagged as misconfigured. 𝑟𝑒𝑐𝑎𝑙𝑙 quantifies the ability of the approach to detect actual misconfigurations by calculating the proportion of true misconfigured parameters that are correctly identified. 𝐹1-𝑠𝑐𝑜𝑟𝑒 provides a balanced measure that accounts for the significance of both false positives and false negatives. These metrics are calculated through True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN), explained in Table 1. 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =𝑇𝑃 𝑇𝑃+𝐹𝑃,𝑟𝑒𝑐𝑎𝑙𝑙 =𝑇𝑃 𝑇𝑃+𝐹𝑁, and𝐹1-𝑠𝑐𝑜𝑟𝑒 =2×𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛×𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑟𝑒𝑐𝑎𝑙𝑙. Values range from 0% to 100%, with scores closer to 100% indicating greater effectiveness. 4.3 Baseline Methods We implement two types of baselines to evaluate effectiveness. Given the lack of approaches specifically tailored for detecting misconfigurations in serverless computing, we first draw on principles from established data-driven techniques used in prior configuration studies [ 43,44, 61,62]. By adapting these methods, we create a data-driven baseline suited to the characteristics of serverless applications. Additionally, we introduce a straightforward LLM-based baseline as a second comparison, which does not consider our designed constraints. •Baseline 1: Data-driven method (DD method ). We implement a data-driven approach for server- less applications by learning configuration patterns from a dataset of configuration files. As no , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 10]
10 Jinfeng Wen et al. nQuestion: Are there any misconfigurations in the above configuration file?Configuration FileTaskDescriptionResponseØAnswer format (You MUSTfollow this):Detected errors are written between <START> and <END> tags: Fig. 4. The prompt of BL method. existing dataset specifically focuses on serverless application configurations, we collect our data from the AWS Serverless Application Repository (SAR) [ 7], an official repository for serverless applications where each application is packaged with an AWS SAM template and links to relevant configuration files. We include all configuration files associated with serverless applications that have been successfully deployed at least once as of August 18, 2023, which is the date we collected this dataset. This results in a collection of 701 configuration files across 658 serverless applications, with some links providing multiple configuration files representing distinct configurations. Given the correctness of ensuring the dataset, we conduct a careful manual review of the configuration files. This review was performed by the first two authors, who have a background in cloud comput- ing. Identified issues were discussed and resolved with consensus among the authors. To assess the consistency of independent labeling, we employ Cohen’s Kappa ( 𝜅) [23], a widely used metric for measuring inter-rater agreement. The resulting 𝜅value of 0.916 indicates an almost perfect agreement and a reliable labeling procedure [35]. Using this dataset, we learn configuration patterns, focusing on common resource types, config- uration entries, values, and dependencies among entries and values. To streamline this process, we first standardize the configuration files into a uniform representation. Object names for various resource types are identified, with object names replaced by standardized labels (e.g., a placeholder like “PH+resource type”) for consistency across configuration entries and values. Leveraging this standardized dataset, we extract the used resource types, entries, and values. To detect dependencies among both entries and values, we apply association rule mining tech- niques [ 56,59]. Specifically, we use the FP-Growth algorithm [ 29], which is known for its scalability. We need to set a support threshold for frequent itemsets using the formula 𝛼×𝑙𝑒𝑛, where𝑙𝑒𝑛 represents the total number of configuration files, a deterministic value, and 𝛼is a percentage that indicates the desired mining granularity. Leveraging mined frequent itemsets, we generate association rules by utilizing traversal way and dividing items into left and right sets, where items in the right set must appear if those in the left set are present. These rules reveal the configuration dependencies. If the tested file contains all items in a left set, this approach checks whether it includes the corresponding items in the right set. If any items are missing, it reports them. •Baseline 2 :Basic LLM method (BL method) . It is designed using a straightforward prompt that does not take our multi-dimensional constraints into account. This prompt contains the configura- tion file content followed by a task description. Similarly, the output is enclosed within a locator pair, “<START>” and “<END>”, to delimit the required response. This prompt is shown in Fig. 4. 4.4 Evaluation Dataset We conduct experimental evaluations on a dataset comprising three types of configurations. The first type includes error-free configurations, enabling us to evaluate true negatives and false positives in detection. The second type contains configurations with real-world errors, allowing for the assessment of true positives and false negatives. Although this second type is somewhat free of data leakage concerns of LLMs, we include a third type to strengthen the validity of our conclusions. The third type consists of configurations with injected errors, which are not exposed to LLMs during training, thereby eliminating data leakage concerns. By utilizing these diverse configurations, we can achieve a valid evaluation. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 11]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 11 •Configurations without Errors (26). We manually collect configuration files that have been suc- cessfully executed without errors. This data is separate from the one used to mine configuration patterns in the data-driven approach. We collect real-world configuration cases from GitHub. GitHub issues provide rich information, including developer discussions and related code or configuration fragments. We conduct the following steps. First, on July 2, 2024, the date we collected this data, we searched GitHub using the keywords “AWS,” “serverless,” and “configuration,” which yielded more than 8,000 relevant configuration-related issues. We then manually reviewed these issues to extract correct configura- tion fragments from the problematic cases—a time-consuming and challenging process. To facilitate this task, the first two authors jointly review the configurations. Initially, they filter through the configuration fragments by searching for terms including “successful,” “successfully,” and “it works” within the issues to identify correct configurations. For the fragments that matched, they conducted a manual verification process to ensure that the configurations were indeed error-free. Over two months, the two authors identified 52 configuration fragments that met our criteria. These error-free real-world configuration fragments are divided into two sets: 26 (naming from case 1 to case 26) are used to evaluate error-free configurations, while the remaining 26 (naming from case 27 to case 52) are reserved for generating configurations with injected errors, which is explained in detail later. •Real-world Misconfigurations (58). To evaluate the effectiveness of approaches in identifying real- world misconfigurations in serverless applications, we construct a relevant dataset by mining real-world configuration issues from GitHub. These issues need to contain clearly identified root causes as ground truths, enabling us to accurately assess the effectiveness of detection results. The selection process is as follows: First, we use the same keywords (i.e., “AWS,” “serverless,” and “configuration”) to search for relevant issues on GitHub on July 2, 2024. Next, we identify satisfied issues based on the following criteria: (i) the issue is marked as closed, indicating that it has been resolved; (ii) the issue includes a configuration fragment based on AWS SAM for analysis; and (iii) the discussion concludes with a clearly identified root cause of the problem. Using these criteria, we select 58 real-world configuration problems encountered in serverless applications, surpassing the scale of prior studies on configuration-related research [59, 61]. To ensure the accuracy of the configuration errors to be detected, we meticulously review each real-world configuration file in conjunction with its identified root cause. During this process, we also manually identify and address any potential configuration issues (e.g., outdated runtime) that could influence the evaluation. •Injected Misconfigurations (26). We construct injected misconfigurations by generating various errors in the correct configuration files. To achieve this, we use 26 error-free configuration files named from case 27 to case 52. Misconfigurations of different types are then generated, following misconfiguration generation rules from prior studies [ 38,39,41,46,54]. Prior studies [ 38,41] showed that these rules can cover most configurations. In addition to utilizing existing rules, we extend specific misconfiguration generation rules tailored to serverless application configurations, as outlined in Table 2. For each selected configuration file, we randomly sample a configuration parameter that aligns with the subcategories in Table 2 and generate invalid configurations, creating a new erroneous configuration file for detection. In total, we generate 26 configuration files with injected misconfigurations for evaluation. Our evaluation dataset contains 110 configuration files with corresponding ground-truth answers. Fig. 5 shows its details. Of these, 26 are error-free configuration files, 58 contain real-world errors, and 26 have injected errors. Across all configuration parameters, there are 4,108 correct configura- tion parameters and 308 misconfigured ones. Among the misconfigured parameters, 90 involve incorrect resource types, 108 have misconfigured entries, 48 contain incorrect values, 39 exhibit , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 12]
12 Jinfeng Wen et al. Table 2. Misconfiguration generation rules (we use generation rules from previous work [ 38,39,46,54] and customize them in our scenario.) Category Subcategory Specification Generation Rules SyntaxResource typeValue set = {AWS::Serverless::Function, AWS::Serverless::Api, ...}Generate a resource type that does not belong to the value set Entry Value set = {entry1, entry2, ...}, specific en- tries are used in a certain resource typeGenerate an invalid entry for a resource type RangeBasic numericValid range constrained by data type Generate values outside the valid range (e.g., max value+1) Enum Options, value set = {enum1, enum2, ...}, specific values are used in a certain config- uration entryGenerate a value that does not belong to set DependencyEntry relationship(𝑃1,𝑉,⋄) ↦→𝑃2,⋄ ∈ { >,≥,=,≠,<,≤ ,𝑜𝑐𝑐𝑢𝑟𝑟𝑒𝑛𝑐𝑒}Generate invalid entry relationships for configuration entries (𝑃1,𝑉,¬⋄) Value relationship(𝑃1,𝑃2,⋄),⋄ ∈ { >,≥,=,≠,<,≤ ,𝑜𝑐𝑐𝑢𝑟𝑟𝑒𝑛𝑐𝑒}Generate invalid value relationship for configuration entry values (𝑃1,𝑃2,¬⋄) 110 Configuration Files (Evaluation Dataset)26 Configuration Files without Errors58 Configuration Files with Real-world Errors26 Configuration Files with Injected Errors4,108 Correct Configuration Parameters308 Misconfigured Parameters90Misconfigured Resource Types108Misconfigured Entries48Misconfigured Values39Misconfigured EntryDependencies23Misconfigured ValueDependencies Fig. 5. The Details of Evaluation Dataset. entry dependency issues, and 23 have value dependency issues. We analyze the detection results across all configuration parameters to obtain TP, FP, TN, and FN. We then calculate 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 , 𝑟𝑒𝑐𝑎𝑙𝑙 , and𝐹1-𝑠𝑐𝑜𝑟𝑒 to evaluate the effectiveness of the detection. 4.5 Experimental Settings We introduce our parameter settings, experimental repetitions, and experimental environment. Parameter Settings. ForRQ1 , the compared DD method needs to specify a frequent threshold, 𝛼. We experiment with various threshold levels: low (1%), medium (3% and 5%), and high (10%). A lower threshold corresponds to a lower support value, enabling the discovery of more dependencies. For comparisons with SlsDetector , we use a default 𝛼value of 5%. Experimental results also show that 5% is optimal for achieving the best effectiveness results in DD methods. We also report results for both SlsDetector and DD method across other thresholds. For RQ2 , we compare SlsDetector with the BL method, both of which leverage LLMs. We select ChatGPT-4o as the default LLM due to the widespread use and outstanding performance of ChatGPT in recent research [ 41,60]. A crucial parameter of LLMs is the temperature, which controls the level of randomness in the generated responses. To ensure reproducibility and consistency, we follow the previous work [ 21,28,53,57] to set the temperature to 0 for all identical queries. For RQ3 , there are no specific parameters to be set. ForRQ4 , we evaluate the generalization capability of SlsDetector across various LLMs, excluding ChatGPT-4o. Specifically, we utilize an open-source model, Llama 3.1 (405B) Instruct Turbo, and a proprietary model, Gemini 1.5 Pro. These models are among the top-ranked LLMs [ 11]. As with RQ2, we set the temperature of LLMs to 0 to maintain consistent outputs across repeated queries. Experimental Repetitions. For experiments involving stochastic processes, we follow established best practices [ 28,53], repeating each experiment five times and reporting the mean evaluation metrics to reduce the impact of random variations. Experimental Environment. Our experiments were conducted on an Ubuntu 18.04.4 LTS server with an Intel Xeon (R) 4-core processor and 24 GiB of memory. The LLMs were accessed through their , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 13]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 13 Table 3. RQ1: Results about SlsDetector and DD method. Methods 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝐹1-𝑠𝑐𝑜𝑟𝑒 DD method with 5% threshold (default) 19.06% 70.78% 30.03% SlsDetector (vs DD method with 5% threshold) 72.88% (↑53.82%) 88.18% (↑17.40%) 79.75% (↑49.72%) DD method with 10% threshold 17.70% 64.61% 27.79% DD method with 3% threshold 18.83% 70.13% 29.69% DD method with 1% threshold 18.85% 70.45% 29.75% Table 4. RQ1: Results*of TP, FN, FP, and TN for DD method and SlsDetector . Methods308 misconfigured parameters 4,108 correct configuration parameters TP FN FP TN DD method (default) 218 (70.78%) 90 (29.22%) 926 (22.54%) 3,182 (77.46%) SlsDetector (default) 272 (88.31%) ✓ 36 (11.69%) ✓ 102 (2.48%) ✓ 4,006 (97.52%) ✓ *Higher TP and TN are preferable, while lower FN and FP are desired. respective APIs. While all methods are implemented in Python, their misconfiguration detection capabilities are independent of the underlying programming language. 5 EVALUATION RESULTS This section gives and discusses the results of each research question. 5.1 RQ1: Effectiveness of SlsDetector and Data-Driven Method (DD method) This section explores the effectiveness of SlsDetector in comparison to the DD method. SlsDetector has a significant advantage in the effectiveness aspect. Table 3 presents their results in detecting misconfigurations in serverless applications. Specifically, SlsDetector achieves a𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 72.88%, 𝑟𝑒𝑐𝑎𝑙𝑙 of 88.18%, and 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 79.75%. In contrast, the DD method, with its default threshold of 5%, only reaches a 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 19.06%,𝑟𝑒𝑐𝑎𝑙𝑙 of 70.78%, and 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 30.03%. SlsDetector outperforms the DD method, increasing 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 by 53.82 percentage points, 𝑟𝑒𝑐𝑎𝑙𝑙 by 17.40 percentage points, and𝐹1-𝑠𝑐𝑜𝑟𝑒 by 49.72 percentage points, showing its superior effectiveness. We investigate why the DD method produces less effective results. One major issue is its low 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 (19.06%) and 𝐹1-𝑠𝑐𝑜𝑟𝑒 (30.03%). We further observe TP, FN, FP, and TN values obtained by the DD method across all configuration parameters, as shown in Table 4. Results show that the FP value is 926, indicating that 22.54% of the 4,108 correct configuration parameters are mistakenly flagged as misconfigurations. In contrast, on average, SlsDetector misclassifies only 2.48% of correct configuration parameters as misconfigurations. Thus, the low effectiveness of the DD method is attributed to high false positives. As a data-driven approach, the DD method learns configuration patterns based on historical data, which mainly includes previously used configurations. This reliance makes it difficult to accurately identify configurations that are either rare or newly supported, resulting in numerous false positives. Thus, the DD method fails to detect some valid configurations that are indeed supported, leading to its low 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 and𝐹1-𝑠𝑐𝑜𝑟𝑒 . We also compare the effectiveness of the DD method under different thresholds 𝛼: 10%, 3%, and 1%, with the results presented in Table 3. As 𝛼decreases from 10% to 1%, the evaluation metrics show improvement. Specifically, 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 increases from 17.70% to 18.85%, 𝑟𝑒𝑐𝑎𝑙𝑙 rises from 64.61% to 70.45%, and 𝐹1-𝑠𝑐𝑜𝑟𝑒 improves from 27.79% to 29.75%. To further explore the reasons for their changes, we give TP, FN, FP, and TN results of the DD method under different thresholds, as shown in Table 5. The primary reason for improvements is that lower 𝛼mines more dependencies among entries or values. This enables the accurate identification of a larger number of misconfigured parameters. Specifically, the TP value for the DD method at a 10% threshold is 199, whereas at a 1% , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 14]
14 Jinfeng Wen et al. Table 5. RQ1: Results of TP, FN, FP, and TN for DD method with different thresholds 𝛼. Methods308 misconfigured parameters 4,108 correct configuration parameters TP FN FP TN DD method with 10% threshold 199 109 925 3,183 DD method with 3% threshold 216 92 931 3,177 DD method with 1% threshold 217 91 934 3,174 Table 6. RQ2: Results about SlsDetector and BL method using the default LLM (ChatGPT-4o). Baseline 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙𝐹1-𝑠𝑐𝑜𝑟𝑒 Our Approach 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝐹1-𝑠𝑐𝑜𝑟𝑒 BL method 51.65% 65.00% 57.55%SlsDetector (vs BL method)72.88% (↑21.23%)88.18% (↑23.18%)79.75% (↑22.20%) Table 7. RQ2: Results of TP, FN, FP, and TN for BL method and SlsDetector , on average. Methods308 misconfigured parameters 4,108 correct configuration parameters TP FN FP TN BL method (default) 200 (64.94%) 107 (34.74%) 188 (4.58%) 3,920 (95.42%) SlsDetector (default) 272 (88.31%) ✓ 36 (11.69%) ✓ 102 (2.48%) ✓ 4,006 (97.52%) ✓ *Higher TP and TN are preferable, while lower FN and FP are desired. threshold, it increases to 217. This improvement leads to a higher 𝑟𝑒𝑐𝑎𝑙𝑙 , increasing from 64.61% to 70.45%. However, a lower 𝛼also increases the risk of generating potentially invalid dependencies, resulting in correctly configured parameters being mistakenly flagged as misconfigurations. This is evident from the FP values: the FP value for the DD method at a 10% threshold is 925, while at a 1% threshold, it increases to 934. As a result, 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 shows only a modest improvement, from 17.70% to 18.85%. For 𝐹1-𝑠𝑐𝑜𝑟𝑒 , lowering𝛼enhances the effectiveness of the DD method, reaching a value of 29.75%. However, it still significantly lags behind the 79.75% achieved by SlsDetector . In addition, we observe that a threshold of 5% for the DD method yields superior results compared to 1%, 3%, and 10%, suggesting that 5% is an optimal threshold for the data-driven method in this scenario. In the threshold of 5%, the FP-growth algorithm can effectively mine relationships without losing valid dependencies or generating an excessive number of invalid dependencies. However, even at 5%, the effectiveness of the DD method remains significantly lower than that of SlsDetector , with particularly low 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 and𝐹1-𝑠𝑐𝑜𝑟𝑒 . Ans. to RQ1: SlsDetector achieves a𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 72.88%,𝑟𝑒𝑐𝑎𝑙𝑙 of 88.18%, and 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 79.75%, surpassing data-driven methods across all metrics. It shows significant improvements, with increases of 53.82 percentage points in 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 , 17.40 percentage points in 𝑟𝑒𝑐𝑎𝑙𝑙 , and 49.72 percentage points in 𝐹1-𝑠𝑐𝑜𝑟𝑒 . These results suggest the high effectiveness of SlsDetector . 5.2 RQ2: Effectiveness of SlsDetector and Basic LLM-based Method (BL method) We explore the effectiveness of SlsDetector in comparison to the BL method using the default ChatGPT-4o for detecting misconfigurations in serverless applications. Table 6 presents their results, showing that SlsDetector is more effective than the BL method. Specifically, SlsDetector achieves a𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 72.88%,𝑟𝑒𝑐𝑎𝑙𝑙 of 88.18%, and an 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 79.75%. The BL method achieves a𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 51.65%,𝑟𝑒𝑐𝑎𝑙𝑙 of 65.00%, and an 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 57.55%. SlsDetector outperforms the BL method across all metrics, with increases in 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 by 21.23 percentage points, 𝑟𝑒𝑐𝑎𝑙𝑙 by 23.18 percentage points, and 𝐹1-𝑠𝑐𝑜𝑟𝑒 by 22.20 percentage points. We investigate the reasons for the low effectiveness of the BL method. Table 7 shows TP, FN, FP, and TN values obtained by the BL method across all configuration parameters. The results indicate that the BL method has a low TP value of 200, successfully identifying only 64.94% of , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 15]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 15 Table 8. RQ2: The average number of misconfigured parameters correctly identified as misconfigured across different categories. MethodsMisconfigured resource types (90)Misconfigured entries (108)Misconfigured values (48)Misconfigured entry dependencies (39)Misconfigured value dependencies (23) BL 62 (68.89%) 83 (76.85%) 39 (81.25%) 7 (17.95%) 12 (52.17%) SlsDetector 84 (93.33%) ✓ 93 (86.11%) ✓ 43 (89.58%) ✓ 38 (97.44%) ✓ 19 (82.61%) ✓ Table 9. RQ3: Evaluation metrics results of SlsDetector across five repetitions. Metrics Repetition 1 Repetition 2 Repetition 3 Repetition 4 Repetition 5 Mean 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 71.83% 70.78% 70.35% 75.28% 76.14% 72.88% 𝑟𝑒𝑐𝑎𝑙𝑙 91.88% 91.23% 84.74% 86.04% 87.01% 88.18% 𝐹1-𝑠𝑐𝑜𝑟𝑒 80.63% 79.72% 76.88% 80.30% 81.21% 79.75% the 308 misconfigured parameters. In contrast, SlsDetector accurately identifies an average of 272 (88.31%) misconfigured parameters. To further explore the root causes of the BL method’s low effectiveness, we examine the average number of misconfigured parameters correctly identified across different categories. As presented in Table 8, the BL method identifies fewer errors than SlsDetector in each category, including resource types, entries, values, entry dependencies, and value dependencies. Particularly, the BL method only detects 17.95% of misconfigured entry dependencies, while SlsDetector detects 97.44%. We check specific configurations and observe that the BL method struggles to identify configuration entries related to cloud service resources that should co-occur with the event sources defined by serverless functions. For instance, the configuration entry RestApiId under an event source of type “Api” should be associated with configuration entries of the “AWS::Serverless::Api” resource type. Overall, these results indicate that relying solely on the raw capabilities of LLMs, as done in the BL method, is inadequate for the complex task of detecting misconfigurations in serverless applications. A key factor contributing to the improved effectiveness of SlsDetector is its ability to incorporate multi-dimensional constraints for guiding LLM inferences. These constraints are designed across various dimensions. By integrating them into the analysis, SlsDetector enhances the decision-making process, resulting in a more effective identification of misconfigurations. Ans. to RQ2: SlsDetector outperforms the BL method across all metrics using the default ChatGPT-4o, with increases in 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 by 21.23 percentage points, 𝑟𝑒𝑐𝑎𝑙𝑙 by 23.18 percentage points, and𝐹1-𝑠𝑐𝑜𝑟𝑒 by 22.20 percentage points. This suggests that integrating multi-dimension constraints is beneficial for handling misconfiguration detection in serverless applications. 5.3 RQ3: Impact of Non-determinism on SlsDetector We explore how the non-determinism of LLMs impacts our evaluation results. As detailed in Section 4.5, each experiment is repeated five times. We analyze their results shown in Table 9 to assess the reliability of our conclusions. Results show that while the non-determinism of LLMs can influence evaluation results, its effect is relatively minor. SlsDetector consistently achieves high effectiveness across different trials. 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ranges from 70.35% to 76.14%, 𝑟𝑒𝑐𝑎𝑙𝑙 varies between 84.74% and 91.88%, and 𝐹1-𝑠𝑐𝑜𝑟𝑒 falls between 76.88% and 81.21%. Even the lowest values, i.e., 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 at 70.35%,𝑟𝑒𝑐𝑎𝑙𝑙 at 84.74%,𝐹1-𝑠𝑐𝑜𝑟𝑒 at 76.88%, are still higher than 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 (19.06%), 𝑟𝑒𝑐𝑎𝑙𝑙 (70.78%), and 𝐹1-𝑠𝑐𝑜𝑟𝑒 (30.03%) of the data-driven approach. Furthermore, the lowest metric values for SlsDetector remain approximately 20 percentage points higher than the average results (i.e.,𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 at 51.65%,𝑟𝑒𝑐𝑎𝑙𝑙 at 65.00%,𝐹1-𝑠𝑐𝑜𝑟𝑒 at 57.55%) of the basic LLM-based method, as shown in Table 6. This suggests that our conclusions regarding SlsDetector are not affected by the non-determinism of LLMs. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 16]
16 Jinfeng Wen et al. Table 10. RQ4: Results about SlsDetector and BL method using various LLMs. BL Method 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙𝐹1-𝑠𝑐𝑜𝑟𝑒 Our Approach 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝐹1-𝑠𝑐𝑜𝑟𝑒 BL (GPT-4o) 51.65% 65.00% 57.55%SlsDetector (GPT-4o) (vs BL)72.88% (↑21.23%)88.18% (↑23.18%)79.75% (↑22.20%) BL (Llama) 48.88% 58.38% 53.09%SlsDetector (Llama) (vs BL)70.27% (↑21.39%)78.38% (↑20.00%)74.05% (↑20.96%) BL (Gemini) 44.41% 22.86% 30.11%SlsDetector (Gemini) (vs BL)71.72% (↑27.31%)74.35% (↑51.49%)72.93% (↑42.82%) Ans. to RQ3: Our conclusions are not impacted by the non-determinism of LLMs. 5.4 RQ4: Generalization Capability of SlsDetector To explore the generalization of SlsDetector , we use two additional models: the open-source Llama 3.1 (405B) Instruct Turbo model and the proprietary Gemini 1.5 Pro model. SlsDetector consistently achieves high effectiveness across all metrics, with 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ,𝑟𝑒𝑐𝑎𝑙𝑙 , and𝐹1-𝑠𝑐𝑜𝑟𝑒 values exceeding 70%, regardless of the LLM utilized. Table 10 shows their results. Specifically, with the Llama 3.1 (405B) Instruct Turbo, SlsDetector achieves a𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 70.27%,𝑟𝑒𝑐𝑎𝑙𝑙 of 78.38%, and an 𝐹1- 𝑠𝑐𝑜𝑟𝑒 of 74.05%. With the Gemini 1.5 Pro model, SlsDetector yields a𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 71.72%,𝑟𝑒𝑐𝑎𝑙𝑙 of 74.35%, and an 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 72.93%. Among these, SlsDetector with ChatGPT-4o offers the highest effectiveness, while SlsDetector with the Gemini 1.5 Pro model shows comparatively lower metrics but still achieves a high 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 72.93%. We also evaluate the BL method with different LLMs, shown in Table 10. We observe considerable variability. While the BL method achieves 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ,𝑟𝑒𝑐𝑎𝑙𝑙 , and𝐹1-𝑠𝑐𝑜𝑟𝑒 values approaching or exceeding 50% when using ChatGPT-4o and Llama 3.1 (405B) Instruct Turbo, its effectiveness drops substantially with the Gemini 1.5 Pro model, where 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 is 44.41%,𝑟𝑒𝑐𝑎𝑙𝑙 is 22.86%, and 𝐹1- 𝑠𝑐𝑜𝑟𝑒 is 30.11%. This indicates a key limitation of the BL method: its effectiveness is dependent on the specific LLM used. In contrast, SlsDetector provides the ability to maintain consistent effectiveness across different models, showing its generalization. We compare the effectiveness differences between SlsDetector and the BL method when using the same LLM. As discussed in RQ2, SlsDetector outperforms the BL method with ChatGPT-4o by over 20 percentage points across all evaluation metrics. From Table 10, when utilizing the Llama 3.1 (405B) Instruct Turbo model, SlsDetector also achieves improvements of over 20 percentage points across all evaluation metrics compared to the BL method. With the Gemini 1.5 Pro model, SlsDetector outperforms the BL method with even greater gains, achieving 27.31 percentage points higher in 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 , 51.49 percentage points higher in 𝑟𝑒𝑐𝑎𝑙𝑙 , and 42.82 percentage points higher in 𝐹1-𝑠𝑐𝑜𝑟𝑒 . The effectiveness gap is especially pronounced with Gemini 1.5 Pro, showing an effectiveness difference of around 50% in 𝑟𝑒𝑐𝑎𝑙𝑙 and𝐹1-𝑠𝑐𝑜𝑟𝑒 , underscoring the effectiveness of our approach. Ans. to RQ4: SlsDetector exhibits generalization capability, consistently achieving highly effective results across various LLMs. In contrast, the effectiveness of the BL method varies significantly depending on the chosen LLM. When using the Gemini 1.5 Pro model, SlsDetector outperforms the BL method by approximately 50 percentage points in both 𝑟𝑒𝑐𝑎𝑙𝑙 and𝐹1-𝑠𝑐𝑜𝑟𝑒 . 6 THREATS TO VALIDITY Data Leakage Concerns. One potential risk when using LLMs is data leakage, as these models are trained on vast datasets. Specifically, open-source configuration data may have been exposed to the LLMs utilized in SlsDetector , raising concerns about memorization of our evaluated error-free , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 17]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 17 configurations available on platforms such as GitHub. However, during our evaluation, we found that the model did not recognize outdated configuration values as correct, suggesting that the error-free configurations we evaluated were not fully present in the LLM’s training data. Note that outdated configuration values were manually corrected before our experimental evaluation. Our evaluation data also includes both real-world and manually injected misconfigurations. The ground truths for real-world errors are established through an analysis of developer discussions on GitHub to identify root causes. Injected misconfigurations are deliberately introduced into correct configurations through misconfiguration generation rules. These misconfigurations were not exposed to the LLM during training. In addition, the number of configuration files evaluated with errors (58 real-world + 26 injected = 84) significantly exceeded those without errors (26). Thus, the likelihood of our effectiveness results being significantly affected by data leakage is negligible. We also compare the effectiveness of the BL method without our multi-dimensional constraints. The BL method yields a 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 51.65%, a𝑟𝑒𝑐𝑎𝑙𝑙 of 65.00%, and an 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 57.55%, indicating low effectiveness. If our evaluation dataset had been exposed to the LLMs, we would expect the BL method to achieve significantly higher results. However, the results do not reflect this, suggesting that our evaluation dataset was not exposed to the LLMs. SlsDetector incorporates multi-dimensional constraints to detect the same evaluation dataset, resulting in improved metrics: a 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 of 72.88%, a𝑟𝑒𝑐𝑎𝑙𝑙 of 88.18%, and an 𝐹1-𝑠𝑐𝑜𝑟𝑒 of 79.75%. These enhancements indicate that our design effectively boosts detection results, rather than being influenced by potential data leakage. Randomness Concerns. Randomness can impact evaluation results in two key aspects. The first is the inherent randomness in LLM behavior. To address this, we set a crucial parameter, specifically initializing temperature as 0, to ensure that the model produces consistent outputs for the same input. The second aspect involves randomness in the experimental process. To mitigate this, we conduct five independent experiments for each experimental setup and use the mean results as the final outcome. These strategies minimize the potential impact of randomness on our results. 7 RELATED WORK 7.1 Serverless Computing The increasing adoption of serverless computing has attracted widespread interest from the re- search community, particularly within SE. A broad range of topics has been explored, including literature reviews [ 50], evolution and current state [ 47], and analyses of serverless application characteristics [ 25,26]. Additional research has delved into the challenges faced by developers [ 51], the development of stateful serverless applications [ 19], and methods for testing and debugging [ 36]. For example, a comprehensive literature review [ 50] was conducted to explore the breadth and depth of serverless computing research. Eismann et al. [25] analyzed 89 serverless applications to assess them from multiple dimensions. Wen et al. [51] identified 36 challenges developers face when developing serverless applications, highlighting configuration issues as a prominent concern. Despite these efforts, to the best of our knowledge, no prior work has addressed misconfiguration detection in serverless computing. This paper fills this gap by introducing SlsDetector . 7.2 Traditional Misconfiguration Detection Existing misconfiguration detection methods can be categorized into two types: white-box and black-box approaches. White-box approaches [ 42,46,48,49,54] generally focus on source code or program analysis to identify misconfigurations within the codebase, relying on manually defined domain-specific rules. For example, Rex [ 42] detected dependency violations between source code and configurations that must be updated together. Ctest [ 46] identified configuration-induced failures in code affected by configuration changes. SPEX [ 54] employed static program analysis to , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 18]
18 Jinfeng Wen et al. infer configuration constraints, designing predefined rules from variables in the source code to uncover misconfiguration vulnerabilities. However, these methods are not well-suited for detecting misconfigurations in serverless applications, which rely on YAML-based configuration files rather than source code structures. Serverless-specific misconfigurations, embedded in configuration files, require new approaches that extend beyond traditional white-box techniques. Black-box approaches [ 43,44,61,62] are generally data-driven and rely on learning configura- tion patterns from a dataset of example configurations. For instance, EnCore [ 61] used numerous configurations to learn and customize rule templates, inferring correlations and detecting mis- configurations in server applications. ConfigC [ 44] analyzed a dataset of correct configurations to build a language model that could detect errors in new configurations. DRIVE [ 62] created a Dockerfiles dataset and applied sequential pattern mining to extract frequent patterns, identifying rule violations through heuristic-based reduction and human intervention. However, these data- driven methods have inherent limitations: (i) They require a well-curated dataset, but ensuring the completeness and correctness of such datasets is challenging. As a result, configurations not represented in the training data may be missed, while normal configurations might be incorrectly flagged as anomalies due to dataset gaps. (ii) To compensate for dataset issues, these methods incor- porate domain-specific knowledge (e.g., customized rule templates), requiring significant manual effort and continuous checking. These limitations hinder the practical application of data-driven approaches. Our results on RQ1 show that such approaches are less effective in our scenario. 7.3 LLM-based Misconfiguration Detection LLM-based approaches offer a promising alternative. A recent arXiv paper presented Ciri [ 41], an LLM-based configuration validator. It demonstrated the potential of LLMs for detecting misconfigu- rations in systems such as Alluxio, Django, Etcd, and HDFS. However, Ciri depends on an external database containing valid configurations, misconfigurations, related questions, and ground-truth responses. Constructing this database is costly and challenging for various scenarios. In contrast, SlsDetector employs zero-shot learning that does not require external datasets, eliminating the need for predefined data. On the other hand, Ciri used a prompt without any constraint, limiting its ability to detect dependencies [ 41]. Serverless applications have complex configuration structures and stronger interdependencies, making simple prompt-based methods less effective. Our results on RQ2 and RQ3 show that such a method (i.e., BL method) is less effective in our scenario. Instead, SlsDetector incorporates carefully designed multi-dimensional constraints without predefined data, providing a more effective detection for serverless application configurations. 8 CONCLUSION Our work opens a promising research direction, showing that LLMs can effectively address con- figuration issues in cloud applications built on emerging serverless computing. Specifically, we introduced SlsDetector , the first LLM-based framework specifically designed for detecting miscon- figurations in serverless applications. It leveraged advanced prompt engineering and zero-shot learning to effectively identify configuration issues with minimal input effort. SlsDetector included a prompt generation component that integrates the configuration file to be detected, task descrip- tion, multi-dimensional constraints, and customized responses. Particularly, the multi-dimensional constraints are tailored to the configuration characteristics of serverless applications, offering context-aware guidance using the Chain of Thought technique. The customized responses focused on both content and format demands, ensuring that the LLM outputs deterministic and clearly explained detection results. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 19]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 19 Our evaluation on a curated dataset of 110 configuration files demonstrated that SlsDetector achieved a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%, surpassing state-of-the- art data-driven methods by 53.82, 17.40, and 49.72 percentage points, respectively. Furthermore, we investigated the generalization capability of SlsDetector across various LLMs, finding that it consistently maintains high effectiveness across these models. REFERENCES [1]2024. Advantages of building serverless applications on AWS. https://www.embitel.com/blog/ecommerce-blog/ advantages-of-building-serverless-applications-on-aws. [2] 2024. Amazon S3. https://aws.amazon.com/s3/. [3]2024. AWS CloudFormation template. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws- resource-lambda-function.html. [4]2024. AWS resource and property types reference. https://docs.aws.amazon.com/AWSCloudFormation/latest/ UserGuide/aws-template-resource-type-ref.html. [5] 2024. AWS SAM. https://aws.amazon.com/cn/serverless/sam. [6]2024. AWS SAM resources and properties. https://docs.aws.amazon.com/serverless-application-model/latest/ developerguide/sam-specification-resources-and-properties.html. [7] 2024. AWS Serverless Application Repository. https://serverlessrepo.aws.amazon.com/applications. [8]2024. AWSTemplateFormatVersion. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/format- version-structure.html. [9] 2024. Azure Functions. https://docs.microsoft.com/en-us/azure/azure-functions/. [10] 2024. Chain-of-Thought Prompting. https://learnprompting.org/docs/intermediate/chain_of_thought. [11] 2024. Chatbot Arena LLM Leaderboard: Community-driven Evaluation for Best LLM and AI chatbots. https://lmarena. ai/. [12] 2024. DoorDash confirms data breach affected 4.9 million customers, workers and merchants. https://techcrunch.com/ 2019/09/26/doordash-data-breach/. [13] 2024. An example of the configuration file. https://github.com/aws/serverless-application-model/issues/214. [14] 2024. Google Cloud Functions. https://cloud.google.com/functions. [15] 2024. Serverless architectures with AWS SAM. https://medium.com/@christopheradamson253/serverless-architectures- with-aws-sam-serverless-application-model-2b83298fbcbc. [16] 2024. Utah COVID-19 testing service. https://www.comparitech.com/blog/information-security/utah-covid-test- center-leak/. [17] Toufique Ahmed and Premkumar Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering . 1–5. [18] Lixiang Ao, Liz Izhikevich, Geoffrey M Voelker, and George Porter. 2018. Sprocket: A serverless video processing framework. In Proceedings of the ACM Symposium on Cloud Computing . 263–274. [19] Daniel Barcelona-Pons, Pierre Sutra, Marc Sánchez-Artigas, Gerard París, and Pedro García-López. 2022. Stateful serverless computing with crucial. ACM Transactions on Software Engineering and Methodology 31, 3 (2022), 1–38. [20] Qingrong Chen, Teng Wang, Owolabi Legunsen, Shanshan Li, and Tianyin Xu. 2020. Understanding and discovering software configuration dependencies in cloud and datacenter systems. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 362–374. [21] Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao, Xuedong Gao, Hao Fan, Ming Wen, et al.2024. Automatic root cause analysis via large language models for cloud incidents. In Proceedings of the 19th European Conference on Computer Systems . 674–688. [22] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. A survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402 (2023). [23] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 1 (1960), 37–46. [24] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2024. Self-collaboration code generation via ChatGPT. ACM Transactions on Software Engineering and Methodology 33, 7 (2024), 1–38. [25] Simon Eismann, Joel Scheuner, Erwin Van Eyk, Maximilian Schwinger, Johannes Grohmann, Nikolas Herbst, Cristina Abad, and Alexandru Iosup. 2021. The state of serverless applications: collection, characterization, and community consensus. IEEE Transactions on Software Engineering (2021). [26] Simon Eismann, Joel Scheuner, Erwin Van Eyk, Maximilian Schwinger, Johannes Grohmann, Nikolas Herbst, Cristina L Abad, and Alexandru Iosup. 2020. Serverless applications: Why, when, and how? IEEE Software 38, 1 (2020), 32–39. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 20]
20 Jinfeng Wen et al. [27] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. 2023. Automated repair of programs from large language models. In Proceedings of the 2023 IEEE/ACM 45th International Conference on Software Engineering . IEEE, 1469–1481. [28] Fatemeh Hadadi, Qinghua Xu, Domenico Bianculli, and Lionel Briand. 2024. Anomaly detection on unstable logs with GPT models. arXiv preprint arXiv:2406.07467 (2024). [29] Jiawei Han, Jian Pei, and Yiwen Yin. 2000. Mining frequent patterns without candidate generation. ACM SIGMOD Record 29, 2 (2000), 1–12. [30] Hassan B Hassan, Saman A Barakat, and Qusay I Sarhan. 2021. Survey on serverless computing. Journal of Cloud Computing 10, 1 (2021), 1–29. [31] Mia Mohammad Imran, Preetha Chatterjee, and Kostadin Damevski. 2024. Uncovering the causes of emotions in software developer communication using zero-shot llms. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1–13. [32] Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti, Chia-Che Tsai, Anurag Khandelwal, Qifan Pu, Vaishaal Shankar, Joao Carreira, Karl Krauth, Neeraja Yadwadkar, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica, and David A. Patterson. 2019. Cloud programming simplified: A Berkeley view on serverless computing. arXiv preprint arXiv:1902.03383 (2019). [33] David Kavaler, Sasha Sirovica, Vincent Hellendoorn, Raul Aranovich, and Vladimir Filkov. 2017. Perceived language complexity in GitHub issue discussions and their effect on issue resolution. In Proceedings of the 2017 32nd IEEE/ACM International Conference on Automated Software Engineering . IEEE, 72–83. [34] Stratos Kourtzanidis, Alexander Chatzigeorgiou, and Apostolos Ampatzoglou. 2020. RepoSkillMiner: Identifying software expertise from GitHub repositories using natural language processing. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering . 1353–1357. [35] J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics 33, 1 (1977), 159–174. [36] Valentina Lenarduzzi and Annibale Panichella. 2020. Serverless testing: Tool vendors’ and experts’ points of view. IEEE Software 38, 1 (2020), 54–60. [37] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering and Methodology (2023). [38] Shanshan Li, Wang Li, Xiangke Liao, Shaoliang Peng, Shulin Zhou, Zhouyang Jia, and Teng Wang. 2018. Confvd: System reactions analysis and evaluation through misconfiguration injection. IEEE Transactions on Reliability 67, 4 (2018), 1393–1405. [39] Wang Li, Zhouyang Jia, Shanshan Li, Yuanliang Zhang, Teng Wang, Erci Xu, Ji Wang, and Xiangke Liao. 2021. Challenges and opportunities: An in-depth empirical study on configuration error injection testing. In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis . 478–490. [40] Zijun Li, Linsong Guo, Jiagan Cheng, Quan Chen, Bingsheng He, and Minyi Guo. 2022. The serverless computing survey: A technical primer for design architecture. Comput. Surveys 54, 10s (2022), 220:1–220:34. [41] Xinyu Lian, Yinfang Chen, Runxiang Cheng, Jie Huang, Parth Thakkar, and Tianyin Xu. 2023. Configuration validation with large language models. arXiv preprint arXiv:2310.09690 (2023). [42] Sonu Mehta, Ranjita Bhagwan, Rahul Kumar, Chetan Bansal, Chandra Maddila, Balasubramanyan Ashok, Sumit Asthana, Christian Bird, and Aditya Kumar. 2020. Rex: Preventing bugs and misconfiguration in large services using correlated change analysis. In Proceedings of the 17th USENIX Symposium on Networked Systems Design and Implementation . 435–448. [43] Mark Santolucito, Ennan Zhai, Rahul Dhodapkar, Aaron Shim, and Ruzica Piskac. 2017. Synthesizing configuration file specifications with association rule learning. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1–20. [44] Mark Santolucito, Ennan Zhai, and Ruzica Piskac. 2016. Probabilistic automated language learning for configuration files. In Proceedings of the Computer Aided Verification: 28th International Conference . Springer, 80–87. [45] Vaishaal Shankar, Karl Krauth, Kailas Vodrahalli, Qifan Pu, Benjamin Recht, Ion Stoica, Jonathan Ragan-Kelley, Eric Jonas, and Shivaram Venkataraman. 2020. Serverless linear algebra. In Proceedings of the 11th ACM Symposium on Cloud Computing . 281–295. [46] Xudong Sun, Runxiang Cheng, Jianyan Chen, Elaine Ang, Owolabi Legunsen, and Tianyin Xu. 2020. Testing configu- ration changes in context to prevent production failures. In Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation . 735–751. [47] Davide Taibi, Josef Spillner, and Konrad Wawruch. 2020. Serverless computing-where are we now, and where are we heading? IEEE software 38, 1 (2020), 25–31. [48] John Toman and Dan Grossman. 2016. Staccato: A bug finder for dynamic configuration updates. In Proceedings of the 30th European Conference on Object-Oriented Programming . Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. , Vol. 1, No. 1, Article . Publication date: November 2024.

[Página 21]
LLM-Based Misconfiguration Detection for AWS Serverless Computing 21 [49] Teng Wang, Zhouyang Jia, Shanshan Li, Si Zheng, Yue Yu, Erci Xu, Shaoliang Peng, and Xiangke Liao. 2023. Under- standing and detecting on-the-fly configuration bugs. In Proceedings of the 45th International Conference on Software Engineering . [50] Jinfeng Wen, Zhenpeng Chen, Xin Jin, and Xuanzhe Liu. 2023. Rise of the planet of serverless computing: a systematic review. ACM Transactions on Software Engineering and Methodology 32, 5 (2023), 1–61. [51] Jinfeng Wen, Zhenpeng Chen, Yi Liu, Yiling Lou, Yun Ma, Gang Huang, Xin Jin, and Xuanzhe Liu. 2021. An empirical study on challenges of application development in serverless computing. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 416–428. [52] Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, and Jianwei Yin. 2023. ChatUniTest: a ChatGPT-based automated unit test generation tool. arXiv preprint arXiv:2305.04764 (2023). [53] Junjielong Xu, Ruichun Yang, Yintong Huo, Chengyu Zhang, and Pinjia He. 2024. DivLog: Log parsing with prompt enhanced in-context learning. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1–12. [54] Tianyin Xu, Jiaqi Zhang, Peng Huang, Jing Zheng, Tianwei Sheng, Ding Yuan, Yuanyuan Zhou, and Shankar Pasupathy. 2013. Do not blame users for misconfigurations. In Proceedings of the 24th ACM Symposium on Operating Systems Principles . 244–259. [55] Tianyin Xu and Yuanyuan Zhou. 2015. Systems approaches to tackling configuration errors: A survey. Comput. Surveys 47, 4 (2015), 1–41. [56] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I Jordan. 2009. Detecting large-scale system problems by mining console logs. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles . 117–132. [57] Xin Yin, Chao Ni, and Shaohua Wang. 2024. Multitask-based evaluation of open-source llm on software vulnerability. IEEE Transactions on Software Engineering (2024). [58] Minchen Yu, Zhifeng Jiang, Hok Chun Ng, Wei Wang, Ruichuan Chen, and Bo Li. 2021. Gillis: Serving large neural networks in serverless functions with automatic model partitioning. In Proceedings of the 2021 IEEE 41st International Conference on Distributed Computing Systems . IEEE, 138–148. [59] Ding Yuan, Yinglian Xie, Rina Panigrahy, Junfeng Yang, Chad Verbowski, and Arunvijay Kumar. 2011. Context-based online configuration-error detection. In Proceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference . 28–28. [60] Zhiqiang Yuan, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, Xin Peng, and Yiling Lou. 2024. Evaluating and improving ChatGPT for unit test generation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1703–1726. [61] Jiaqi Zhang, Lakshminarayanan Renganarayana, Xiaolan Zhang, Niyu Ge, Vasanth Bala, Tianyin Xu, and Yuanyuan Zhou. 2014. EnCore: Exploiting system environment and correlation information for misconfiguration detection. In Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems . 687–700. [62] Yu Zhou, Weilin Zhan, Zi Li, Tingting Han, Taolue Chen, and Harald Gall. 2023. DRIVE: Dockerfile rule mining and violation detection. ACM Transactions on Software Engineering and Methodology 33, 2 (2023), 1–23. , Vol. 1, No. 1, Article . Publication date: November 2024.