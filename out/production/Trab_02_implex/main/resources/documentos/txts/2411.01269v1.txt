
[Página 1]
Disaggregated Database Management Systems⋆ Shahram Ghandeharizadeh1, Philip A. Bernstein2, Dhruba Borthakur3, Haoyu Huang4, Jai Menon5, Sumit Puri6 1USC, Los Angeles, CA, USA, shahram@usc.edu 2Microsoft Research, Redmond, WA, USA, phil.bernstein@microsoft.com 3Rockset, San Mateo, CA, USA, dhruba@gmail.com 4Google, Mountain View, CA, USA, haoyuhuang@google.com 5Fungible, Santa Clara, CA, USA, jai.menon@fungible.com 6Liqid, Broomfield, CO, USA, sumit@liqid.com Abstract. Modern applications demand high performance and cost effi- cient database management systems (DBMSs). Their workloads may be diverse, ranging from online transaction processing to analytics and de- cision support. The cloud infrastructure enables disaggregation of mono- lithic DBMSs into components that facilitate software-hardware co-design. This is realized using pools of hardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using high-speed networks. This disaggregation trend is being adopted by cloud DBMSs because hard- ware re-provisioning can be achieved by simply invoking software APIs. Disaggregated DBMSs separate processing from storage, enabling each to scale elastically and independently. They may disaggregate compute usage based on functionality, e.g., compute needed for writes from com- pute needed for queries and compute needed for compaction. They may also use disaggregated memory, e.g., for intermediate results in a shuf- fle or for remote caching. The DBMS monitors the characteristics of a workload and dynamically assembles its components that are most effi- cient and cost effective for the workload. This paper is a summary of a panel session that discussed the capability, challenges, and opportunities of these emerging DBMSs and disaggregated hardware systems. 1 Introduction Emerging data centers disaggregate hardware into pools of resources and con- nect them using fast networks such as high-speed Ethernet or Remote Direct Memory Access, RDMA [10]. The pool of resources may include CPUs, GPUs, memory, NVMe, disks, SSDs, FPGAs, and specialized hardware such as Amazon Trainium and Google TPU for machine learning among others. The software that implements a database management system may also be disaggregated into micro-services. Both trends facilitate a software-hardware co-design. Moreover, ⋆This paper appeared in the Performance Evaluation and Benchmarking - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW, Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1.arXiv:2411.01269v1 [cs.DB] 2 Nov 2024

[Página 2]
2 S. Ghandeharizadeh, et. al. they enable composition of micro-services into new services. For example, an in-memory key-value store may be realized using a subset of micro-services that implement a relational database management system (DBMS) [6]. Assembly of the components must consider the communication latency and employ caches to prevent this latency from dominating performance. Disaggregated DBMSs hold the potential to transform today’s obsolete prac- tices to enhance efficiency by providing sustainable solutions. Instead of asking a user to size a server, they may ask a user for their daily budget and de- sired performance objective. Now, it is the responsibility of an intelligent agent to assemble the hardware and software to meet the price and performance re- quirements. With a high (low) system load, the assembled system may scale-up (down). The system may use alternative forms of storage that provide different price/performance characteristics [7]. (a) A monolithic DBMS. File SystemBuffer Pool ManagerAbstraction of recordsIndex structuresQuery InterpreterQuery OptimizerÕQuery ParsersÈÇ File System High Speed SwitchQuery Cache (b) A disaggregated DBMS. Fig. 1: Today’s monolithic DBMS and the envisioned disaggregated DBMS. Physical data design is a task performed by data administrators to enhance efficiency and meet the performance requirements of an application. They make decisions such as whether the data is stored in a column format or a row format. However, many startups do not have a budget to pay these experts, resulting in inefficiencies. A disaggregated DBMS will address this by monitoring the system workload and fine-tuning the physical design of the database, choice of hardware including a storage hierarchy, and microservices assembled to realize the most cost effective deployment. Today’s use of data centers by scientists, including those in the area of ma- chine learning and AI, requires them to upload their data to a data center and run their computation in the cloud. A key question is what data to copy to the cloud? As an example, NIH’s-National Library of Medicine has 36.4 Petabytes of Genomic sequencing data on two commercial cloud platforms [15]. How are scientists to discover and use this data? Scientists want to know what is the minimum cost configuration for an experiment. They may have a fixed bud- get for running an experiment. Once the budget is exhausted, they may want

[Página 3]
Disaggregated Database Management Systems 3 to save their result files and have the service shut-down so there are no addi- tional charges [15]. They may also want to take a snapshot of their mid-flight experiment that enables them to continue where it was stopped. A vision for a future system is one that reduces data movement and repli- cation [15]. One way to realize this is to extend the disaggregation beyond a data center to include a scientist’s desktop. It provides for physical data inde- pendence, a concept pioneered by the database community, where the scientist is no longer burdened with the placement of data. It is the responsibility of the infrastructure to manage placement of data and computation seamlessly. The rest of this paper is organized as follows. Sections 2, 3 and 4 describe hardware, memory, and DBMS disaggregation in turn. Brief future research di- rections are presented in Section 5. 2 Hardware Disaggregation Storage, GPUs, memory, and other hardware resources are traditionally included as part of a traditional server. A limitation of this organization is the box that contains these resources. Success is effectively limited to what can fit in a box. To obtain enough of a critical resource, customers are forced to purchase larger, more expensive servers than required for new deployments or remove and replace a server when that critical resource is maxed-out. Disaggregating the hardware resources from the servers allows for much bet- ter utilization of these resources. There are three questions to answer when dis- aggregating resources: Which hardware resources are being disaggregated? What bus, fabric, or network to use to connect the disaggregated resources? And, what is the performance impact of disaggregation? Consider each question in turn. What hardware resources are being disaggregated? Storage has been disaggregated for many years. Storage products that disaggregate at the file level (NAS) and at the block level (SAN) have been available for several decades. GPUs and memory have not been disaggregated until very recently. Prod- ucts that disaggregate GPUs have become available in the last year. Memory disaggregation is emerging. What fabric is used for disaggregation? Storage disaggregation has been accomplished using Fiber Channel (FC), Ethernet, and InfiniBand (IB). Proto- cols such as SCSI over FC, NFS over Ethernet and SCSI over Ethernet (iSCSI) have been employed for this purpose. An emerging approach is NVMeoF for block level disaggregation. See https://nvmexpress.org/wp-content/uploads/NVMe Over Fabrics.pdf for details. GPU disaggregation has been realized using PCIe Fabrics and over Ethernet. Memory disaggregation is being attempted over emerging buses such as CXL (https://www.computeexpresslink.org/). It is also possible over Ethernet.

[Página 4]
4 S. Ghandeharizadeh, et. al. What is the performance impact of disaggregation? Disaggregation im- proves resource utilization, but it may result in some performance loss. The closer the performance of the disaggregated resource is to its performance when locally attached to the server, the more likely it is that disaggregation is employed. There is a tradeoff between using fabrics that allow data center wide disag- gregation (such as Ethernet) and those that support disaggregation over shorter distances. It is harder to achieve good performance over data center wide dis- tances. However, customers like the flexibility offered by disaggregation at scale. With new protocols such as NVMeoF, disaggregated storage performance close to that of server attached storage has been demonstrated, even at data center scale. Similarly, new approaches have also shown that disaggregated GPU performance can be anywhere from 80% to 99% of local GPU performance at data center scale over Ethernet. The next two sections present two approaches to hardware disaggregation. 2.1 Fungible’s DPU-Based Disaggregation A DPU (data processing unit) is a specialized programmable processor tailored to efficiently execute data-centric tasks. These are tasks that require stateful processing simultaneously on multiple high bandwidth streams of data. An in- creasing fraction of work done in modern data centers is data centric in nature, and CPUs and GPUs are inefficient at such tasks. Disaggregation is essentially a data centric task and DPUs have proven to be very efficient at disaggregation. As a result, an emerging trend is the use of DPUs for hardware disaggregation. Fig. 2: DPU-based disaggregated data center using standard IP networks. Fungible has built 2 DPUs [14],one small enough to fit on a PCIe card inside a server, a second one powerful enough to build a disaggregated storage system. Using the DPU, Fungible has built a disaggregated storage system with per- formance indistinguishable from that of server attached storage. This system is called the Fungible Storage Cluster (FSC). It has also built a DPU-based PCIe card that plugs into the PCIe slot of a standard server and disaggregates networking, security and storage functions transparently from the server. This allows server cores to be dedicated to running applications instead of being used inefficiently to run infrastructure tasks.

[Página 5]
Disaggregated Database Management Systems 5 Finally, it has developed a DPU-based disaggregated GPU appliance with performance between 80-99% of server attached GPUs. Customers can now run AI/ML on servers without any GPUs, and they have the flexibility to change the mix of CPU cores and GPU cores applied to a given problem. Fungible’s vision of the next-generation data center is shown in Figure 2. All servers have DPU based cards. Storage and GPUs are disaggregated and are built with DPUs. The result is a data center that costs about 30% of one without DPU-based disaggregation. It allows the disaggregated components to be dynamically composed on the fly to meet workload needs for great agility. Fig. 3: Cassandra DBMS: Fungible FSC vs. DAS. Figure 3 shows the performance of Fungible’s disaggregated storage system versus server attached storage, also known as Direct Attached Storage (DAS), for a database workload and shows that performance is indistinguishable. Additional details on Fungible’s storage offering using DPUs can be found in [12,13]. 2.2 Liqid’s Composable Disaggregated Infrastructure (CDI) Composable Disaggregated Infrastructure (CDI) solutions add a software com- ponent to hardware disaggregation. CDI solutions consist of three parts: the disaggregated hardware components, a fabric that connects the disaggregated hardware components, and software (sometimes referred to as a composer) that allows for dynamically configuring the hardware components to create hardware infrastructure that precisely matches workload needs. Instead of physically ordering hardware infrastructure with the required cores, memory, GPUs and Storage, CDI can dynamically compose and deploy such infrastructure in minutes. Liqid CDI disaggregates a datacenter’s infrastructure using PCIe-deployed devices. This includes GPU’s, FPGA’s, SSD’s, and Storage Class Memory. NIC’s are not installed in the server chassis. Instead, they are disaggregated, and placed into external PCIe enclosures, called expansion chassis.

[Página 6]
6 S. Ghandeharizadeh, et. al. An organization can have as many expansion chassis as are required to hold their storage, accelerators, and/or networking resources. Liqid is vendor agnostic enabling customers to choose what resources they compose. Visit https://www. liqid.com/resources/all for details. Liqid’s composable fabric switch then interconnects all the resources to be composed, providing every composable resource direct access to each other. For connections between chassis and racks to be transparent to workloads, Liqid leverages high bandwidth technology, including PCIe (Gen 4 and Gen 3). While Liqid also supports Ethernet and InfiniBand (Eth/IB) fabrics, they are not cov- ered in the scope of this document. Organizations choose the fabric type that best meets their composability requirements and can even create multi-fabric environments that support them all. All expansion chassis support PCIe and a subset support either PCIe or Eth/IB connectivity. Compute resources (servers/blades) are connected to the fabric via PCIe HBA and/ or 100GbE network cards. All composable resources connected to high-speed fabric switches, either PCIe or Ethernet. Once resources are disaggregated and connected over distributed fabric(s), the data center has essentially been flattened and turned into massive computing, accelerator, and storage pools. At this point, Liqid Matrix ™composable software is used to create bare metal servers composed of resources tuned to meet any workload need, in seconds. Liqid Matrix software lives on the fabric, allowing IT to configure and deploy servers that meet explicit workload requirements in seconds via software without worrying if a server can physically support its GPU and or storage resources. If demand increases, add more resources on-demand. As compute needs evolve, unused resources can be reclaimed for use by other applications. Liqid Matrix CDI software enables organizations to: –Accelerate time-to-results with right-sized systems, deployed real-time via software; –Adapt in real-time to evolving business needs with a zerotouch, change-ready agility; –Drive new levels of efficiency with superior resource utilization and an as-a- service approach to infrastructure; –Save on capital and operational expenses while providing a dynamic, disag- gregated infrastructure that is part of a more sustainable, software-defined data center ecosystem. Liqid has seen adoption for the following use cases: –AI+ML & data science: Accelerate time-to-research for scientists by enabling them to tailor systems that meet challenging workload needs in seconds, rather than forcing them to manually configure servers and accelerators. –HPC: Get answers to today’s most urgent questions faster by composing previously impossible server configurations for HPC. Quickly deploy sys- tems with precise amounts of GPU, accelerator, storage, and networking, eliminating the need to manually install or remove components from the server chassis.

[Página 7]
Disaggregated Database Management Systems 7 –End user computing: Meet the most demanding virtual desktop infrastruc- ture (VDI) requirements with Liqid CDI. Compose only what’s needed to meet today’s desktop requirements and then scale GPU resources up or down via software as workload demands dictate. –Server virtualization: Extend the flexibility of virtualization to VMware ESX host servers with Liqid CDI. Quickly compose bare-metal host servers that meet precise workload requirements all via the Liqid vCenter Plug-in. No longer is GPU performance capacity limited by what can fit in a server. Increase VM and workload density with Liqid. –Edge computing: Cameras, sensors, and cell phones will continue to create vast amounts of data; edge compute is increasingly needed to process that data quickly. Liqid composability creates flexible configurations that make edge deployments a reality. Liqid is uniquely suited to address common edge challenges including limited power, floor space, cooling and human access. Software-defined CDI enables IT organizations to reap cloud-like speed and flexibility in their own core and edge infrastructure. 3 Memory Disaggregation Stateful on-line applications maintain a large amount of data and require fast processing times. Examples include interactive games, fraud detection, and social networking, among others. These applications cache their data in memory to provide fast response time. If main memory were free, these applications would cache all their data in memory. However, memory is not free. Moreover, each server in the cloud has a limited amount of memory to offer an application. This limitation is exacerbated during peak system load of a stateful service when it requires additional memory to meet its Service Level Objective (SLO). Today’s data centers are awash in unused main memory [19,20]. By some estimates, more than 50% of data center memory is either un-allocated or unused at any given time [4,17]. One reason for this is the over-provisioning a virtual machine (VM) to handle the occasional VM resize operation without requiring a VM migration. Another is external fragmentation when VMs allocated on a host machine leave insufficient resources to allocate another VM of a useful size. A disaggregated hardware platform provides fast networking to enable data management systems, DMSs7, to access remote memory with acceptable latency, enhancing overall memory utilization. In [19], we present an elastic memory management system named Redy. Redy tunes RDMA in a particular deployment to satisfy a user-provided SLO and minimize resource cost. A future research direction is how to size the server-local memory cache to satisfy the required latency and throughput of a workload. This depends on the cache miss rate as a function of cache size and the latency of servicing a cache miss. The former depends on the degree of skew of references to data which in 7A DMS includes traditional relational database management systems, key-value stores, document stores, etc.

[Página 8]
8 S. Ghandeharizadeh, et. al. turn dictates the placement of data. The latter depends on the type of stor- age that services the miss. Example storage include remote memory, server-local disk, or cloud storage. This research direction may benefit from open industry standards such as Compute Express Link (CXL) for CPU-to-memory connec- tions. CXL is designed for high performance data center computers and provides cache-coherent protocols for accessing system memory (CXL.cache) and device memory (CXL.mem), and block input/output protocol (CXL.io). 4 Disaggregated Database Management Systems 4.1 AlloyDB Today’s applications have high demand on database performance and stringent service level agreement (SLA) requirements. An SLA may require a database to provide high throughput with a 90thpercentile response time less than a given threshold, e.g., 100 milliseconds. Applications also have a high velocity. They want to perform complex analytics on fresh data to provide the latest business insights. Op t imiz e d P o s t gr eSQL Op t imiz e d P o s t gr eSQL Prim ar y R e a d poo l nod e O nly L og W ri t e s Da t aba s e S t o r a g e E n gine G oogl e’ s Dis trib u t e d F il e S y s t em - C o l o ssu s C o lumn ar E n gine C o lumn ar F o rm a t R o w F o rm a t Ul tr a-f a s t C a che C o lumn ar E n gine C o lumn ar F o rm a t R o w F o rm a t Ul tr a-f a s t C a che Fig. 4: AlloyDB architecture.

[Página 9]
Disaggregated Database Management Systems 9 AlloyDB is a new enterprise grade SQL database product that aims at meet- ing those application demands. It combines PostgreSQL with compute-storage disaggregation, read pools for horizontal scalability, and HTAP support. Fig- ure 4 shows the architecture of AlloyDB, with the primary database instance, a set of read pools, and an intelligent, distributed storage engine as the key building blocks. AlloyDB disaggregates the primary and read pools from the database storage engine and enables each layer to scale independently of the others. An AlloyDB cluster consists of one primary and multiple read pools. A read pool consists of multiple read replicas. The storage engine persists data in the distributed file system, Colossus. Read pools isolate performance for different workloads of an application. An application may categorize its workloads and issue queries from the same category to the same read pool. For example, a wholesale retailer may create three read pools for its web sales, store sales, and catalog sales. This ensures that read queries for web sales are isolated from the queries for store sales. Read pools also provide horizontal scalability. A read pool balances the load across its replicas. An application may scale the number of replicas in a read pool dynamically based on the system load. The scaling is elastic and does not require moving data in the storage engine. When the system load becomes high, an application may create additional replicas to shed load. It may reduce the number of replicas in a pool when the load dials down. To provide fast query response time for both transactional and analytical queries, the primary and replicas employ an ultra-fast row cache and a pluggable columnar engine. The row cache caches the working set in row-oriented format while the columnar engine caches them in column-oriented format. Columnar engine uses single instruction, multiple data (SIMD) vectorization to facilitate query processing. It also monitors the workload and automatically columnarizes data to maximize performance. The primary and replicas process a query using the columnar engine, the cached data in row format, or a hybrid of two. AlloyDB’s storage engine is further disaggregated into several components to provide high performance. It also decouples durability from availability, see Figure 5. AlloyDB stores log records durably in the regional log storage and the database in the regional block storage, Colossus. The log storage optimizes for append-only log operations to reduce the transaction commit latency. To ensure all blocks are readily available for primary and replicas, the storage engine uses multiple log processing servers (LPS) that ingest log records and materialize blocks from log records continuously. LPS materializes the blocks in the same zones as the primary and replicas. They are purely compute-attached to a shared regional storage and can flexibly scale without needing to copy any data. Inter- nally, AlloyDB scales up the number of LPSs when the system load is high to provide consistent performance. It scales down during low system load to reduce cost. The storage engine also handles the backup operations completely and does not impact the performance and resources of the compute layer. AlloyDB also adopts a component-based architecture. It offers rich function- alities through multiple extensions to PostgreSQL. For example, the columnar

[Página 10]
10 S. Ghandeharizadeh, et. al. Fig. 5: AlloyDB storage engine. engine extension plugs into the query optimizer and query execution to facili- tate query processing. The database advisor extension provides physical database design recommendations based on the workload, e.g., indexes, and the query in- sights extension offers observability into the query performance. AlloyDB also supports widely used PostgreSQL extensions from the open-source community. 4.2 Rockset Real-time databases demand disaggregation: This section describes a dis- aggregated architecture for realtime databases. We examine, in detail, Rockset, which is a real-time analytics database service for processing low latency, highly concurrent analytical queries at scale. Rockset builds a Converged Index ™on structured and semi-structured data from OLTP databases, streams and lakes in real-time and exposes a RESTful SQL interface. We first discuss the require- ments of a disaggregated architecture for powering realtime data applications. We then explain why the Aggregator Leaf Architecture (ALT) is able to power realtime data applications. Finally, we present how Rockset disaggregates the RocksDB architecture to separate compute from storage. Many applications are powered by realtime databases. Examples include food ordered and tracked online, Facebook or LinkedIn’s newsfeed, and others. These applications require real-time analytics to provide interactive user requests with fresh data. This means fast queries with sub-second response times. New events are streamed into the database at hundreds of megabytes a second. Fresh data means new data becomes visible to queries within a few seconds of it being up- dated. The update rate to the database is inherently bursty in nature and, at the same time, these realtime data applications demand that queries are not im-

[Página 11]
Disaggregated Database Management Systems 11 pacted by bursty writes. This motivates processing of the writes to be separated from the processing of reads. This is the primary reason why disaggregation is key to powering realtime databases. Aggregator Leaf Tailer (ALT) is the dis- aggregated data architecture favored by web-scale companies, like Facebook, LinkedIn, and Google, for its efficiency and scalability for powering realtime applications. Fig. 6: Aggregator Leaf Tailer Architecture (ALT) The Aggregator Leaf Tailer disaggregated architecture (ALT): The ALT architecture [1] is a design pattern for realtime databases, see Figure 6. This architecture facilitates a three way disaggregation among the compute re- quired for writes, the compute required for reads, and the storage required to store and retrieve the data. The salient features of the ALT architecture include the following. First, the Tailer pulls new incoming data from a static or stream- ing source into an indexing engine. Its job is to fetch from all data sources, be it a data lake, like S3, or a dynamic source, like Kafka or Kinesis. Second, the Leaf is a powerful indexing engine. It indexes all data as it arrives via the Tailer. The indexing component builds multiple types of indexes—inverted, columnar, document, geo, and many others—on the fields of a data set. Its indexes expedite processing of queries that reference a data field. Third, the scalable Aggregator tier is designed to deliver low-latency aggregations, be it columnar aggregations, joins, relevance sorting, or grouping. The Aggregators leverage indexing so effi- ciently that complex logic typically executed by data-pipeline software in other architectures can be executed on the fly as a part of the query. The ALT architecture enables the application developer to run low-latency queries on raw data sets with minor prior transformation. A large portion of the data transformation process occurs as a part of the query itself. There are three

[Página 12]
12 S. Ghandeharizadeh, et. al. reasons why this is possible. First, indexing is critical to making queries fast. The Leaves, see Figure 6, maintain a variety of indexes concurrently, so that relevant data can be accessed quickly regardless of the type of query—aggregation, key- value, time series, or search. Every document and field is indexed, including both value and type of each field, resulting in fast query performance that allows significantly more complex data processing to be inserted into queries. Second, Queries are distributed across a scalable Aggregator tier. The abil- ity to scale the number of Aggregators, which provide compute and memory resources, allows compute power to be concentrated on any complex processing executed on the fly. Third, the Tailer, Leaf, and Aggregator run as discrete microservices in a dis- aggregated manner. Each Tailer, Leaf, or Aggregator tier can be independently scaled up and down as needed. The system scales Tailers when there is more data to ingest, scales Leaves when data size grows, and scales Aggregators when the number or complexity of queries increases. This independent scalability al- lows the system to bring significant resources to bear on complex queries when needed, while making it cost-effective to do so. The ALT architecture has been in existence for almost a decade, employed mostly on high-volume real-time data systems. Facebook’s Multifeed Architec- ture [5] has been using the ALT methodology since 2010, backed by the open- source RocksDB engine, which allows large data sets to be indexed efficiently. LinkedIn’s FollowFeed [9] was redesigned in 2016 to use the ALT architecture. Their previous architecture used a pre-materialization approach, also called fan- out-on-write, where results were precomputed and made available for simple lookup queries. LinkedIn’s new ALT architecture uses a query on demand or fan- out-on-read model using RocksDB indexing instead of Lucene indexing. Much of the computation is done on the fly, allowing greater speed and flexibility for developers in this approach. Rockset uses RocksDB as a foundational data store and implements the ALT architecture [16] in a cloud service. Disaggregation of compaction CPU in RocksDB in the cloud: Rockset uses RocksDB-Cloud as one of the building blocks of its distributed Converged Index. Rockset is designed with cloud-native principles, and one of the primary design principles of a cloud-native database is to separate compute from stor- age. Below, we describe how Rockset extends RocksDB-Cloud to realize this separation. This is open source software and may be adopted by other realtime databases. RocksDB-Cloud stores data in locally attached SSD or spinning disks. New writes to RocksDB-Cloud are written to an in-memory memtable. Once the memtable is full, it is flushed to a new SST file in the storage. Being an LSM storage engine, a set of background threads are used for compaction. Compaction is a process of combining a set of SST files and generating new SST files with overwritten keys and deleted keys purged from the new files. Compaction is a compute intensive task. It requires more resources with a higher rate of writes to the database, enabling the system to keep up with the new writes. See Figure 7.

[Página 13]
Disaggregated Database Management Systems 13 Fig. 7: RocksDB In a typical RocksDB-based system using a shared-nothing architecture [2,18], compaction occurs on CPUs that are local on the server that also hosts the stor- age. In this case, compute and storage are not disaggregated. Hence, if the write rate increases while the total size of the database remains unchanged, the system provisions more servers to process writes. It spreads the data across these addi- tional servers and uses their compute resources to keep up with the compaction load. This shared-nothing approach suffers from the following two limitations: First, Re-organizing data across additional servers is not instantaneous. If the workload changes during the re-organization process then the system may not benefit from the additional servers. Second, The utilization of storage capacity is lowered because the database size did not change. However, it is spread across additional servers with more storage. This lowers price-to-performance ratio due to unused storage on the servers. Next, we describe how Rockset addresses these two limitations by separat- ing compute from storage using disaggregated RocksDB-Cloud. The primary reason why RocksDB-Cloud is suitable for separating out compaction compute and storage is because it is an LSM storage engine. Unlike a B-Tree database, RocksDB-Cloud never updates an SST file once it is created. This means that all the SST files in the entire system are read-only except the miniscule portion of data in the active memtable. RocksDB-Cloud persists all SST files in a cloud storage object store such as Amazon S3. These cloud objects are safely accessible from all the servers because they are read-only. Thus, a RocksDB-Cloud server A may encapsulate a compaction job with its set of cloud objects and send the request to a remote stateless server B. Server B fetches the relevant objects from the cloud store, compacts them, writes a set of output SST files back to the cloud object store, and notifies server A that the compaction job is complete. In essence, the storage (which resides in server A) is separated from the compaction compute (which resides in server B). Server A has the storage and while server

[Página 14]
14 S. Ghandeharizadeh, et. al. B has no permanent storage but only the compute needed for compaction. This disaggregation is superior to the shared-nothing approach by eliminating its two limitations. 4.3 Nova-LSM IP Network HighSpeedSwitch LTCLogCCoordinatorApplicationNova-LSMclient ...............LTCLogCLTCLogC StoCLinux file system StoCLinux file systemLinux file system StoC... Fig. 8: Architecture. Nova-LSM is a distributed LSM-tree key-value store that disaggregates stor- age from processing [11]. Figure 8 shows its architecture, consisting of LSM- tree components (LTC), logging components (LogC), and storage components (StoC). These components are connected using a high speed RDMA network. Application data is range partitioned across LTCs and each LTC is assigned several ranges. An LTC maintains one LSM-tree for each of its assigned ranges and processes application requests using these trees. LogC maintains log records of a LSM-tree and is integrated into LTC. It generates log records when pro- cessing writes. It also fetches log records to recover a LSM-tree. StoC stores, retrieves, and manages blocks. A StoC may consist of main memory (DRAM), non-volatile memory, disk, or a hierarchy of these storage devices. It leverages one-sided RDMA read/write primitives to provide high performance. A StoC may implement compaction as data is shared across all StoCs. An LTC may write data to any StoC. Each write request uses power-of-d to dy- namically selects the fastest StoC. The coordinator maintains the assignment of ranges to LTCs to balance load. In [11], we present experimental results showing Nova-LSM provides 10x higher throughput than RocksDB [3] and LevelDB [8]. Nova-LSM is elastic. It may scale its number of StoCs and LTCs dynamically based on system load. When LTCs are fully utilizes, Nova-LSM may construct additional LTCs to shed load without the need to move data across StoCs. A new LTC is assigned one or more ranges and constructs its LSM-tree metadata to process client requests referencing its ranges. The LSM-tree reads data stored in a StoC. Figure 9 shows the throughput of a system as we increase and decrease

[Página 15]
Disaggregated Database Management Systems 15 Fig. 9: Nova-LSM is elastic. LTCs and StoCs may scale independently. the number of LTCs. The starting configuration consists of 1 LTC and 13 StoCs. Its peak throughput is 100,000 operations per second with the CPU cores of 1 LTC fully utilized. An increase in the system load motivates an increase in the number of LTCs, causing the peak throughput to increase linearly as the new LTCs process requests and reduce the load on the bottleneck LTC. As the system load decreases, the number of LTCs is reduced by removing one LTC at a time and re-assigning its assigned ranges to the other LTCs. This causes system throughput to drop back to 100,000 operations per second with 1 LTC. 5 Future Research Directions Disaggregated database management systems are an emerging research topic that raise many interesting questions. For example, given a workload, what is an online framework to assemble a DBMS using microservices? What hardware and software co-designs maximize its efficiency? How does one verify the correctness of a composition? Is it possible for a system to learn patterns that maximize ef- ficiency, enabling it to incorporate new hardware and services seamlessly? These and other research questions shape the future of disaggregated DBMSs. 6 Acknowledgments We thank Liqid’s Bob Brumfield and George Wagner for input on Section 2.2. References 1. Dhruba Borthakur. The Aggregator Leaf Tailer Ar- chitecture, Feb 6, 2019. See https://rockset.com/blog/ aggregator-leaf-tailer-an-architecture-for-live-analytics-on-event-streams/, 2019. 2. D. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H. Hsiao, and R. Ras- mussen. The Gamma Database Machine Project. IEEE Transactions on Knowledge and Data Engineering , 1(2), March 1990.

[Página 16]
16 S. Ghandeharizadeh, et. al. 3. Siying Dong, Mark Callaghan, Leonidas Galanis, Dhruba Borthakur, Tony Savor, and Michael Strum. Optimizing Space Amplification in RocksDB. In CIDR 2017, 8th Biennial Conference on Innovative Data Systems Research, Chaminade, CA, USA, Online Proceedings , Chaminade, California, USA, 2017. www.cidrdb.org. 4. Juncheng Gu et. al. Efficient Memory Disaggregation with Infiniswap. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 649–667, Boston, MA, March 2017. USENIX Association. 5. Facebook. Multifeed, Mar 10, 2015. See https: //engineering.fb.com/2015/03/10/production-engineering/ serving-facebook-multifeed-efficiency-performance-gains-through-redesign/. 6. Shahram Ghandeharizadeh, Haoyu Huang, and Hieu Nguyen. Nova: Diffused Database Processing Using Clouds of Components [Vision Paper]. In 15th Interna- tional Conference, BDAS 2019, Ustro´ n, Poland , volume 1018 of Communications in Computer and Information Science , pages 3–14. Springer, 2019. 7. Shahram Ghandeharizadeh, Sandy Irani, and Jenny Lam. On Configuring a Hier- archy of Storage Media in the Age of NVM. In 34th IEEE International Conference on Data Engineering, Paris, France, April 16-19, 2018 , pages 1380–1383, 2018. 8. Sanjay Ghemawat and Jeff Dean. LevelDB, https://github.com/google/leveldb, 2022. 9. Ankit Gupta. Linkedin Followfeed, Mar 31, 2016. See https://engineering.linkedin. com/blog/2016/03/followfeed--linkedin-s-feed-made-faster-and-smarter, 2016. 10. Haoyu Huang and Shahram Ghandeharizadeh. An Evaluation of RDMA-based Message Passing Protocols. In International Conference on Big Data (IEEE Big- Data), Los Angeles, CA, USA, December 9-12, 2019 , pages 3340–3349. IEEE, 2019. 11. Haoyu Huang and Shahram Ghandeharizadeh. Nova-LSM: A Distributed, Component-based LSM-tree Key-value Store. In ACM SIGMOD , 2021. 12. Jai Menon. Next Generation Storage will be Built with DPUs, Flash Memory Summit. August, 2022. 13. Jai Menon. Next Generation Storage will Use DPUs Instead of CPUs. Storage Developer Conference, Sept, 2022. 14. Wael Noureddine. The Fungible DPU: A New Category of Mi- croprocessor. See https://lp.fungible.com/hubfs/Assets/Whitepapers/ The-Fungible-DPU-A-New-Category-of-Microprocessor.pdf, 2021. 15. Executive Office of the President of the United States Machine Learning, Artifi- cial Intelligence Subcommittee of the National Science, and Technology Council. Lessons Learned from Federal use of Cloud Computing to Support Artificial In- telligence Research and Development. https://www.whitehouse.gov/wp-content/ uploads/2022/07/07-2022-Lessons-Learned-Cloud-for-AI-July2022.pdf, 2022. 16. Rockset. Whitepaper, June, 2022. See https://rockset.com/whitepapers/rockset- concepts-designs-and-architecture/, 2022. 17. Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang. LegoOS: A Dissem- inated, Distributed OS for Hardware Resource Disaggregation. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18) , pages 69–87, Carlsbad, CA, October 2018. USENIX Association. 18. Michael Stonebraker. The Case for Shared Nothing. Database Engineering , 1986. 19. Qizhen Zhang, Philip A. Bernstein, Daniel S. Berger, and Badrish Chandramouli. Redy: Remote Dynamic Memory Cache. Proc. VLDB Endow. , 15(4):766–779, 2021. 20. Qizhen Zhang, Philip A. Bernstein, Daniel S. Berger, Badrish Chandramouli, Vin- cent Liu, and Boon Thau Loo. CompuCache: Remote Computable Caching using Spot VMs. In 12th Conference on Innovative Data Systems Research, CIDR 2022, Chaminade, CA, USA, January 9-12, 2022 . www.cidrdb.org, 2022.