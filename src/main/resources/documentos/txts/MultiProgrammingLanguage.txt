
[Página 1]
Multi-Programming Language Sandbox for LLMs SHIHAN DOU1, JIAZHENG ZHANG1, JIANXIANG ZANG1, YUNBO TAO1, WEIKANG ZHOU1, HAOXIANG JIA2, SHICHUN LIU1, YUMING YANG1, ZHIHENG XI1, SHENXI WU1, SHAOQING ZHANG5, MULING WU1, CHANGZE LV1, LIMAO XIONG6, WENYU ZHAN6, LIN ZHANG6, RONGXIANG WENG6, JINGANG WANG6, XUNLIANG CAI6, YUEMING WU3, MING WEN4, RUI ZHENG1, TAO JI1, YIXIN CAO1, TAO GUI1, XIPENG QIU1, QI ZHANG1, XUANJING HUANG1∗ 1Fudan University,2Peking University,3Nanyang Technological University,4Huazhong University of Science and Technology,5Harbin Institute of Technology,6Meituan Inc. We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox designed to provide unified and comprehensive feedback from compiler and analysis tools for Large Language Models (LLMs). It can automatically identify the programming language of the code, compiling and executing it within an isolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox also integrates both traditional and LLM-based code analysis tools, providing a comprehensive analysis of generated code. MPLSandbox can be effortlessly integrated into the training and deployment of LLMs to improve the quality and correctness of their generated code. It also helps researchers streamline their workflows for various LLM-based code-related tasks, reducing the development cost. To validate the effectiveness of MPLSandbox, we integrate it into training and deployment approaches, and also employ it to optimize workflows for a wide range of real-world code-related tasks. Our goal is to enhance researcher productivity on LLM-based code-related tasks by simplifying and automating workflows through delegation to MPLSandbox. Additional Key Words and Phrases: large language models, software engineering, multi-programming language, open-source tool 1 INTRODUCTION Recently, researchers have become increasingly interested in the development of large language models (LLMs) for code-related tasks [ 51,69,75]. To improve the performance of LLMs in these tasks, some studies utilize sandboxes to compile and execute code, providing compiler feedback information for LLMs [ 52,60]. However, existing sandbox tools tend to serve mono-programming languages and are also not conveniently integrated into the training and deployment processes of LLMs [ 11,14,16,36]. The lack of well-developed multi-language sandbox environments significantly limits the application of LLMs in tasks involving multiple programming languages. Additionally, comprehensive code analysis, such as code smell, fuzz testing, and execution efficiency, assists LLMs in better understanding the code, thereby improving the quality of the generated code and enhancing their effectiveness for downstream tasks [ 34,62,82]. However, the diversity in analytic tools and their distinct applications increases the complexity of their utilization and development for researchers. Moreover, the same type of analysis tool often varies across different programming languages [ 39,66]. In a multi-programming language context, this variation in analysis tools across different languages significantly increases the cost for researchers to develop and employ these tools to enhance the performance of models in code-related tasks. To address these challenges, we propose MPLSandbox, an out-of-the-box sandbox designed to provide unified compiler feedback across multiple programming languages. Additionally, it integrates traditional code analysis tools, delivering comprehensive code information to LLMs from numerous perspectives. MPLSandbox simplifies code analysis for researchers, and can be seamlessly integrated into LLM training and application processes to enhance the performance of LLMs in a ∗Tao Gui and Xuanjing Huang are the corresponding authors. Shihan Dou, Jiazheng Zhang, and Jianxiang Zang con- tributed equally. Jiazheng Zhang, Shaoqing Zhang, and Muling Wu are interns at Meituan LLM Team. MLPSandbox has been used for large-scale training and various downstream code-related tasks at Meituan Inc. Contact information: shdou21@m.fudan.edu.cn, haoxiangjia@pku.edu.cn, wengrongxiang@meituan.com, and tgui@fudan.edu.cn Preprint. Publication date: November 2024.arXiv:2410.23074v2 [cs.SE] 5 Nov 2024

[Página 2]
Multi-Programming Language Sandbox for LLMs range of code-related tasks. MPLSandbox consists of three core modules: the “Multi-Programming Language Sandbox Environment” which provides unified compiler feedback by compiling and executing the code; the “Code Analysis Module” which includes multiple traditional analysis tools to offer a comprehensive analysis report from numerous perspectives; and the “Information Integration Module” which integrates compilation feedback and various analysis results to accomplish a range of complex code-related tasks. For the first module ( i.e.,the multi-programming language sandbox environment), the code and unit test samples are sent to the sub-sandbox of the corresponding programming language for isolated execution to obtain compiler feedback. The sandbox ensures the program executes safely without jeopardizing the external environment or interrupting the training process [ 14,16]. The second module ( i.e.,the analysis module) provides a comprehensive code analysis from multiple perspectives, such as static analysis ( e.g.,potential bug detection [ 43] and code smell analysis [ 59,70]) and dynamic analysis ( e.g.,fuzz testing [ 40] and efficiency analysis [24,53]). Additionally, this module can also assess other input information besides the code, such as evaluating the coverage of unit tests for the code, aiding researchers in improving the quality of these unit tests. Finally, the third module ( i.e.,the information integration module) integrates these results for LLMs to improve the quality of generated code and enhance their performance on a range of code-related tasks. Specifically, the features of our proposed MPLSandbox include: •Security and Stability. MPLSandbox constructs sub-sandboxes for each programming lan- guage, ensuring that programs are compiled and executed in isolation from the training environment. This setup also prevents LLM-generated code that contains malicious vulnera- bilities or bugs harms the external environment. Additionally, various vulnerability and bug detection tools are integrated into the static analysis module to further ensure safety. •Multi-programming language support. We are the first to propose a multi-programming language sandbox. MPLSandbox can automatically identify the programming language of the code, post it to the corresponding sandbox environment, and effortlessly and thoroughly analyse the code using numerous analysis tools. It significantly reduces the development cost for researchers in developing and deploying LLMs for a range of code-related tasks. •Usability and Extensibility. MPLSandbox integrates multiple analysis tools for each pro- gramming language, and users can also effortlessly design tool templates to integrate their tools into MPLSandbox. Furthermore, users can easily construct prompt templates to combine compiler feedback and analysis results to enhance LLM’s performance in code-related tasks. •Distributed Architecture. MPLSandbox is designed for distributed deployment. In large- scale training scenarios, training nodes can access the optional MPLSandbox nodes. Compared to deployments where both training nodes and sandbox nodes are co-located on a single machine, MPLSandbox offers greater efficiency. To validate the effectiveness of MPLSandbox, we conducted extensive experiments involving training, production, and deployment scenarios by using reinforcement learning [ 52], Best-of-N (BoN) [ 87], and self-correction [ 54]. The experimental results demonstrate that MPLSandbox can be easily integrated into various application scenarios to enhance the correctness and quality of generated code. In addition, we explored the application of MPLSandbox in several real-world code-related tasks, including unit test generation, bug fixing, vulnerability localization, and code translation. By facilitating the easy integration of analysis tools and the flexible combination of different modules, MPLSandbox significantly enhances the performance of LLMs in code-related tasks, while maintaining low development costs for researchers. MPLSandbox is the first to offer a multi-programming language sandbox that simplifies the complexity of employing LLMs in code FudanNLP 2

[Página 3]
Multi-Programming Language Sandbox for LLMs tasks and provides opportunities for their improvement. Our tool can help drive further research in this area. Programming ProblemGiven an array of n integers, find the largest element of it.Programming Language (Optional)PythonCodedef find_max(nums):max_num= nums[0]for num in nums:if num > max_num:max_num= numreturn max_numUnit Test SamplesInputs: ([5, 4, 3, 2, 1], )Outputs: (5, ) Code Basic Information… …(1) Multi-Programming Language Sandbox EnvironmentMaster(2) Analysis ModuleCode Smell AnalysisCode Bug AnalysisUnit Test AnalysisCode Efficiency Evaluation AST:CFG:Code StyleCode SpecificationsDuplicate codeCyclomatic ComplexityMaintainability IndexHalstead Complexity...Unit Test Coverage:Test Suite Quality Metrics... ...Static Analysis:Rule-based AnalysisControl Flow AnalysisData Flow AnalysisDynamic Analysis:Fuzzing TestMemory Leak Detection...line-by-line performance analysis:def find_max(nums):0max_num= nums[0]0for num in nums:0if num > max_num:0max_num= num0return max_numPerformance Profilingdef find_max(nums):max_num= nums[0]for num in nums:if num > max_num:max_num= numreturn max_numInput Programming LangaugeSandbox TemplateTool Template (3) Information Integration ModuleSystem Prompt Template def find_max(nums):...else:left_half= nums[:n//2]right_half= nums[n//2:]left_max= find_max(left_half)right_max= find_max(right_half)if left_max> right_max:return left_maxelse:return right_maxpublic static int findMax(int[] nums) {...} else {int[] leftHalf= Arrays.copyOfRange(nums, 0, n / 2);int[] rightHalf= Arrays.copyOfRange(nums, n / 2, n);int leftMax= findMax(leftHalf);int rightMax= findMax(rightHalf);...}}Unit Test Generation Other Tasks ...Various Code-related TasksCode Optimization Code TranslationTSelf-Correct and Self-RefinementVulnerability LocationImproving Code atTestTimeRL from Compiler Feedback Fig. 1. The architecture of MPLSandbox. It comprises three core modules: (1) Multi-Programming Lan- guage Sandbox Environment, (2) Code Analysis Module, and (3) Information Integration Module. The Multi- Programming Language Sandbox Environment can provide unified compiler feedback by compiling and executing the code. The Code Analysis Module contains multiple traditional analysis tools to offer a com- prehensive analysis report from numerous perspectives. The Information Integration Module integrates compilation feedback and various analysis results to accomplish a range of complex code-related tasks. 2 MPL-SANDBOX In this section, we introduce the architecture and pipeline of our proposed MPLSandbox. 2.1 Architecture MPLSandbox is an out-of-the-box multi-programming language sandbox designed to provide unified compiler feedback and comprehensive code analysis for LLMs. It enables researchers to thoroughly analyze LLM-generated code in any programming language using sandbox and various code analysis tools, significantly reducing development costs. Additionally, MPLSandbox can streamline LLMs’ training and deployment workflows for various code-related tasks. The architecture of MPLSandbox is shown in Figure 1. It receives four inputs: a description of the programming problem, the code to be analyzed, the programming language used (optional), and unit test samples. If the user or LLMs do not specify the programming language of the code, the code is initially processed through both a rule-based and a neural network model-based language parser to determine the programming language. The classification error rate of this combined classifier is less than 0.1% on a dataset of 10 million lines of code. Subsequently, the code is comprehensively analyzed by various modules within MPLSandbox. MPLSandbox comprises three core modules: (1) Multi-Programming Language Sandbox Environment, (2) Code Analysis Module, and (3) Information Integration Module. Multi-Programming Language Sandbox Environment. Based on the programming language specified for the code, the module first sends the code and unit test samples into the sub-sandbox of the corresponding programming language. It then securely compiles and executes the code. The sub-sandbox for compilation and execution is a container isolated from the main sandbox environment to prevent potential vulnerabilities and viruses in the code from jeopardizing the external environment during execution. Additionally, the sub-sandbox is configured with resource FudanNLP 3

[Página 4]
Multi-Programming Language Sandbox for LLMs constraints, such as maximum memory limit, maximum execution time, and maximum PIDs limit, to prevent the code from consuming excessive resources that could crash the sandbox environment. To further ensure the stability of the sandbox during LLM training and deployment, a driver node continuously monitors the state of the sandbox node in real-time and can automatically restart it in case of a crash due to unknown reasons. The sandbox environment also monitors and analyzes runtime and resource usage during program execution (detailed in the Code Analysis Module). Additionally, each programming language sub-sandbox has pre-installed with accordingly widely used dependency libraries. Users can also write a configuration file to effortlessly install additional libraries referenced by the code. The sandbox environment can also report missing libraries based on compiler feedback, allowing users to easily identify and install the required dependency libraries. We have predefined eight commonly used programming languages in the sandbox environment, including Python, Java, C++ (C), C#, Bash, Go, JS, and TS. Expanding to additional, unintegrated programming languages is also straightforward. Users can create their own sub-sandbox and seamlessly integrate it into the sandbox environment. Table 1. Overview of code analysis tools across various programming languages and tool types. JS and TS denote JavaScript and TypeScript, respectively. Type Python Java C++ (C) C# Bash Go JS TS Basic Information AnalysisASTPretty & PyflowchartJavalang & SootClang Roslyn -GoAst Viewer & AngrJoern Ts-morph Code Smell AnalysisPylint & RadonPmd CPPCheck StyleCop.Analyzers ShellCheck golangci-lintESLint & ShkjemESLint & TSLint Code Bug Analysis Bandit CheckstylePVS-Studio & CPPCheckSonarQube Shellcheckgovulncheck & gosecNodeJsScan Snyk Unit Test Analysis Coverage Jacoco GCOV Coverlet shcov gocov Istanbul Istanbul Code Efficiency Evaluation Line_profile Jprofile Benchmark.NET BenchmarkDotNet bashprof pprof V8 Profiler V8 Profiler Code Analysis Module. In this module, we integrate numerous traditional analysis tools to offer a comprehensive analysis report for the code from various perspectives. Additionally, this module can also assess other input information besides the code, including evaluating the coverage of unit tests for the code, aiding researchers in improving the quality of their provided unit test samples. We categorized the tools into five categories based on their purpose and analysis results: (1) basic information analysis, (2) code smell analysis, (3) code vulnerability analysis, (4) unit test analysis, and (5) code efficiency evaluation. Specifically, (1) Basic information analysis , including Abstract Syntax Trees (AST) and Control Flow Graphs (CFG), provides LLMs with detailed information on code structure and semantics, and help them better code comprehension. This information enhances LLM performance in tasks such as code completion, refactoring, security analysis, and code translation. (2) Code smell analysis identifies patterns in code that may indicate issues affecting maintainability, readability, and extensibility, such as code complexity, overengineering, and duplicated code. It can significantly assist LLMs in various code-related tasks by improving code quality, aiding in code reviews by identifying potential issues, offering refactoring suggestions for cleaner code, and enhancing code understanding through contextual and structural insights. (3) Code bug analysis is essential in software development for ensuring quality and stability, and it comprises both static and dynamic analysis. Static analysis detects errors and vulnerabilities without executing code, while dynamic analysis, including fuzz testing, identifies runtime issues. These tools enhance LLMs by improving security, aiding debugging, and generating comprehensive documentation, making code more reliable. (4) Unit test analysis involves evaluating the effectiveness and coverage of user-provided unit tests to ensure code quality and reliability. Tools for unit test analysis can help LLMs identify uncovered code lines, generate new test cases, diagnose errors, and offer code quality suggestions, FudanNLP 4

[Página 5]
Multi-Programming Language Sandbox for LLMs thereby making the development and testing process more efficient and automated. (5) Code efficiency evaluation involves assessing code performance and resource utilization by analyzing aspects such as time and space complexity, line-leve execution time, and resource usage. Code efficiency evaluation can be integrated into LLMs to enhance them in code-related tasks including identifying inefficiencies, pinpointing bottlenecks, providing optimization suggestions, enabling automated improvements, and offering continuous feedback. In each category of code analysis tools, we integrate commonly used tools for each programming language, as shown in Table 1. Additionally, users can effortlessly embed their analysis tools by writing tool templates. These tools provide more comprehensive information about the code, and their analysis results help LLMs better understand the code. The synergistic combination of these tools and LLMs can enhance the performance of LLMs in various code-related tasks. We have demonstrated the ease of use and applicability of MPLSandbox by integrating it into several tasks, as detailed in Section 4. Information Integration Module. This module can integrate the compiler feedback from the Multi-Programming Language Sandbox Environment, and various code analysis results from the Code Analysis Module into LLM, to improve the quality of generated code and accomplish a range of complex code-related tasks. Specifically, MPLSandbox includes rich templates that feed the compilation results and analysis outputs to LLMs through prompt learning. Users can also easily construct custom prompt templates by combining these results. This capability streamlines LLM workflows in various downstream code-related tasks, to reduce development costs. For example, users can enable LLMs to generate more diverse and comprehensive unit test examples based on the results of unit test analysis and compiler feedback. They can also improve code translation performance by leveraging various structural, semantic, and execution information of the code. 2.2 Pipeline UserMPLSandbox Training NodeDriver Node MPLSandboxTraining and EmploymentUser CallsProgramming Problem, LLM-generated Code, Programming Language Class (Optional), Unit Test SamplesUnified Compiler FeedbackTraditional analysis outputsLLM-based analysis outputs Fig. 2. The pipeline of MPLSandbox. It can be deployed as a standalone system for users or a few LLMs, or as a distributed system for large-scale LLMs’ training and deployment. MPLSandbox can be deployed either as a standalone system, serving users or a small number of LLMs, or as a distributed system, serving large-scale training and deployment scenarios. The pipeline of MPLSandbox in these two scenarios is illustrated in Figure 2. Specifically, users can deploy FudanNLP 5

[Página 6]
Multi-Programming Language Sandbox for LLMs MPLSandbox on their personal computers or remote servers. They can easily invoke MPLSandbox through an IP address and port number to leverage compilers and various analysis tools for comprehensive analysis and evaluation of LLM-generated code cases. Additionally, users can integrate MPLSandbox into small-scale LLM training and deployment workflows to enhance their effectiveness. For instance, MPLSandbox can be integrated into the deployment phase to improve the accuracy and quality of generated code using Best-of-N (BoN). It can also be incorporated into the reinforcement learning (RL) training phase by utilizing compiler feedback as an external supervised signal. Furthermore, MPLSandbox can optimize workflows for various downstream code-related tasks, as detailed in Appendix E. In addition, MPLSandbox can also be easily integrated into large-scale distributed training and deployment environments. Specifically, we deploy multiple sandbox node servers and manage them centrally through a driver node. Sandbox nodes can be custom-assigned to training nodes to provide services. To prevent sandbox nodes from causing memory and CPU pressure on the training nodes, sandbox nodes and training nodes are deployed separately. MPLSandbox streamlines the workflow of large-scale LLM training and deployment, thereby researchers’ development time effectively. 3 USAGE MPLSandbox is designed to be flexible enough to allow researchers to configure the workflow and integrate their analysis tools, while providing appropriate abstractions to alleviate the concerns of the low-level implementation. It is ready-to-use and can be easily invoked with just a few lines of code. 3.1 Initialization Firstly, the tool supports initialization as an executor through the following method: from MPLSandbox import MPLSANDBOX config = './path/to/your/config.json' # or a dictionary executor = MPLSANDBOX(config) where the config.json file is a configuration file that contains fields such as questions, model- generated code, input and output for unit testing, language types, etc. Of course, this tool also supports passing configurations directly as a dictionary. Figure 4 in Appendix A shows the detailed configuration for initialization. 3.2 Run Next, this executor can be run through the following method: report = executor.run(analysis="all") # support selecting specific analysis, e.g. analysis = "code_smell_analysis" The executor will first call the Code Analysis Module to obtain the 5 types of code analysis information, and then integrate this information through the Information Integration Module before returning it to the user. analysis supports users in defining the code analysis information they need to obtain (default returns all 5 types’ information). The Appendix A presents detailed case studies demonstrating the five types of analyses performed. 4 APPLICATIONS In this section, we introduce code datasets, baseline models, and implementation details of our experiments. Then, we present MPLSandbox in three application scenarios to improve the quality FudanNLP 6

[Página 7]
Multi-Programming Language Sandbox for LLMs of LLM-generated code: (1) acting as a verifier for inference, (2) providing compiler feedback for reinforcement learning, and (3) offering code analysis for self-correction and code optimization. We also provide more application scenarios and cases of MPLSandbox in a wide range of code-related tasks in Appendix E. 4.1 Datasets To validate the effectiveness of MPLSandbox, we conduct all experiments on the TACO dataset [55], which contains two widely used datasets, i.e.,APPS [ 44] and CodeContests [ 56], and a portion of the newly crawled data from the contest sites. APPS is a code generation benchmark collected from open-access sites, including Codewars, AtCoder, Kattis, and Codeforces. CodeContests is a comprehensive collection of competitive programming problems, designed to facilitate research and development in the fields of code synthesis. For the APPS part of TACO, we replace the data with APPS+ [ 32], which is the curated version of APPS. It excludes instances lacking input, output, or canonical solutions, and eliminates issues such as incomplete code and syntax errors. The final dataset contains 12,000, 1,000, and 1,000 programming problems for training, validation, and testing, respectively. We can train and evaluate LLMs under different programming languages by modifying the system template, as illustrated in Appendix B. 4.2 Models To validate our tool, we integrate it into a wide range of LLMs, including DeepSeek-Coder-Instruct- 6.7B [ 42], DeepSeek-Coder-V2-Lite-Instruct-16B [ 89], Qwen2.5-Coder-1.5B-Instruct [ 78], Qwen2.5- Coder-7B-Instruct [ 78], Codestral-v0.1-22B [ 12], Llama-3.1-Instruct-70B [ 35], and GPT-4o [ 68]. DeepSeek-Coder-Instruct-6.7B and DeepSeek-Coder-V2-Lite-Instruct-16B are trained by DeepSeek. Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B are developed by the Qwen group. Codestral-v0.1-22B and Llama-3.1-Instruct-70B are developed by Mistral and Meta, respectively. GPT-4o is a widely used closed-source LLM developed by OpenAI. The details of these LLMs are provided in the Appendix C.1. 4.3 Implementation Details First, we utilize MPLSandbox as a verifier to verify LLM-generated code at inference time. We employed the Best-of-N (BoN) strategy to sample LLM’s responses multiple times for the same programming problem, and then evaluate the code using Pass@k metric [ 26]. In the Pass@1 and Pass@10 settings, the sample temperature is set to 0.2 and 0.8, respectively. All inference experiments are conducted on a single node equipped with eight NVIDIA A100-80G GPUs. Second, we use the compiler feedback provided by MPLSandbox as a supervised signal to optimize LLMs through reinforcement learning. We use DeepSeek-Coder-Instruct-6.7B [ 42] as the foundation LLM. In each programming language experiment, we modify system prompt templates to enable LLM to solve problems using the according programming language. All training experiments are conducted on 16 training nodes, totaling 128 NVIDIA A100-80G GPUs, and two MPLSandbox nodes. The global batch size is set to 512. The system prompt template used for constructing multi-programming language instructions is illustrated in Appendix B. The global training step is set to 8000 , with a 0.1 ratio of warmup. We report the accuracy point on the test set using the checkpoint at which the model achieves its best performance on the validation set. The learning rate for the policy model and the critic model is 5𝑒−7and1.5𝑒−6, respectively. For each example, we collect a 16roll-out code using nucleus sampling. The sampling temperature is set to 0.8, top-p is set to0.9, and the maximum output token length is set to 2048. The token-level KL penalty coefficient 𝛽is set to 0.02, with a clip value of 0.8. In the decoding phase, the temperature and top-p are set to 0.2and0.95, respectively. FudanNLP 7

[Página 8]
Multi-Programming Language Sandbox for LLMs Finally, through self-reflection and self-correction, we utilize LLMs to correct their generated errors, and improve the quality of codes using the results of analysis tools. The system prompt templates are shown in Appendix B. The temperature is set to 0.8. 4.4 Results To showcase our tool, we evaluate it in three application scenarios, to improve the quality of LLM-generated code and help users streamline LLM workflows in various downstream code-related tasks. 4.4.1 MPLSandbox as a Verifier at inference time. First, we integrate MPLSandbox into the deploy- ment environment of LLMs to act as a verifier, using the multi-programming language sandbox environment to verify the correctness of generated code at inference time. The experimental results are illustrated in Table 2. The results demonstrate that MPLSandbox can efficiently verify the correctness of model-generated code in multiple programming languages. Moreover, the Pass@10 results significantly outperform the Pass@1 results, indicating that MPLSandbox can be stably integrated into various LLMs to provide reliable verification and feedback. Table 2. Performance of LLMs on multi-programming languages integrated with MPLSandbox. JS and TS denote JavaScript and TypeScript, respectively. DS denotes DeepSeek. The results indicate that our tool can provide reliable verification and feedback. Model Size Pass@K Python Java C++ (C) C# Go Bash JS TS Qwen2.5-Coder-Instruct 1.5BK=1 2.4% 2.8% 2.8% 0.4% 1.1% 0.0% 0.4% 0.4% K=10 13.9% 4.9% 8.5% 7.3% 4.9% 4.5% 2.4% 1.7% Qwen2.5-Coder-Instruct 7BK=1 7.0% 14.3% 11.9% 11.5% 3.5% 4.9% 9.1% 3.8% K=10 24.7% 23.7% 32.1% 28.6% 23.7% 20.6% 25.4% 17.8% DS-Coder-Instruct 6.7BK=1 9.4% 10.5% 9.1% 8.0% 3.8% 2.4% 7.0% 3.1% K=10 23.7% 24.7% 22.3% 25.1% 21.6% 16.4% 21.6% 15.3% DS-Coder-V2-Lite-Instruct 16BK=1 29.6% 26.8% 25.1% 23.7% 10.5% 5.6% 12.9% 8.0% K=10 50.2% 47.7% 44.6% 42.9% 35.5% 19.9% 39.4% 25.1% Codestral-v0.1 22BK=1 9.8% 21.3% 22.0% 20.2% 12.2% 10.1% 9.8% 7.0% K=10 34.2% 41.8% 38.7% 41.1% 34.8% 28.9% 34.8% 28.6% Llama-3.1-Instruct 70BK=1 15.0% 17.4% 15.7% 13.2% 6.6% 7.4% 9.4% 6.1% K=10 38.0% 38.3% 34.5% 35.5% 35.5% 17.1% 33.5% 14.6% GPT-4o -K=1 39.3% 47.4% 46.3% 16.0% 43.6% 33.8% 44.6% 40.4% K=10 52.6% 68.6% 65.9% 47.4% 64.5% 58.2% 66.2% 63.4% This integration simplifies various real-world deployment scenarios, including code evaluation, code data production and filtering, and automated testing. In practice, we can deploy multiple sandbox nodes where users can verify the correctness of the code and obtain feedback. Data production teams can utilize MPLSandbox to filter LLM-generated code in bulk. Additionally, MPLSandbox can be integrated into various evaluation environments to provide compiler feedback. 4.4.2 MPLSandbox for Reinforcement Learning with Compiler Feedback. We further leverage the compilation feedback provided by MPLSandbox as a feedback signal for reinforcement learning to improve the quality of code generated by LLMs. The reinforcement learning algorithm for code generation is detailed in Appendix C.2. The experimental results, shown in Figure 3, demonstrate that compiler feedback provided by MPLSandbox is effective and stable, significantly enhancing LLM performance in code generation. Users can access MPLSandbox with just a few lines of code to obtain compiler feedback for their code. This compiler feedback is then standardized into unified signals regardless of the programming language. Users can also customize the rules to define the numericalization method. FudanNLP 8

[Página 9]
Multi-Programming Language Sandbox for LLMs Python Java C++ (C) C# Go Bash JS TS Programming Language024681012Accuracy (%)Baseline PPO Fig. 3. Multi-programming language results of baseline and model trained by PPO. JS and TS denote JavaScript and TypeScript, respectively. We use DeepSeek-Coder-Instruct [ 42] as our foundation model and report the Pass@1 results for all programming languages. Through MPLSandbox, users can easily obtain reliable compiler feedback and effortlessly streamline their LLM training workflow. In summary, MPLSandbox allows users to avoid focusing on trivial tasks such as isolating and building a multi-programming language execution environment. By simply invoking MPLSandbox, users can obtain robust compiler feedback, enabling them to devote more time to developing and optimizing their training algorithms. 4.4.3 MPLSandbox for Self-Correction and Code Optimization. Self-correcting and optimizing codes generated by LLMs is crucial but often complicated and tedious. This process requires providing LLMs with detailed information about code errors and various characteristics of the code, such as complexity, execution efficiency of each line, and adherence to coding standards. Achieving this involves developing a sandbox environment, utilizing various analysis tools, and integrating their results. With MPLSandbox, users can effortlessly compile and analyze LLM-generated code. You can also easily customize the system prompt template to combine these results in various ways. As a demonstration, we utilize compiler feedback to enable GPT-4o to correct erroneous code and code smell analysis results to refine the correct code, respectively. The system prompts are shown in Appendix B. Table 3. Improvement results after self-correction and self-refinement show significant enhancements Metric Python Java C++(C) C# Go Bash JavaScript TypeScript Pass@1 +3.7% +4.9% +2.7% +6.5% +5.0% +4.8% +4.1% +3.1% Avg num of comment lines +8.9 +7.0 +7.2 +5.2 +5.6 +4.3 +7.9 +6.3 The experimental results, as shown in Table 3, show that by using MPLSandbox, GPT-4o can effectively and efficiently correct and refine its generated code. Through self-correction and self- refinement, GPT-4 can solve more programming problems, and the generated code includes more comments. Moreover, the generated code exhibits lower complexity and is more compliant with programming specifications, as detailed in Appendix D. This will help developers understand and maintain the programs generated by LLMs. The results demonstrate that our tool enables users to implement LLM self-correction and optimization easily. This can assist users in serving downstream tasks more conveniently and efficiently, such as improving the quality of LLM-generated code at inference time and producing reflection data for training. We also provide more application scenarios and cases of MPLSandbox in a wide range of code- related tasks in Appendix E, including unit test generation, vulnerability localization, and code FudanNLP 9

[Página 10]
Multi-Programming Language Sandbox for LLMs translation. These cases and results indicate that MPLSandbox is effective for most workflows, significantly reducing the user’s development effort. 5 RELATED WORK In this section, we review existing works on LLMs used for code-related tasks, and introduce existing code analysis tools. 5.1 Large Language Models for Code Recently, the advancement of LLMs [ 1,2,10,17,20,21,23,26,29,45,63,68,79] has significantly propelled the field of software engineering [ 31,48,67,84]. For instance, in code generation and program repair, state-of-the-art approaches improve the correctness and quality of LLM-generated codes by learning from compiler feedback. Specifically, researchers integrate compiler information into prompt templates to improve the performance of LLMs on these code-related tasks [ 30,46,51, 71,75]. Some work also transforms compiler information into feedback signals to optimize LLMs to enhance their performance [ 32,52,60,76,86]. For security, stability, reliability, and providing robust monitoring capability, these compilation and execution processes are needed in an isolated sandbox [ 38,42,57]. However, the development of open-source sandboxes is still in its early stages. Meanwhile, existing sandboxes developing for LLM-generated code mostly focus on a single programming language, such as Python [ 14,16,36] or a few programming languages [ 11], which lacks numerous commonly used dependency libraries. The absence of an easy-to-use sandbox necessitates that researchers spend considerable time on installing environments and dependency packages, as well as constructing a distributed sandbox to support multiple programming languages. Some researchers also combine compiler feedback with other analysis results of generated codes to enhance LLM’s performance on more code-related tasks [ 18,22,49,58,61,72]. For instance, some work focuses on developing LLM-based software vulnerability detection approaches by using compiler feedback and traditional code features [ 34,62]. Some work also aims to enhance LLM’s performance in generating more diverse unit test samples ( i.e.,software test generation) [27,41,73], generating more comprehensive code specification ( i.e.,specification generation) [48,65,88], optimizing the code snippets ( i.e.,code efficiency optimization) [ 33,37], summarizing the code with natural language ( i.e.,code summary) [ 50,67,80], and translating code from one programming language to another programming language ( i.e.,code translation) [25, 69, 85]. The application of LLMs to code-related tasks necessitates leveraging a vast array of traditional analysis tools from the field of software engineering. Researchers will expend much time and effort on trivial work such as constructing the environment and resolving versioning and dependency issues. An out-of-the-box framework that integrates a multilingual isolated compilation and execution sandbox with multilingual analysis tools remains unexplored. 5.2 Program Analysis Tools The analysis of structural information, intermediate variable flow during execution, and resource usage can help researchers comprehensively understand and improve the quality of code [73, 81]. For instance, various tools are employed to obtain code structure information and code smells, such as abstract syntax trees (AST) ( e.g.,Python’s ASTPretty [ 3] and Java’s Javalang [ 6]), control flow graphs (CFG) ( e.g.,C++’s Clang [ 4] and JS’s Joern [ 7]), complexity measurement tools ( e.g., Python’s Radon [ 15] and Java’s Pmd [ 13]), and unit test coverage analysis tools such as Coverage [5]. Fuzzing test tools employ techniques such as coverage-based feedback-driven testing and fault injection to discover security vulnerabilities in programs [ 83]. Additionally, tools that evaluate code efficiency, such as performance profilers ( e.g.,Line_profile [ 9] and Jprofile [ 8]), help researchers analyze, optimize, and maintain high-quality programs. FudanNLP 10

[Página 11]
Multi-Programming Language Sandbox for LLMs These program analysis tools can also be utilized by LLMs to enhance various downstream code-related tasks [ 28,34]. We have predefined five categories of tools and integrated open-source mainstream tools for each programming language, redirecting the output of these tools to text. Additionally, more options for program analysis tools are available. We also design templates that allow users to integrate their own tools with ease. MPLSandbox can unify results from compiler feedback and a variety of powerful analysis tools to streamline LLM workflows in a wide range of code-related tasks. 6 CONCLUSION In this paper, we introduce MPLSandbox, an out-of-the-box multi-programming language sandbox designed to provide unified compiler feedback and comprehensive analysis using both traditional- based and LLM-based methods for LLM-generated code. MPLSandbox not only can be effortlessly used by researchers to thoroughly analyze LLM-generated codes, but also can be integrated into the training and deployment phases to improve the correctness and quality of generated codes. Additionally, MPLSandbox can enhance the performance of LLMs across a wide range of code- related tasks through the flexible combination of various analysis tools within the sandbox. The goal of MPLSandbox is to support and advance further research in LLMs for software engineering by simplifying the complexity of training and employing LLMs in code-related tasks. REFERENCES [1] 2023. gpt-3.5-turbo. https://platform.openai.com/docs/models/gpt-3-5. [2] 2023. Llama-2. https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models. [3] 2024. astpretty. https://pypi.org/project/astpretty/. [4] 2024. clang. https://clang.llvm.org/. [5] 2024. coverage. https://coverage.readthedocs.io/en/7.6.4/. [6] 2024. javalang. https://github.com/c2nes/javalang. [7] 2024. joern. https://github.com/joernio/joern. [8] 2024. jprofiler. https://www.ej-technologies.com/jprofiler. [9] 2024. lineprofiler. https://github.com/pyutils/line_profiler. [10] 2024. Llama-3. https://ai.meta.com/blog/meta-llama-3. [11] 2024. LLMSandbox. https://hackernoon.com/introducing-llm-sandbox-securely-execute-llm-generated-code-with- ease. [12] 2024. mistralAI. https://mistral.ai/news/codestral/ [13] 2024. pmd. https://pmd.github.io/. [14] 2024. PromptFoo. https://www.promptfoo.dev/docs/guides/sandboxed-code-evals/. [15] 2024. radon. https://pypi.org/project/radon/. [16] 2024. Terrarium. https://github.com/cohere-ai/cohere-terrarium. [17] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al .2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219 (2024). [18] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A transformer-based approach for source code summarization. arXiv preprint arXiv:2005.00653 (2020). [19] Vishwanath Akuthota, Raghunandan Kasula, Sabiha Tasnim Sumona, Masud Mohiuddin, Md Tanzim Reza, and Md Mizanur Rahman. 2023. Vulnerability Detection and Monitoring Using LLM. In 2023 IEEE 9th International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE) . IEEE, 309–314. [20] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al .2023. SantaCoder: don’t reach for the stars! arXiv preprint arXiv:2301.03988 (2023). [21] AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card 1 (2024). [22] S Antoy, Paola Forcheri, and Maria Teresa Molfino. 1990. Specification-based code generation. In Twenty-Third Annual Hawaii International Conference on System Sciences , Vol. 2. IEEE Computer Society, 165–173. [23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al . 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023). FudanNLP 11

[Página 12]
Multi-Programming Language Sandbox for LLMs [24] Emery D Berger, Sam Stern, and Juan Altmayer Pizzorno. 2023. Triangulating python performance issues with SCALENE. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23) . 51–64. [25] Manish Bhattarai, Javier E Santos, Shawn Jones, Ayan Biswas, Boian Alexandrov, and Daniel O’Malley. 2024. Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation. arXiv preprint arXiv:2407.19619 (2024). [26] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al .2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [27] Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, and Jianwei Yin. 2024. Chatunitest: A framework for llm-based test generation. In Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering . 572–576. [28] Yiran Cheng, Lwin Khin Shar, Ting Zhang, Shouguo Yang, Chaopeng Dong, David Lo, Shichao Lv, Zhiqiang Shi, and Limin Sun. 2024. LLM-Enhanced Static Analysis for Precise Identification of Vulnerable OSS Versions. arXiv preprint arXiv:2408.07321 (2024). [29] DeepSeek-AI. 2024. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. arXiv preprint arXiv:2401.02954 (2024). https://github.com/deepseek-ai/DeepSeek-LLM [30] Paul Denny, Viraj Kumar, and Nasser Giacaman. 2023. Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language. In Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1 . 1136–1142. [31] Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, et al .2024. What’s Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv preprint arXiv:2407.06153 (2024). [32] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan, Zhiheng Xi, et al .2024. StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback. arXiv preprint arXiv:2402.01391 (2024). [33] Mingzhe Du, Anh Tuan Luu, Bin Ji, and See-Kiong Ng. 2024. Mercury: An efficiency benchmark for llm code synthesis. arXiv preprint arXiv:2402.07844 (2024). [34] Xiaohu Du, Ming Wen, Jiahao Zhu, Zifan Xie, Bin Ji, Huijun Liu, Xuanhua Shi, and Hai Jin. 2024. Generalization- Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning. arXiv preprint arXiv:2406.03718 (2024). [35] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al .2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [36] Markus Engelberth, Jan Göbel, Christian Schönbein, and Felix C Freiling. 2012. Pybox-a python sandbox. (2012). [37] Shuzheng Gao, Cuiyun Gao, Wenchao Gu, and Michael Lyu. 2024. Search-Based LLMs for Code Optimization. arXiv preprint arXiv:2408.12159 (2024). [38] Tal Garfinkel, Mendel Rosenblum, et al .2003. A virtual machine introspection based architecture for intrusion detection.. In Ndss, Vol. 3. San Diega, CA, 191–206. [39] Robert Gentleman and Duncan Temple Lang. 2007. Statistical analyses and reproducible research. Journal of Computational and Graphical Statistics 16, 1 (2007), 1–23. [40] Patrice Godefroid, Michael Y Levin, David A Molnar, et al .2008. Automated whitebox fuzz testing.. In NDSS , Vol. 8. 151–166. [41] Siqi Gu, Chunrong Fang, Quanjun Zhang, Fangyuan Tian, and Zhenyu Chen. 2024. TestART: Improving LLM-based Unit Test via Co-evolution of Automated Generation and Repair Iteration. arXiv preprint arXiv:2408.03095 (2024). [42] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence. https://api.semanticscholar.org/CorpusID:267211867 [43] Andrew Habib and Michael Pradel. 2018. How many of all bugs do we find? a study of static bug detectors. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering . 317–328. [44] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. InProceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual , Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks- proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html [45] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al .2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). FudanNLP 12

[Página 13]
Multi-Programming Language Sandbox for LLMs [46] Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Binta Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, and Anoop Deoras. 2024. Training LLMs to Better Self-Debug and Explain Code. arXiv preprint arXiv:2405.18649 (2024). [47] Zongze Jiang, Ming Wen, Jialun Cao, Xuanhua Shi, and Hai Jin. 2024. Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering . 1408–1420. [48] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. 2024. From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future. arXiv preprint arXiv:2408.02479 (2024). [49] Bogdan Korel. 1990. Automated software test data generation. IEEE Transactions on software engineering 16, 8 (1990), 870–879. [50] Jahnavi Kumar and Sridhar Chimalakonda. 2024. Code Summarization without Direct Access to Code-Towards Exploring Federated LLMs for Software Engineering. In Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering . 100–109. [51] Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, and Shafiq Joty. 2023. CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. In The Twelfth International Conference on Learning Representations . [52] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems 35 (2022), 21314–21328. [53] Bolun Li, Pengfei Su, Milind Chabbi, Shuyin Jiao, and Xu Liu. 2023. Djxperf: Identifying memory inefficiencies via object-centric profiling for java. In Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization . 81–94. [54] Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, and Tianyi Zhou. 2023. Reflection-tuning: Data recycling improves llm instruction-tuning. arXiv preprint arXiv:2310.11716 (2023). [55] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. 2023. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852 (2023). [56] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al .2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092–1097. [57] Zhenkai Liang, VN Venkatakrishnan, and R Sekar. 2003. Isolated program execution: An application transparent approach for executing untrusted programs. In 19th Annual Computer Security Applications Conference, 2003. Proceedings. IEEE, 182–191. [58] Guanjun Lin, Sheng Wen, Qing-Long Han, Jun Zhang, and Yang Xiang. 2020. Software vulnerability detection using deep neural networks: a survey. Proc. IEEE 108, 10 (2020), 1825–1848. [59] Hui Liu, Jiahao Jin, Zhifeng Xu, Yanzhen Zou, Yifan Bu, and Lu Zhang. 2019. Deep learning based code smell detection. IEEE transactions on Software Engineering 47, 9 (2019), 1811–1837. [60] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. RLTF: Reinforcement Learning from Unit Test Feedback. arXiv preprint arXiv:2307.04349 (2023). [61] Edward S Lowry and Cleburne W Medlock. 1969. Object code optimization. Commun. ACM 12, 1 (1969), 13–22. [62] Guilong Lu, Xiaolin Ju, Xiang Chen, Wenlong Pei, and Zhilong Cai. 2024. GRACE: Empowering LLM-based software vulnerability detection with graph structure and in-context learning. Journal of Systems and Software 212 (2024), 112031. [63] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021). [64] Yang Luo, Richard Yu, Fajun Zhang, Ling Liang, and Yongqiang Xiong. 2024. Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering . 2448–2449. [65] Lezhi Ma, Shangqing Liu, Yi Li, Xiaofei Xie, and Lei Bu. 2024. SpecGen: Automated Generation of Formal Program Specifications via Large Language Models. arXiv preprint arXiv:2401.08807 (2024). [66] Valentin JM Manès, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J Schwartz, and Maverick Woo. 2019. The art, science, and engineering of fuzzing: A survey. IEEE Transactions on Software Engineering 47, 11 (2019), 2312–2331. [67] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1–13. [68] OpenAI. 2023. GPT-4 Technical Report. arXiv:cs.CL/2303.08774 [69] Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2024. Lost in translation: A study of bugs introduced FudanNLP 13

[Página 14]
Multi-Programming Language Sandbox for LLMs by large language models while translating code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1–13. [70] José Pereira dos Reis, Fernando Brito e Abreu, Glauco de Figueiredo Carneiro, and Craig Anslow. 2022. Code smells detection and visualization: a systematic literature review. Archives of Computational Methods in Engineering 29, 1 (2022), 47–94. [71] Houxing Ren, Mingjie Zhan, Zhongyuan Wu, Aojun Zhou, Junting Pan, and Hongsheng Li. 2024. ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation. arXiv preprint arXiv:2405.17057 (2024). [72] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of programming languages. Advances in neural information processing systems 33 (2020), 20601–20611. [73] Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Murali Krishna Ramanathan, and Baishakhi Ray. 2024. Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM. Proceedings of the ACM on Software Engineering 1, FSE (2024), 951–971. [74] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [75] Jiho Shin, Clark Tang, Tahmineh Mohati, Maleknaz Nayebi, Song Wang, and Hadi Hemmati. 2023. Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks. arXiv preprint arXiv:2310.10508 (2023). [76] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023). [77] Qingxiao Tao, Tingrui Yu, Xiaodong Gu, and Beijun Shen. 2024. Unraveling the Potential of Large Language Models in Code Translation: How Far Are We? arXiv preprint arXiv:2410.09812 (2024). [78] Qwen Team. 2024. Qwen2.5: A Party of Foundation Models. https://qwenlm.github.io/blog/qwen2.5/ [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [80] Yuvraj Virk, Premkumar Devanbu, and Toufique Ahmed. 2024. Enhancing Trust in LLM-Generated Code Summaries with Calibrated Confidence Scores. arXiv preprint arXiv:2404.19318 (2024). [81] Wenhan Wang, Kaibo Liu, An Ran Chen, Ge Li, Zhi Jin, Gang Huang, and Lei Ma. 2024. Python Symbolic Execution with LLM-powered Code Generation. arXiv preprint arXiv:2409.09271 (2024). [82] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022. Execution-based evaluation for open-domain code generation. arXiv preprint arXiv:2212.10481 (2022). [83] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. 2024. Fuzz4all: Univer- sal fuzzing with large language models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1–13. [84] HanXiang Xu, ShenAo Wang, Ningke Li, Yanjie Zhao, Kai Chen, Kailong Wang, Yang Liu, Ting Yu, and HaoYu Wang. 2024. Large language models for cyber security: A systematic literature review. arXiv preprint arXiv:2405.04760 (2024). [85] Xin Yin, Chao Ni, Tien N Nguyen, Shaohua Wang, and Xiaohu Yang. 2024. Rectifier: Code Translation with Corrector via LLMs. arXiv preprint arXiv:2407.07472 (2024). [86] Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, and Hongxia Yang. 2023. Beta-Coder: On Value-Based Deep Reinforcement Learning for Program Synthesis. In The Twelfth International Conference on Learning Representations . [87] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302 (2023). [88] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. 2023. Towards an understanding of large language models in software engineering tasks. arXiv preprint arXiv:2308.11396 (2023). [89] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al.2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. arXiv preprint arXiv:2406.11931 (2024). FudanNLP 14

[Página 15]
Multi-Programming Language Sandbox for LLMs A CASE STUDY ON USAGE This section demonstrates the usage of various analysis function of MPLSandbox through a config- uration example shown in Figure 4, where: •Question field presents the code generation problem posed to the LLM, requiring a function named calculation(n) that performs complex calculations based on different input values of n. The problem description provides the input requirements (an integer, ranging from 0 to 299), output requirements (an integer or a list, depending on the result of the calculation), and an example (when the input is 3, the output should be [1, 3, 6, 10]). •Code field is the code provided by the LLM in response to the Question field. •Unit Cases field provides 3 unit test cases, which includes two sub-fields: Unit Inputs and Unit Outputs .Unit Inputs shows three test inputs provided: "51", "120", "211". Unit Outputs shows the corresponding expected output results provided. •Language field specifies the language of the code, which is set to "AUTO" here, meaning the code language is automatically detected. def calculation(): n = int(input()) if n <= 100: result = [i * (i + 1) // 2 for i in range(1, n + 1)] elif n <= 200: result = [i ** 2 for i in range(n)] if n % 3 == 0: result.append(n ** 3) elif n < 300: result = [] if n % 2 == 0: for i in range(n): if i % 2 == 0: result.append(i * 2) else: result.append(i ** 3) else: for j in range(2, 10): for i in range(n): if i % j == 0: result.append(i * j) if n % 2 == 0: result = [i + 1 for i in result] return result calculation() -----Description----- Write a example function calculation() that performs a calculation based on the input value n (0 <= n < 300). If n is less than or equal to 100...If n is even, increment each element in the result list by 1. -----Input----- An integer, n (0 <= n < 300). -----Output----- An integer or a list, the result of the calculation. -----Example----- Input:3 Output:[1, 3, 6, 10] "Unit_Inputs": ["51","120","211"], "Unit_Outputs": ["[1, 3, 6, 10, ..., 1176, 1225, 1275, 1326]", "[1, 2, 5, 10, 17, 26,...,13925, 14162, 1728001]", "[0, 4, 8, 12, 16, 20, 24, 28, 32, ..., 1377, 1458, 1539, 1620, 1701, 1782, 1863]"] "AUTO"(d) Code(a) Question (b) Unit Cases (c) Language Fig. 4. The Form of Configuration Moreover, it still supports specifying docker client instance, docker image, and dockerfile for building custom Docker images. For more details about parameter configuration, please move to: https://github.com/Ablustrund/MPLSandbox After initializing MPLSandbox as an executor using the configuration, it can be run by calling therun() method. MPLSandbox analyzes the code across five levels by calling the Code Analysis Module and integrates the analysis results through the Information Integration Module. Users can choose the required related information according to their needs. The following sections will conduct corresponding case studies on the example code in the configuration. FudanNLP 15

[Página 16]
Multi-Programming Language Sandbox for LLMs (b) AST （c）CFG (a) Basic Feedback----Reward---- 1.0 ----Compiler Feedback---- "All Unit tests Pass!" ----Correct Rate---- 1.0 ----Unit Inputs---- ["51","120","211"], "Required Outputs" ["[1, 3, 6, 10, ..., 1176, 1225, 1275, 1326]", "[1, 2, 5, 10, 17, 26,...,13925, 14162, 1728001]", "[0, 4, 8, 12, 16, 20, 24, 28, 32, ..., 1377, 1458, 1539, 1620, 1701, 1782, 1863]"] ----Language---- "python" Fig. 5. The Report of Code Basic Analysis A.1 Basic Information Analysis As shown in Figure 5, Code Basic Analysis returns a Basic Feedback along with Abstract Syntax Tree (AST) and Control Flow Graph (CFG). The basic feedback includes fields such as Reward ,Compiler Feedback ,Correct Rate ,Unit Inputs ,Required Outputs and Language . From the compiler feedback, it can be seen that the code has successfully passed all unit tests, achieving a correct rate of 1.0 and a reward of 1.0. These values can serve as signals for downstream analysis or training tasks. Additionally, MPLSandbox parses the code into the forms of an AST and a CFG. The AST presents the syntactic structure of the code in a tree diagram, where each node represents a syntactic element in the code, such as function definitions, assignment operations, conditional judgments, and loops. This structure helps to understand the logic and hierarchical relationships of the code, facilitating code optimization and error detection. The CFG, on the other hand, graphically displays the execution paths and decision points of the code, including basic blocks (representing a series of consecutive instructions) and edges (indicating the direction of control flow), which helps to reveal the execution order of the program and potential branching conditions. The CFG is beneficial for identifying loop dependencies, potential performance bottlenecks, and logical errors in the program. A.2 Code Smell Analysis & Code Bug Analysis Code Smell Analysis and Code Bug Analysis identify potential issues or vulnerabilities within the code, reporting the specific line numbers as well as the categories of smells or bugs. To better demonstrate Code Smell Analysis and Code Bug Analysis, we have artificially introduced some code smells and vulnerabilities into the code. The yellow warning boxes in Figure 6 represent the parts of the code where MPLSandbox has detected code smells, which include: •Line 2, 3, and 10 exhibit the Vague Comments smell due to the docstring and comments being filled with vague and abstract descriptions, not providing concrete information to help understand the actual functionality of the function. •The Complex Conditional smell on lines 7, 9, 15, 28 is due to the use of multiple if-elif statements within the function, making the conditional logic complex and difficult to trace. The Hardcoded Values smell refers to the values 100, 200, and 300 being hardcoded within the function, requiring changes in multiple places if these values need to be modified. •The Repeated Initialization smell on lines 8, 11, 16 is due to the variable result being initialized in multiple branches, violating the DRY (Don’t Repeat Yourself) principle. Unclear Naming refers to the variable result not clearly expressing its meaning and a more descriptive name might be better. FudanNLP 16

[Página 17]
Multi-Programming Language Sandbox for LLMs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34lRange Errors lUndefined Behavior lComplex Conditional lHardcoded Values lType ErrorlInconsistent Code Style lHigh Code ComplexitylUnvalidated Input lRepeated Initialization lUnclear Naming Lines: 5 Lines: 7 Lines: 7, 9, 15, 28 Lines: 30Lines: 8, 11, 16 Lines: 18, 24, 30, 32Code Bug Code Smell def calculation(): """This function does some calculations. It processes the input through various statements""" # Get input from user n = int(input()) if n <= 100: result = [i * (i + 1) // 2 for i in range(1, n + 1)] elif n <= 200: # Some calculation result = [i ** 2 for i in range(n)] if n % 3 == 0: result.append(n ** 3) elif n < 300: result = [] if n % 2 == 0: for i in range(n): if i % 2 == 0: result = [i + 1 for i in result] else: result.append(i ** 3) else: for i in range(2, 10): for j in range(n): if j % i == 0: result.append(i * j) else: if n % 2 == 0: result = [i + 1 for i in result] else: result = [i for i in range(n)] print(result) calculation()lVague CommentsLines: 2, 3, 10 lUndefined VariableLines: 30 Fig. 6. The Report of Code Smell Analysis and Code Bug Analysis. •The Inconsistent Code Style smell on lines 18, 24, 30, 32 refers to the inconsistent use of variables iand jin for loops across different branches, which may cause confusion. The High Code Complexity smell indicates that the function contains multiple conditional branches and nested loops, making it difficult to understand and maintain the code. It is worth noting that the bad smell determines the overall maintainability of the code, which is often difficult to quantify. Therefore, Code Smell Analysis provides a sub-report specifically for analyzing the maintainability of the code. As shown in Figure 7, the report is divided into three parts: Raw Analysis, Halstead Metrics, and Maintainability Index. The raw analysis provides the distribution of source code, comments, multiline comments, and blank lines, showing a preliminary state of the code. Halstead Metrics offer various indicators of the Halstead volume to quantify the complexity of the code. The Maintainability Index, on the other hand, is a comprehensive calculation of the overall Maintainability Index based on the number of lines of source code, the volume indicator in the Halstead volume, and the cyclomatic complexity of the code, providing a comprehensive assessment of the code’s complexity. It’s important to note that the Maintainability Index in the report is a standardized result, with its range limited to 0-100. This threshold can be broken down into 0-9 (red), indicating that the code is difficult to maintain. Code within this range may have many issues and requires significant effort to improve its maintainability. 10-19 (yellow) indicates that the code’s maintainability is moderate. Although not the worst, code within this range may still require some improvements to enhance its maintainability. 20-100 (green), which is the range where the code falls, indicates that the code has a good structure and clear coding style, making it easy to maintain. FudanNLP 17

[Página 18]
Multi-Programming Language Sandbox for LLMs Indicator Value Indicator Value H1 8 Calculated Length 93.49 H2 17 Volume 320.43 N1 23 Difficulty 10.82 N2 46 Effort 3468.14 Vocabulary 25 Times 192.67 Length 69 Bugs 0.11 Function Cyclomatic Complexity Halstead Volume Source LinesMaintainability Index calculation() 12 320.43 28 49.24 20-100 10-19 0-9(a) Raw Analysis (b) Halstead Metrics (c) Maintainability Index Fig. 7. The Sub Report of Code Maintainability Analysis The red warning boxes in Figure 6 represent the parts of the code where MPLSandbox has detected code bugs, which include: •The Unvalidated Input bug on line 5 is due to user input being used directly in calculations without any validation or restriction. •The Range Errors bug on line 7 occurs when n=0, causing range(1, n + 1) to raise a Val- ueError. The Undefined Behavior bug is when nis negative, the behavior of range(n) is undefined. •The Type Error bug on line 20 occurs when the variable result is empty, executing result = [i + 1 for i in result] will raise a TypeError. •The occurrence of an Undefined Variable on line 30 is due to the fact that the variable result was not initialized within this conditional branch. Consequently, the result = [i + 1 for i in result] is invalid code. A.3 Unit Test Analysis Unit Test Analysis returns a comprehensive coverage report for the given unit tests. As shown in Figure 8, green lines represent the overlapping parts of the executed lines for different unit inputs, while yellow, blue, and red lines represent the non-overlapping parts of the executed lines for the test cases "51", "120", and "210", respectively. For the unit input "51", 7 lines of code were executed, with a coverage rate of 0.3. For the unit input "120", 11 lines of code were executed, with a coverage rate of 0.48. For the unit input "211", 14 lines of code were executed, with a coverage rate of 0.61. For a total of 23 lines of code, the overall average coverage rate is 0.46. This indicates that the current test cases do not fully cover the code paths. Furthermore, Unit Test Analysis has conducted a complete coverage statistics for all test inputs within the given range. It can be observed that within the range of unit input 0 <= n < 300, this set of code has resulted in 7 different coverage possibilities, with the highest being 0.65 and the lowest being 0.35. The distribution of unit inputs across various coverage rates is relatively even. It is evident that after iterating through all possible test inputs, the code coverage remains at a relatively low level, suggesting that the logical framework of the code itself still has significant room for improvement. A.4 Code Efficiency Evaluation Code Efficient Evaluation provides an analysis of code execution efficiency for different test cases. Figure 9 reports the Hits (the number of times a code line is executed), Time (the total execution FudanNLP 18

[Página 19]
Multi-Programming Language Sandbox for LLMs def calculation(): n = int(input()) if n <= 100: result = [i * (i + 1) // 2 for i in range(1, n + 1)] elif n <= 200: result = [i ** 2 for i in range(n)] if n % 3 == 0: result.append(n ** 3) elif n < 300: result = [] if n % 2 == 0: for i in range(n): if i % 2 == 0: result.append(i * 2) else: result.append(i ** 3) else: for j in range(2, 10): for i in range(n): if i % j == 0: result.append(i * j) if n % 2 == 0: result = [i + 1 for i in result] return result calculation() overlap coverage unit input: "120" unit input: "211"unit input: "51" Unit Input "51" "120" "211" Total Lines 23 Executed Lines 7 11 14 Coverage Rate 0.3 0.48 0.61 Avg. Coverage Rate 0.46 (a) Code Execution Status (d) Unit Input - Coverage Rate Distribution(c) Coverage Information(b) Executed Code Legend Fig. 8. The Report of Unit Test Analysis time of the code line in milliseconds), Per Hits (the average time required for each execution of the code line in milliseconds), and %Time (the percentage of the total execution time taken by the execution time of the code line). As shown in Figure 9, code lines 2, 3, 5, 22 and 24 have common execution records under different test inputs, with some code lines taking a longer execution time under specific inputs. For example, code line 6 takes 58.1 milliseconds to execute under the input "120" because in this case, line 6 is a loop that iterates 120 times. Code line 23 takes 33.2 milliseconds to execute under the input "210" because this line of code contains a loop that iterates based on the variable result , which is strongly related to the input 210. Code lines 12, 13, and 14 have a large number of executions under the input "210" (211 times, 210 times, and 105 times, respectively), because this part involves the processing of a large range loop. Therefore, these perceptions of code line execution efficiency undoubtedly provide very important basis for further performance optimization. B PROMPTS IN DETAIL The system prompt for multi-programming language is as follows: You are a master programmer. Now given a programming problem, you need to analyze it carefully and answer this programming problem directly in {lang} programming language. Note that generated code will accept a unit test case through standard input (stdin) from the terminal. And after printing the output result, the program will terminate ### Prompt: [Programming problem] {programming problem} Solve this problem directly in {lang} . The code needs to be surrounded by backquotes (i.e., “‘code“‘) ### Response: FudanNLP 19

[Página 20]
Multi-Programming Language Sandbox for LLMs Unit Input: "51" Unit Input: "120" Unit Input: "211" Code Lines HitsTimePer Hits%TimeHitsTimePer Hits%TimeHitsTimePer Hits%Time 1 1 2.3 2.3 7 1 2.4 2.4 2.2 1 2.5 2.5 0.7 2 1 0.7 0.7 2 1 0.7 0.7 0.6 1 0.7 0.7 0.2 3 1 17.1 17.1 52 4 1 0.4 0.4 0.4 1 0.4 0.4 0.1 5 1 58.1 58.1 54.7 6 1 0.8 0.8 0.8 7 1 1.7 1.7 1.6 8 1 0.4 0.4 0.1 9 1 0.4 0.4 0.1 10 1 0.8 0.8 0.1 11 12 13 14 15 16 17 9 4.6 0.5 0.3 18 1696543.7 0.3 39.4 19 1688593.7 0.4 43.0 20 391 189.0 0.5 13.7 21 1 0.9 0.9 2.7 1 0.6 0.6 0.6 1 0.6 0.6 0 22 1 23.2 23.2 21.8 23 1 12 12 36.4 1 18.3 18.3 17.2 1 27 27 7.7 24def calculation(): n = int(input()) if n <= 100: result = [i * (i + 1) // 2 for i in range(1, n + 1)] elif n <= 200: result = [i ** 2 for i in range(n)] if n % 3 == 0: result.append(n ** 3) elif n < 300: result = [] if n % 2 == 0: for i in range(n): if i % 2 == 0: result.append(i * 2) else: result.append(i ** 3) else: for j in range(2, 10): for i in range(n): if i % j == 0: result.append(i * j) if n % 2 == 0: result = [i + 1 for i in result] return result Fig. 9. The Report of Code Efficiency Evaluation The system prompts for correcting and refining code are as follows: You are a master of program correction. Now given a programming problem, an incorrect solution to this problem, and the compiler error message. You need to carefully analyze and correct the incorrect code ### Prompt: [Programming problem] {programming problem} [Incorrect code] {incorrect code} [Compiler error message] {error message} Please correct this problem, and the new code needs to be surrounded by backquotes (i.e., “‘code“‘) ### Response: FudanNLP 20

[Página 21]
Multi-Programming Language Sandbox for LLMs You are a master of program refinement. Now given a programming problem, the correct solution, and some suggestions for improvement including readability, complexity and code Specification. You need to refine the code according to these suggestions ### Prompt: [Programming problem] {programming problem} [Original code] {code} [Suggestions through code smell] {analysis results} # including code style and specifications ( e.g.,long method, long param- eter list and tight coupling) Please first summarize recommendations for refinement from these analysis results, and then refine this code. The new code needs to be surrounded by backquotes (i.e., “‘code“‘) ### Response: C EXPERIMENTS SETUP IN DETAIL C.1 Models •DeepSeek-Coder-Instruct-6.7B [42], developed by DeepSeek AI, demonstrates state-of- the-art performance among open-source code models across various programming languages. The training corpus for these models comprises an impressive 2 trillion tokens, combining code and natural language texts. Each model is trained to utilize a window size of 16K. •DeepSeek-Coder-V2-Lite-Instruct-16B [89] is an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT-4 Turbo in code-specific tasks. It has 2.4 billion active parameters. The model is further pre-trained from an interme- diate checkpoint of DeepSeek-V2 with an additional 6 trillion tokens. DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. •Qwen2.5-Coder-1.5B-Instruct [78] and Qwen2.5-Coder-7B-Instruct [78] are members of the Qwen2.5 series. They are built on the Qwen2.5 architecture and have been further pre-trained on an extensive dataset exceeding 5.5 trillion tokens, which includes source code, text-code grounding data, and synthetic data. These two LLMs support up to 128K tokens of context and cover 92 programming languages. •Codestral-v0.1-22B [12] is trained by Mistral AI. This model is trained on a diverse dataset encompassing over 80 programming languages, including popular ones like Python, Java, C, C++, JavaScript, and Bash. It also excels in more specialized languages such as Swift and Fortran. This model supports up to 32K context window. •Llama-3.1-Instruct-70B [35] is developed by Meta AI. It is pre-trained and instruction- tuned generative models that handle text input and output. The Llama 3.1 instruction-tuned models are specifically optimized for multilingual dialogue applications, surpassing many open-source and proprietary chat models on standard industry benchmarks. The context window of this model is 128K. •GPT-4o [68] is an advanced iteration of OpenAI’s language model, designed to offer more refined and contextually aware responses. GPT-4o is exceptionally versatile, and capable of excelling in a wide range of applications from creative writing to technical problem-solving. The context window and maximum output tokens are 128K and 16,384, respectively. FudanNLP 21

[Página 22]
Multi-Programming Language Sandbox for LLMs C.2 Reinforcement Learning from Compiler Feedback Following prior work [ 32,60] in code generation, we use Proximal Policy Optimization (PPO) algorithm [ 74] to optimize the policy model 𝜋𝜃with parameter 𝜃. Consider a programming problem 𝑥, the policy model generates the response ˆ𝑦according to 𝑥. The reward model provides the reward 𝑟according to the correctness of the code snippet 𝜏extracted from response ˆ𝑦, where we use the same setting as previous approaches [32, 52, 76] as follows: 𝑟(𝑥,𝜏)= +1,if𝜏passed all unit tests −0.3,if𝜏failed any unit test −0.6,if𝜏happened runtime error −1,if𝜏happened compile error .(1) The policy model 𝜋𝜃can be optimized by maximizing the objection function as follows: Objective(𝜃)=𝐸(𝑥,ˆ𝑦)∼𝐷𝜋𝜃[𝑟(𝑥,𝜏)−𝛽log(𝜋𝜃(ˆ𝑦|𝑥)/𝜋ref(ˆ𝑦|𝑥))] (2) where𝜋refis the reference model in PPO, which is initialized with the parameters of the initial policy model and kept frozen throughout the training process. D INSTANCES Figure 10 illustrates the mechanism of self-refinement for a instance in the test dataset. It can be seen that the workflow of this mechanism begins with Code Smell Analysis for smell detection, identifying existing issues such as No Docstrings and Lack of Comments, Unclear Variable Naming, Hardcoded Limits, High Complexity, and Redundant Sorting. Subsequently, the built-in AI system proposes corresponding improvements for these suggestions. Finally, these suggestions are used as system prompts to achieve self-refinement. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 291 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26def find_interesting_numbers(): # Read number of test cases T = int(input()) for _ in range(T): M = int(input()) interesting_nums = [] for A in range(1, M + 1): for B in range(A, M + 1): if (A * B) % M == 0: if (A * B) // M == A + B: interesting_nums.append(A) break interesting_nums = sorted(set(interesting_nums)) # Output results print(len(interesting_nums)) for num in interesting_nums: print(num) find_interesting_numbers()def find_interesting_numbers(): """ This function identifies all interesting numbers for given positive integers M. An interesting number A has at least one integer B (A <= B) such that: - A * B is divisible by M - (A * B) / M = A + B It reads multiple test cases, calculates the interesting numbers for each M, and prints the results. """ # Read number of test cases T = int(input("Enter the number of test cases: ")) for _ in range(T): divisor = int(input("Enter a positive integer M: ")) interesting_nums = set() # Use a set to store unique interesting numbers # Find interesting numbers for interesting_num in range(1, divisor + 1): if (divisor % interesting_num) == 0: multiplier = (divisor // interesting_num) + interesting_num # Check if A * B is divisible by M and meets the condition if (interesting_num * multiplier) % divisor == 0: interesting_nums.add(interesting_num) # Convert to sorted list interesting_nums = sorted(interesting_nums) # Output results print(len(interesting_nums)) for num in interesting_nums: print(num) find_interesting_numbers()Lines: 1, 2,11,18Lines: 1--9, 16, 23 No Docstrings and Lack of CommentsAdded a docstring and comments Lines: 8, 12, 13Lines: 13, 14, 19 Unclear Variable NamingM -> divisor A -> interesting_num B -> multiplier Lines: 12 Lines: 17 Hardcoded LimitsFocused the logic to avoid unnecessary computations Lines: 12, 13Lines: 18, 19 High ComplexityUse a single loop and mathematical properties Lines: 19 Lines: 24 Redundant SortingUse a single loop and mathematical propertiesSuggestion Code Smell Smell Detection Refinement Fig. 10. The Code Refinement Mechanism Figure 11 shows the results of analyzing certain metrics of the code before and after refinement using maintainability analysis, quantifying the effectiveness of the refinement. As can be seen from Figure 11(a), there is a significant increase in the ratio of single-line and multi-line comments in the code compared to before, which undoubtedly enhances the interpretability of the code. Furthermore, FudanNLP 22

[Página 23]
Multi-Programming Language Sandbox for LLMs as shown in Figure 11(b), after refinement, the overall cyclomatic complexity and Halstead Volume of the code have decreased, resulting in an increase in the Maintainability Index compared to before refinement, further demonstrating the positive feedback of the entire refinement on code maintainability. Function find_interesting_numbers() Refinement Before After Cyclomatic Complexity 7 6 Halstead Volume 105.49 95.38 Source Lines 16 15 Maintainability Index 59.24 59.68(a) Raw Analysis Before & After Refinement (b) Maintainability Index Before & After Refinement Fig. 11. Code Maintainability Report Before and After Refinement E ADDITIONAL APPLICATION SCENARIOS To further demonstrate the effectiveness and efficiency of our tool, we present it in three downstream code-related tasks. Our tool can significantly streamline LLM workflows in these tasks, greatly reducing developers’ time and costs. E.1 Unit Test Generation In this experiment, we show how to generate more effective unit test samples through MPLSandbox to evaluate the user’s code. User-provided unit test samples may not cover the entire code, i.e.,the unit test coverage is not 100%. This means there could be errors in code snippets that have not been verified. Following prior work [ 47], we can utilize the unit test analysis tool to identify the uncovered code snippets. We then integrate these snippets into the prompt to enable the LLM to generate additional unit test samples to verify these uncovered snippets. Through MPLSandbox, unit test generation is simple and effective. Users can modify the system prompt to achieve this task easily. An instance is illustrated in Figure 12. E.2 Code Translation Second, we showcase translating code from one programming language to another. Large language models have been widely used in code translation. Prior work [ 64,77] has demonstrated that including structural information of the code, such as unit test cases and the control flow graph, into the system prompt can help LLMs better understand the code, significantly increasing the success rate of code translation. FudanNLP 23

[Página 24]
Multi-Programming Language Sandbox for LLMs ###SystemPrompt:Youareacodeanalysismasterofgeneratingunittestcases.Now,givenaprogrammingproblem,thecode,unittestcasesforcode,andacoverageanalysisreportofthesetestcases.Basedonthereport,youneedtogeneratenewunittestcasestocovertheuncoveredlines###Prompt:[Programmingproblem]GivenanarrayAwhereeachelementrepresentstheamountofcoinsinapathnodeandanumberB,whichrepresentsthemaximumnumberofstepsyoucanjumpinonemove,findthepathfromthefirstnodetothelastnodesuchthatthetotalamountofcoinscollectedismaximized,andthepathistheshortestintermsofstepstaken.Iftherearemultiplepathswiththesamenumberofcoins,returnthelexicographicallysmallestpath.[Code]class Solution:def maxCoinsPath(self, A, B):n = len(A)dp= [-float('inf')] * n # dp[i] stores the max coins at index ipath = [-1] * n # Stores the previous index for reconstruction of pathdp[0] = A[0]for iin range(1, n):for j in range(1, B + 1):if i-j >= 0 and A[i] != -1 and dp[i-j] != -float('inf'):if dp[i] < dp[i-j] + A[i]:dp[i] = dp[i-j] + A[i]path[i] = i-jif dp[-1] == -float('inf'):return []res = []idx= n -1while idx!= -1:res.append(idx+ 1)idx= path[idx]return res[::-1][UnitTestCases]Input:A=[1,2,4,-1,2],B=2Output:[1,3,5]Input:A=[1,-1,3,2,1],B=3Output:[1,3,5][UnitTestAnalysisReport]NameStmtsMissCoverMissing-----------------------------------------solution.py20195%16test_solution.py110100%------------------------------------------TOTAL31197%Pleasegeneratenewunittestcases###Response:Basedonthecoverageanalysisreport,wecanseethatthemissingcoverageisatline16ofthecode.Thislinelikelycorrespondstotheconditionwherethedp[-1]==-float('inf'),meaningthatthereisnovalidpathtothelastnode.Weneedtocreateatestcasewhereit’simpossibletoreachthelastnode(i.e.,thereisnovalidpath),whichwillcausethefunctiontoreturnanemptylist.Input:A=[1,2,4,-1,2],B=2Output:[1,3,5]Input:A=[1,-1,3,2,1],B=3Output:[1,3,5] Input : A = [1, 2, -1, -1, 2], B = 2 Output : []Thistestcasecreatesascenariowherenovalidpathtothelastnodeispossibleduetothe-1valuesatpositions2and3,ensuringthatline16iscoveredandthefunctioncorrectlyreturnsanemptylist. Fig. 12. Unit test generation through MPLSandbox With MPLSandbox, users can effortlessly integrate various code-related information from tradi- tional analysis tools into the system prompt, to enhance the performance of code translation. An instance of translating code from Python into Java is shown in Figure 12. ###SystemPrompt:Youareamasterofcodetranslation.Now,givenaprogrammingproblem,sourcePythoncode,unittestcasesforthecode,andthecontrolflowgraphforcode.Basedontheseinformation,youneedtotranslatethesourcecodefromPythontoJava###Prompt:[Programmingproblem]Designaclasstofindthek-thlargestelementinastream.YoushouldimplementtheclassKthLargestwhichaddsnumberstothestreamandfindsthek-thlargestelement.[SourcePythoncode]import heapqdef __init__(self, k: int, nums: list[int]):self.k= kself.min_heap= numsheapq.heapify(self.min_heap)while len(self.min_heap) > k:heapq.heappop(self.min_heap)def add(self, val: int) -> int:heapq.heappush(self.min_heap, val)if len(self.min_heap) > self.k:heapq.heappop(self.min_heap)return self.min_heap[0][Unittestcases]Input:["KthLargest","add","add","add","add","add"][[3,[4,5,8,2]],[3],[5],[10],[9],[4]]Output:[null,4,5,5,8,8][Controlflowgraph]op2=>operation: import heapqst5=>start: start __init__io7=>inputoutput: input: self, k, numsop10=>operation: self.k = kop12=>operation: self.min_heap = numssub14=>subroutine: heapq.heapify(self.min_heap)cond17=>operation: heapq.heappop(self.min_heap) while (len(self.min_heap) > k)e29=>end: end __init__st33=>start: start addio35=>inputoutput: input: self, valsub38=>subroutine: heapq.heappush(self.min_heap, val)cond41=>operation: heapq.heappop(self.min_heap) if (len(self.min_heap) > self.k)io54=>inputoutput: output: self.min_heap[0]e52=>end: end function returnop2->st5st5->io7io7->op10op10->op12op12->sub14sub14->cond17cond17->e29e29->st33st33->io35io35->sub38sub38->cond41cond41->io54io54->e52PleasetranslatethecodefromPythontoJava,andthenewcodeneedstobesurroundedbybackquotes(i.e.,```code```)###Response:…```javaimportjava.util.PriorityQueue;publicclassKthLargest{privateintk;privatePriorityQueue<Integer>minHeap;publicKthLargest(intk,int[]nums){this.k=k;minHeap=newPriorityQueue<>();//Addallelementsfromnumstotheheapfor(intnum:nums){minHeap.offer(num);}//Maintainonlytheklargestelementsintheheapwhile(minHeap.size()>k){minHeap.poll();}}publicintadd(intval){minHeap.offer(val);if(minHeap.size()>k){minHeap.poll();//Removethesmallestelementtokeeponlyklargest}returnminHeap.peek();//Therootoftheheapisthek-thlargestelement}}```… Fig. 13. Code translation through MPLSandbox E.3 Vulnerability Location LLMs can assist developers in identifying potential vulnerabilities in their code. When combined with traditional vulnerability analysis tools, LLMs can significantly enhance the performance of vulnerability detection and localization [ 19,62]. MPLSandbox can also efficiently deploy LLM- based vulnerability detection workflows, minimizing development costs. An instance of detecting vulnerabilities in the code through MPLSandbox is illustrated in Figure 14. FudanNLP 24

[Página 25]
Multi-Programming Language Sandbox for LLMs ###SystemPrompt:Youareamasterofvulnerabilitylocalization.Now,givenaprogrammingproblem,thecode,theanalysisreportbasedontraditionalvulnerabilitydetectiontools.Basedontheseinformation,youneedtodetectandlocalizethevulnerabilityinthecode###Prompt:[Programmingproblem]Youaregivenadirectorypathwhereseveraltextfilesarestored.Eachtextfilecontainsalistofintegers,oneperline.Yourtaskistowriteafunctionthatreadsallthefilesinthedirectory,extractstheintegersfromeachfile,andreturnsthemaximumintegervaluefoundacrossallfiles.Youcanassumethatthedirectoryandfilesarevalidandaccessible,andallfilescontainvalidintegers.[Code]import osimport subprocessclass Solution:def find_max_in_directory(self, directory_path: str) -> int:max_value = -float('inf')command = f"ls {directory_path}"result = subprocess.run(command, shell=True, capture_output=True, text=True)# Get list of files in the directoryfiles = result.stdout.splitlines()for file in files:file_path = os.path.join(directory_path, file)# Another subprocess command to read the contents of each filecommand = f"cat {file_path}"result = subprocess.run(command, shell=True, capture_output=True, text=True)# Process file contents and find the maximum valuefor line in result.stdout.splitlines():num = int(line)max_value = max(max_value, num)return max_value[Analysisreport]>>Issue:[B404:blacklist]Considerpossiblesecurityimplicationsassociatedwiththesubprocessmodule.Severity:LowConfidence:HighCWE:CWE-78Location:./solution.py:2:0>>Issue:[B602:subprocess_popen_with_shell_equals_true]subprocesscallwithshell=Trueidentified,securityissue.Severity:HighConfidence:HighCWE:CWE-78Location:./solution.py:9:17Location:./solution.py:18:21Pleasedetectandlocalizethevulnerabilityinthecode###Response:Basedontheanalysisreportandthecodeprovided,therearetwohigh-severityvulnerabilitiesidentified,specificallyintheuseofsubprocess.run()withshell=True.Thisopensthecodetopotentialcommandinjectionattacks(CWE-78),whereanattackercouldinjectmaliciousshellcommandsthroughuserinput.1.Vulnerability 1: Command Injection in ls Command (Line 9)Issue:Thedirectory_pathparameterisdirectlyembeddedintheshellcommandwithoutanysanitization.Ifanattackercontrolsthevalueofdirectory_path,theycaninjectarbitraryshellcommands.Forexample,settingdirectory_path="valid_dir;rm-rf/"couldcausedangerouscommandstobeexecutedonthesystem.Location:Line9,subprocess.run(command,shell=True,...)2. Vulnerability 2: Command Injection in cat Command (Line 18)Issue:Similarly,file_pathispasseddirectlyintoashellcommandwithoutproperescapingorvalidation.Ifthefilenamecontainsmaliciouscontent(likefile.txt;rm-rf/),thiscouldalsoleadtoarbitrarycommandexecution.Location:Line18,subprocess.run(command,shell=True,...) Fig. 14. Vulnerability location through MPLSandbox In summary, MPLSandbox can enhance the efficiency of deploying LLMs in various code-related tasks. FudanNLP 25