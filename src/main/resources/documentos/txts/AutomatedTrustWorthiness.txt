
[PÃ¡gina 1]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers LAM NGUYEN TUNG, Monash University, Australia STEVEN CHO, University of Auckland, New Zealand XIAONING DU, Monash University, Australia NEELOFAR NEELOFAR, Monash University, Australia VALERIO TERRAGNI, University of Auckland, New Zealand STEFANO RUBERTO, JRC European Commission, Italy ALDEIDA ALETI, Monash University, Australia Machine learning (ML) for text classification has been widely used in various domains, such as toxicity detection, chatbot consulting, and review analysis. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Several studies indicate that traditional uncertainty metrics, such as model confidence, and performance metrics, like accuracy, are insufficient to build human trust in ML models. These models often learn spurious correlations during training and predict based on them during inference. When deployed in the real world, where such correlations are absent, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are made reasonably based on valid patterns in the data, Along with this, a challenge known as the trustworthiness oracle problem has been introduced. So far, due to the lack of automated trustworthiness oracles, the assessment requires manual validation, based on the decision process disclosed by explanation methods. However, this approach is time-consuming, error-prone, and not scalable. To address this problem, we propose TOKI, the first automated trustworthiness oracle generation method for text classifiers. TOKI automatically checks whether the words contributing the most to a prediction are semantically related to the predicted class. Specifically, we leverage ML explanation methods to extract the decision-contributing words and measure their semantic relatedness with the class based on word embeddings. As a demonstration of its practical usefulness, we also introduce a novel adversarial attack method that targets trustworthiness vulnerabilities identified by TOKI. We compare TOKI with a naive baseline based solely on model confidence. To evaluate their alignment with human judgement, experiments are conducted on human-created ground truths of approximately 6,000 predictions. Additionally, we compare the effectiveness of TOKI-guided adversarial attack method with A2T, a state-of-the-art adversarial attack method for text classification. Results show that (1) relying on prediction uncertainty metrics, such as model confidence, cannot effectively distinguish between trustworthy and untrustworthy predictions, (2) TOKI achieves 142% higher accuracy than the naive baseline, and (3) TOKI-guided adversarial attack method is more effective with fewer perturbations than A2T. CCS Concepts: â€¢Software and its engineering â†’Software testing and debugging ;â€¢Human-centered computing ;â€¢Computing methodologies â†’Learning paradigms ;Natural language processing ; Additional Key Words and Phrases: SE4AI, Oracle Problem, Trustworthy Text Classifier, Explainability Warning: This paper contains extracts from datasets on mental health and hate speech, which may upset some readers. Reader discretion is advised. Authorsâ€™ Contact Information: Lam Nguyen Tung, lam.nguyentung@monash.edu, Monash University, Melbourne, Victoria, Australia; Steven Cho, scho518@aucklanduni.ac.nz, University of Auckland, Auckland, New Zealand; Xiaoning Du, xiaoning. du@monash.edu, Monash University, Melbourne, Victoria, Australia; Neelofar Neelofar, neelofar.neelofar@monash.edu, Monash University, Melbourne, Victoria, Australia; Valerio Terragni, v.terragni@auckland.ac.nz, University of Auckland, Auckland, New Zealand; Stefano Ruberto, Stefano.RUBERTO@ec.europa.eu, JRC European Commission, Ispra, Italy; Aldeida Aleti, aldeida.aleti@monash.edu, Monash University, Melbourne, Victoria, Australia.arXiv:2410.22663v1 [cs.SE] 30 Oct 2024

[PÃ¡gina 2]
2 Lam et al. Prediction probabilities 0.999 negative 0.001 positive Sounds like a great idea, BUT ....... It read on the back cover like a great book. Inside though is another story . I am halfway thru it and I have not seen him CLEAN anything . It is too slow, but I want to still read to ï¬nd out what happens .It is a rough patch now but I am sloggin through . If it does not pick up soon I will give up.W elty69 (a) Original text Prediction probabilities 0.070 negative 0.930 positive Sounds like a great idea, BUT ....... It read on the back cover like a great book. Inside though is another story . I am halfway thru it and I have not seen him CLEAN anything . It is too slow, but I want to still read to ï¬nd out what happens . It is a rough patch now but I am sloggin through . If it does not pick up soon I will give up.Welty69 (b) Some irrelevant words (strikethrough) removed Prediction probabilities 0.010 negative 0.990 positive I noticed that my garage door wouldn't go down and one of the sensor's led was off. I checked with Sears. Their replacement part plus extravagant shipping and handling would make it a better value to simply replace the whole garage door opener with a new one.My web search revealed this Company's part which turns out is the identical replacement part at a reasonable price . I let my 18 year old do the replacement hook up, which took about 20 minutes (we took our time). Works like new now. (c) A trustworthy prediction of a positive review Prediction probabilities 0.928 negative 0.072 positive I noticed that my back door wouldn't go down and one of the sensor's led was off. I checked through Sears. Their replacement part plus extravagant shipping and handling would make anything a better value to simply replace the whole back door opener with a new one.My web search revealed this Company's part which turns out is the identical replacement part at a reasonable price . I let my 18 year old do the replacement hook up, which took about 20 minutes ( we took our time). Works like new now. (d) Irrelevant words (underlined) injected Fig. 1. LIME explanations of predictions analysing amazon reviews. Words highlighted in orange and blue indicate which class, negative orpositive , they contribute to, with the shade representing their importance. 1 Introduction Machine learning (ML) holds significant importance in contemporary advanced systems, such as spam detection, clinical text analysis, and vulnerability detection, with text classification being a primary application. Despite their superior performance during development, ML models can still fail in real-life scenarios [ 9,28], raising concerns about trusting their decisions. When assessing a model, it is important not only to evaluate its general task-solving ability using metrics, such as prediction uncertainty or classification accuracy, but also to understand its decision-making process [ 32]. Indeed, several studies show that these metrics alone are insufficient indicators of model reliability [ 8,42], as the model might learn spurious patterns [ 60], leading to overconfident decisions based on irrelevant features [21]. An ML model relying on irrelevant features, when faced with unseen data, can misclassify in the absence of these features or be fooled by them, leading to problematic predictions. We illustrate this using a Bert-based binary sentiment classifier [ 14] applied to amazon reviews. Figure 1 shows four review examples, the modelâ€™s predictions, and the probabilities of each class. We also highlight the important words steering these predictions, which are identified by LIME [ 49], an explanation method. Orange and blue colors highlight words contributing to the negative and positive classes, respectively, with darker shades indicating higher importance. The review in Figure 1a is correctly classified as negative with high confidence. However, a closer look at the most important words the classifier relies on reveals that words playing a vital role, such as â€œbackâ€, â€œtheâ€, â€œanythingâ€, and â€œthroughâ€, are unrelated to either class. When removing some of them, as shown in Figure 1b, the review is misclassified as positive by the same model, despite negative sentiment in phrases â€œnot seen him CLEANâ€, â€œslowâ€, â€œrough patchâ€, and â€œslogginâ€. To further explore the potential harm of this phenomenon, we synthesise a new review using a positive review shown in Figure 1c. This new review, as illustrated in Figure 1d, is injected with the words â€œbackâ€, â€œthroughâ€, and â€œanythingâ€. Although, these injections do not change the original meaning, the confident correct prediction is flipped to its opposite. These shifts underscore the importance of assessing whether the learned patterns are genuinely valid and generalisable or if they are merely based on spurious correlations within the training data, typically referred to as shortcut learning [16,21]. A shortcut learning model is unlikely to provide correct classifications for the right reasons. Therefore, it becomes ineffective once deployed in the real world, where spurious correlations are absent. To ensure the quality of ML systems in a reliable and cost-effective way, considerable effort has been focused on automating various aspects of the testing process, particularly through automated test oracles. Traditional software testing has inspired automated test oracles for ML

[PÃ¡gina 3]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 3 to test various properties, such as correctness, fairness, and robustness using techniques like metamorphic testing [ 23,45]. However, trustworthiness testing has not received as much attention and consequently remains less resolved. Trustworthiness refers to the ability of a model to make reasonable predictions based on relevant features, such as semantically related words in text classification. Developing automated test oracles [ 3] for trustworthiness testing is a long-lasting challenge hindering the broader adoption of ML models in the real world. Due to the lack of automated trustworthiness oracles, most studies [17, 28, 49] rely on human-based evaluation as the oracle to assess the ML prediction reasoning uncovered by explanation methods [ 65]. However, these approaches are time-consuming, error-prone, and not scalable [30, 60]. In this paper, we focus on the oracle problem of assessing the trustworthiness of predictions made by text classifiers. Intuitively, trustworthiness assessment can be done by measuring the semantic relatedness between each decision-contributing word and the class name. However, measuring the semantic relatedness between two words is non-trivial, as discussed in Section 2.1, Given a word, there exists a group of words naturally related to it with very similar semantics like its synonyms. Their semantic similarity can be easily measured with existing metrics, such as cosine similarity. Some other words can also be related to the target word, but require a semantic hop, which we call indirectly related words. For example, the words â€œ computer â€ and â€œ fileâ€ are naturally related. Although the word â€œ extension â€ may not seem naturally related to â€œ computer â€, it is strongly semantically connected to â€œ fileâ€ and thus indirectly related to â€œ computer â€. We argue that examining the distribution of words helps better recognise semantically related words. Based on this, we propose an automated trustworthiness oracle generation method that leverages explanation methods to extract decision-contributing words and assesses their semantic relatedness to the class based on such distribution. The key idea is to identify keywords, which act as anchors for assessing semantic relatedness and indicate what a text classifier should rely on for predictions. A prediction is then deemed trustworthy if it is mainly based on these keywords. For instance, the prediction in Figure 1a is untrustworthy because the top contributing words, â€œbackâ€, â€œtheâ€, â€œanythingâ€, and â€œthroughâ€, are all semantically unrelated to â€œnegativeâ€. To reveal the negative impact of vulnerabilities in trustworthiness, we also design an adversarial attack method guided by trustworthiness oracles. Our main contributions are summarised as follows. â€¢TOKI, the first approach for generating automated trustworthiness oracles. â€¢A novel attack method that targets trustworthiness vulnerabilities identified by TOKI. â€¢A benchmark for trustworthiness assessment for text classification, containing approx- imately 6,000 predictions in various domains, including topic classification, sentiment analysis, clinical mental text classification, and hate speech detection. â€¢An investigation on the relation between the uncertainty and trustworthiness of predictions, revealing that relying on prediction uncertainty metrics, such as model confidence, overlooks untrustworthy high-confidence predictions and trustworthy low-confidence predictions. â€¢An ablation study and comparative evaluations of TOKI. For trustworthiness assessment, we compare TOKI with a naive baseline solely based on model confidence. For adversarial attacks, we compare our method with A2T [ 61], a state-of-the-art adversarial attack method. The results demonstrate TOKIâ€™s superior effectiveness and efficiency. 2 Problem Definition and Motivation This section clarifies the definition of trustworthiness used in the paper, the trustworthiness oracle problem, and our intuition to address it.

[PÃ¡gina 4]
4 Lam et al. 2.1 Definition of Trustworthiness Trustworthiness is a complex concept that has raised numerous scholarly debates among researchers. This paper focuses on trustworthiness, in particular, whether a model makes predictions based on valid and reasonable patterns rather than spurious ones. Hence, we adopt the definition of trustworthiness proposed by KÃ¤stner et al. [27] as shown in Definition 1. Definition 1. An ML model is trustworthy to a stakeholder in a given context if and only if it works properly in the context and the stakeholder has justified belief in it. KÃ¤stner et al. also distinguish between trustworthiness and trust: trustworthiness is a property of an ML model, while trust is the perception a person has towards it [ 27]. Therefore, people can still trust an untrustworthy ML model. According to Definition 1, understanding a model thoroughly helps justify our beliefs about how well it works. Model explanations can serve as a means to gain this understanding and justification [ 58], thereby promoting trustworthiness [ 7,27]. There are two main types of model explanations: global [ 9], which provides insights into the entire modelâ€™s inner workings, and local [ 31,38,49], which focuses on individual predictions. Local explanations, by breaking down a model into its components, allow users to grasp its functionality and decision-making process in a way that aligns with human cognitive patterns. For this reason, they are more readily applicable [ 1]. Hence, we leverage local explanations to uncover the reasoning behind individual predictions by highlighting the most relevant features contributing to those predictions. A trustworthy model should make correct predictions, and the reasoning behind them, supported by explanations, should also be plausible. In this way, we propose the following definition of a trustworthy prediction. Definition 2. A trustworthy prediction is correct and the reasoning behind it is also plausible . In the context of text classification, a trustworthy prediction should rely on words in the input text that align with human reasoning. To simulate human judgement, the reasoning behind a prediction can be considered plausible if words contributing the most to the prediction are semantically related to the predicted class. We follow the definition of semantic relatedness described by Budanitsky et al. [ 6]. Words can be semantically related by lexical relationships, such as meronymy (carâ€“wheel) and antonymy (hotâ€“cold), or just by any kind of functional association or other â€œnon-classical relationsâ€ (pencilâ€“paper, penguinâ€“Antarctica, and rainâ€“flood) [ 39]. It is important to distinguish semantic relatedness from semantic similarity . Semantic relatedness is a more general concept [ 6] while semantic similarity refers to the degree of overlap or resemblance in meaning between two words [ 54]. For example, â€œdeskâ€ and â€œchairâ€ are semantically related but not semantically similar, â€œdeskâ€ and â€œtableâ€, on the other hand, are both semantically related and semantically similar. It is also important to note that semantically relatedness is contextual, meaning two words are semantically related in a specific context, but might not in others. For instance, â€œbankâ€ in â€œ the bank exploits small firms â€ is semantically related to economics, while â€œbankâ€ in â€œ we walked along the river bank â€ is not. Definition 2 describes a trustworthy prediction, referring to local trustworthiness. In contrast, global trustworthiness is the ability to make trustworthy predictions across a broad range of inputs. A globally trustworthy model can perfectly interpret every input like human domain experts. Although achieving global trustworthiness is the ultimate goal, it is challenging due to the more complex architectures and training procedures required, which result in higher computational costs. Therefore, we focus on local trustworthiness of individual predictions. While most observed predictions by a model are trustworthy, this does not directly mean the entire model is trustworthy.

[PÃ¡gina 5]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 5 Prediction probabilities 0.823 negative 0.177 positive Sounds like a right on, way....... It read on the back cover like a great book. Inside though is another story . I am halfway thru it and I have not seen him CLEAN anything . a is too slow , but I want to still read to ï¬nd out what happens .over is a rough patch now but I am sloggin through. If it does not pick up soon she will keep up.Welty69 (a) An adversarial example generated by TextAttack Prediction probabilities 0.999 negative 0.001 positive Sounds like a great recommendation , NEVER THELESS ....... This read on this end cover like a great book. Inside though is another story . Me am halfway thru it and me have n't seen him CLEAN whatsoever . This is too obtuse , but me want to still read to ï¬nd out what happens .This is a rough patch now but me am sloggin by. If it does n't pick up soon me will give up.Welty69 (b) An adversarial example generated manually Fig. 2. LIME explanations of predictions analysing amazon reviews. Bold words represent perturbations, while words highlighted in orange and blue indicate their contribution to the negative orpositive class, with the shade reflecting their importance. 2.2 Trustworthiness and Robustness Sharing the same goal of improving the modelâ€™s generalisability, robustness testing might be able to identify problems that can be spotted during trustworthiness testing. Local robustness measures the modelâ€™s ability to retain its prediction on a sample under perturbations, also known as adversarial examples [ 63], that do not affect human perception and decision. For example, removing the header in Figure 1b should not alter a humanâ€™s decision to classify it as Atheism. However, the model fails to maintain its original decision on this example, and we can conclude that the adversarial example reveals a local robustness issue. Before investigating the oracle problem of model trustworthiness, one important question we need to answer is that to what extent does the trustworthiness problem differ from the robustness problem? More essentially, are there any issues that can be uncovered during trustworthiness testing but not during robustness testing? When a modelâ€™s prediction is trustworthy on a sample, the prediction is correct and based on justifiable reasons. This prediction is not necessarily robust, meaning it might be altered under minor perturbations, such as replacing the decision-essential words with their semantic equivalents, or removing/modifying the decision-unessential ones. On the other hand, when a prediction by the model is robust, it is still possible that the model relies on some justifiable shortcut reasons, resulting in problematic predictions in the future where the shortcuts are absent or the shortcuts appear in samples having opposite classes. Hence, we conclude that the trustworthiness problem overlaps with the robustness problem, but they are not the same. We differentiate trustworthiness from robustness by revisiting the prediction in Figure 1a, show- ing that robustness testing is unable to identify issues in the modelâ€™s behavior, but trustworthiness testing can. TextAttack, a framework implementing several state-of-the-art adversarial example generation methods [ 40], is applied to attack [ 63] the model on the same text input. An additional ad- versarial example is generated manually to ensure grammar and semantic preservation by replacing orange-highlighted words in Figure 1a with their synonyms from WordNet [ 37]. The substitutions include { â‘ ideaâ€“recommendation, â‘¡BUTâ€“NEVERTHELESS, â‘¢Itâ€“This, â‘£theâ€“this, â‘¤backâ€“end, â‘¥anythingâ€“whatsoever, â‘¦slowâ€“obtuse, â‘§Iâ€“me,â‘¨throughâ€“by, â‘©notâ€“nâ€™t}. Figures 2a and 2b, with bold words representing perturbations, show LIME explanations for two predictions to adver- sarial examples generated by TextAttack and manually, respectively. All adversarial examples fail to attack the model since it maintains the same prediction. These results indicate that although the model is robust against certain adversarial examples, its prediction can still be untrustworthy. 2.3 Trustworthiness Oracle ML testing involves providing a model with inputs and observing its responses. The oracle prob- lem [ 3] in ML testing is the challenge of determining whether these responses are appropriate. In trustworthiness testing, a response is an explainable prediction, as described in Definition 3.

[PÃ¡gina 6]
6 Lam et al. deploy Model Real-world dataAI enginnerdevelop predictpredict Misclassifiedgood relationship anxiety birthday suicideChristmasclean Correctdetected r/offmychest nursing student anxiety take prozac suicide (a) Traditional development process Trustworthiness Oracleassess alert issuesdeploy Model Real-world dataAI enginnerdevelop predictpredict Correctdetected anxiety good relationship Christmas birthdaysuicide Correctdetected r/offmychest nursing student anxiety take prozac suiciderepair r/offmychest nursing student (b) Integration with trustworthiness testing Fig. 3. Comparison between two ML system development processes: without and with trustworthiness testing. Blue lines indicate the differences between these processes. Definition 3. For the classifier ğ‘š,ğ‘¥is an individual data input into ğ‘š, andğ‘=âŸ¨Ë†ğ‘¦,ğ‘’âŸ©is the explainable prediction toğ‘¥ofğ‘š, where Ë†ğ‘¦is the predicted class and ğ‘’is the explanation. The explainable prediction ğ‘assigns the input ğ‘¥to the predicted class Ë†ğ‘¦, with its reasoning explained by the explanation ğ‘’. The explanation ğ‘’is a list of decision-contributing words and the corresponding importance scores measuring their contribution to the prediction ğ‘. Three cases can occur with an explainable prediction: (1) incorrect, (2) correct due to semantically related words, and (3) correct due to semantically unrelated words. In the first case, investigating incorrectness becomes more important than trustworthiness assessment. Although the explanation might hint at why the prediction is incorrect, we leave this interesting avenue for future work. In this way, this paper only focuses on differentiating the last two cases, meaning that it examines only correct explainable predictions in the context of trustworthiness testing. We then adopt the definition of test oracles from Barr et al. [ 3]. Definition 4 describes trustworthiness test data while Definition 5 defines trustworthiness oracles. Definition 4. For the classifier ğ‘š, X is the dataset correctly predicted by ğ‘šand P is the set of correct explainable predictions to an instance of ğ‘š. Trustworthiness test data forms the set ğ‘‡=ğ‘‹âŠğ‘ƒ. Definition 5. A trustworthiness oracle ğ·:ğ‘‡â†¦â†’Bis a function from an instance of trustworthiness test datağ‘¡to true or false, indicating whether the prediction p in t is trustworthy according to Definition 2. A trustworthiness oracle is a predicate that determines whether the prediction in an individual trustworthiness test data is trustworthy according to Definition 2. The trustworthiness oracle formulated in Definition 5 can be applied to any text classifier, as long as there is an explanation method that uncovers the reasoning behind its predictions in the form of word attributions. 2.4 Applications of Trustworthiness Oracles The trustworthiness problem exists in various ML applications. For example, a clinical mental text classifier from social media posts shows good held-out performance but might rely on irrelevant clues frequently present in the training data, such as post tags, job positions, and special occa- sions [ 24]. When deployed in the real world, such classifier is unlikely to perform well on unseen data. Hence, it is crucial to assess the reasoning of ML models to detect any behavioral issues before deploying them in real-world scenarios, rather than only evaluating their classification performance using metrics such as accuracy, precision, recall, and F1-score. Figure 3a shows a traditional development process of ML systems. AI engineers collect all available data, preprocess and split it into training and test sets. The training data is used to train the model, while the test data evaluates its performance. If the model performs poorly, the engineers improve the learning algorithm to enhance performance. Once the model achieves good held-out performance, it can be deployed in the real world. However, its performance often deteriorates due to the modelâ€™s reliance on spurious correlations learned during training, making it overconfident

[PÃ¡gina 7]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 7 Training dataModel under testp1 p2 pn Predictionsexpl1 LIMEExplanation methodexpl2 expln Explanations Test instance xp expl Predicted class. . . . . .. . .m word pool wp1 ... wpm for m classes classm - keywordsnegative 'worst': 1.05 'stupid': 0.64 'script': 0.21 'material': 0.09 ...positive 'good': 1.24 'interesting': 0.7 'plot': 0.73 'happy': 0.52 .... . . explp ... embedding of word kx ... embedding of word 1 ...embedding of word 21-NN in wpp ...check with p-keywordssimw1 relatedness between word 1 and class p relatedness between word kx and class p... ... embedding of word k1 ... embedding of word 1 ...embedding of word 2Hierarchical Clustering Cluster selection based on relatedness ...class1 - keywords trust- worthi- ness labelget only words contributing to predicting p Fig. 4. The process of TOKI, with blue and yellow background colors indicating the first and second pipelines. in held-out evaluations. In the real world, where these correlations are absent, the model tends to misclassify unseen data. A common solution is to augment the dataset with new manually annotated data and retrain the model, which is costly and inefficient. Figure 3b shows the process integrated with trustworthiness testing. In addition to evaluating traditional performance metrics, this process also assesses the modelâ€™s behavior behind correct predictions. In this process, trustworthiness oracles determine whether a prediction is trustworthy or based on spurious correlations. Identifying such issues allows the engineers to mitigate the impact of spurious correlations, ensure the model is correct for the right reasons, and improve its generalizability. If undetected, these issues can lead to problematic predictions in the future. Human annotations often serve as trustworthiness oracles, but this approach is not scalable [60]. For this reason, developing automated trustworthiness oracles can greatly advance the development and applications of ML systems in the real world. 3 TOKI: Trustworthiness Oracle through Keyword Identification As outlined in Section 2, a prediction is considered trustworthy if its reasoning is plausible. In text classification, a trustworthy prediction should rely on words semantically related to the predicted class. We argue that examining the distribution of words helps better recognise these semantically related words. Specifically, directly related words can act as anchors, and words indirectly related to the class are likely to form clusters around them. Following this intuition, we present Trustworthiness Oracle through Keyword Identification (TOKI). The key idea of TOKI is to use a list of keywords for each class to indicate what a text classifier should rely on to make predictions. To identify these keywords, TOKI selects clusters containing directly related words and their surrounding indirectly related words. A prediction is then deemed trustworthy if it mainly relies on the keywords of the predicted class. While the ultimate goal is to identify a complete list of keywords, this is impractical due to resource and computational constraints. Therefore, we identify only a partial list of keywords by extracting decision-contributing words from the classifierâ€™s responses on the training data and applying clustering analysis to them. Since not all decision-contributing words are genuinely related to the class, clustering analysis also helps separate related words from unrelated ones. Figure 4 describes the process of TOKI, which consists of two pipelines: keyword identification and trustworthiness label computation. 3.1 Keyword Identification ML models learn correlations in training data, including both valid and spurious ones [ 21]. Therefore, not all predictions are solely based on the spurious correlations . Predictions reflecting the valid corre- lations rely on words semantically related to the predicted class, regardless of their importance scores .

[PÃ¡gina 8]
8 Lam et al. While the spurious correlations can make predictions untrustworthy, TOKI identifies keywords for each class through the valid correlations, following four steps. Step 1: Explaining .To extract keywords, we focus on analysing the reasoning behind correct predictions, as incorrect ones might provide meaningless reasoning. Therefore, TOKI identifies the decision-contributing words from instances that have been predicted correctly in the training set. Letğ‘‹,ğ‘“(Â·), andğ‘represent the training set, the text classifier, and the true class, respectively. The explaining step is formalised in Equation 1, which returns a list of explanations ğ¸ğ‘for each class ğ‘. ğ¸ğ‘= ğ‘’(ğ‘“,ğ‘¥)|ğ‘¥âˆˆğ‘‹,ğ‘“(ğ‘¥)=ğ‘ (1) TOKI leverages the explanation method ğ‘’(Â·,Â·)to measure the contribution ğ‘ of each word ğ‘¤ to a prediction. Several methods, such as LIME [ 49], achieve this by locally approximating the model as an interpretable surrogate model. Other methods [ 31] perturb input and evaluate model output changes. Another common way [ 38] is to compute the gradient of the output with respect to the input. All these methods generate explanations for the prediction ğ‘“(ğ‘¥)in the form of ğ‘’(ğ‘“,ğ‘¥)={âŸ¨ğ‘¤,ğ‘ âŸ©}, a list of top decision-contributing words with their importance scores. Step 2: Word pool construction .A word pool of decision-contributing words and their averaged importance scores across explanations is created for each class, as formalised in Equation 2. ğ‘Šğ‘= âŸ¨ğ‘¤,ğ‘ ğ‘¤âŸ©|ğ‘¤âˆˆğ¸ğ‘ ,whereğ‘ ğ‘¤=Ã ğ‘’âˆˆğ¸ğ‘,âŸ¨ğ‘¤,ğ‘ ğ‘–âŸ©âˆˆğ‘’ğ‘ ğ‘–Ã ğ‘’âˆˆğ¸ğ‘,âŸ¨ğ‘¤,ğ‘ ğ‘–âŸ©âˆˆğ‘’1(2) Words in the explanations ğ¸ğ‘are categorised based on the predicted class ğ‘, with their importance scores averaged. This results in a word pool ğ‘Šğ‘={âŸ¨ğ‘¤,ğ‘ ğ‘¤âŸ©}for the class ğ‘, containing words and their averaged importance scores. The averaged importance score ğ‘ ğ‘¤also indicates the correlation between the word ğ‘¤and the class ğ‘. Step 3: Word clustering .The word pool ğ‘Šğ‘of the classğ‘contains both keywords and unrelated words. To distinguish them and collect both directly related and indirectly related words, TOKI clusters the word pool ğ‘Šğ‘, as formalised in Equation 3. ğ¶ğ‘=hierarchical_cluster ğ‘Šğ‘,ğœƒdist (3) TOKI transforms the words in ğ‘Šğ‘into embeddings using word embedding methods. Since the num- ber of clusters is unknown, hierarchical clustering [ 43] is applied to group these word embeddings based on their cosine similarities. Clusters ğ¶ğ‘of the classğ‘are obtained by cutting the dendrogram, a hierarchical tree of relationships between the word embeddings, at the threshold distance ğœƒdist. Step 4: Keyword selection .The list of keywords is identified by selecting the clusters of words semantically related to the class name, as described in Equation 4. ğ¾ğ‘=Ã˜ ğ¶ğ‘–âˆˆğ¶ğ‘n ğ¶ğ‘–|ğ‘ ğ‘–ğ‘š ğ¶ğ‘¤ ğ‘–,ğ‘ â‰¥ğœƒrelateo ,andğ¹ğ‘=ğ‘Šğ‘\ğ¾ğ‘ (4) In TOKI, the word cluster ğ¶ğ‘–is directly related to the class ğ‘ifğ‘ ğ‘–ğ‘š ğ¶ğ‘¤ ğ‘–,ğ‘ â‰¥ğœƒrelate. Here,ğ¶ğ‘¤ ğ‘–is the mean vector of all the embeddings of words in ğ¶ğ‘–,ğ‘ ğ‘–ğ‘š(Â·,Â·)measures the cosine similarity between two word embeddings, and ğœƒrelate is the threshold relatedness. After this step, the list of keywords ğ¾ğ‘of the classğ‘is identified while the remaining words form a list of non-keywords ğ¹ğ‘. Whileğœƒdistneeds manual configuration, ğœƒrelate can be automatically estimated by turning it into a binary classification problem. The key idea to determine ğœƒrelate is that related pairs of words can be found via synonyms. To accomplish this, the top 1,000 most common English words are taken from WordNet [ 37]. Then, TOKI uses the Merriam-Webster dictionary to find all single-word

[PÃ¡gina 9]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 9 synonyms of each word, resulting in approximately 32,000 pairs of related words. Another 32,000 random pairs of words are generated from WordNet to create a list of unrelated pairs. Finally, TOKI determines the value of ğœƒrelate through a binary search on the word embeddings of these lists. At each iteration, all 64,000 pairs are classified, with each pair of words considered as related if the cosine similarity between two word embeddings is higher than or equal to the current ğœƒrelate. The search stops when precision and recall for both related and unrelated classifications are balanced. The value of ğœƒrelate varies depending on the word embedding method, as different methods have their unique ways of embedding, thereby impacting the measurement of similarity between words. 3.2 Trustworthiness Label Computation The second pipeline of TOKI, as highlighted in yellow in Figure 4, focuses on assessing the trustwor- thiness of a correct prediction. The trustworthiness label is determined by comparing the impacts between related and unrelated decision-contributing words based on their total importance scores. To determine whether a decision-contributing word is related to the class, TOKI uses keywords as anchors to assess semantic relatedness. We define an indicator function ğ‘Ÿ(ğ‘¤,ğ‘)for this purpose by checking whether the nearest word in ğ‘Šğ‘toğ‘¤is a keyword, as shown in Equation 5. We then formalise the second pipeline in Equation 6. ğ‘Ÿ(ğ‘¤,ğ‘)= 1,ifmax âŸ¨ğ‘¤ğ‘–,_âŸ©âˆˆğ¾ğ‘ğ‘ ğ‘–ğ‘š(ğ‘¤ğ‘–,ğ‘¤)â‰¥ max âŸ¨ğ‘¤ğ‘–,_âŸ©âˆˆğ¹ğ‘ğ‘ ğ‘–ğ‘š(ğ‘¤ğ‘–,ğ‘¤), 0,otherwise.(5) ğ¼ğ‘†rel=âˆ‘ï¸ âŸ¨ğ‘¤ğ‘–,ğ‘ ğ‘–âŸ©âˆˆğ‘’(ğ‘“,ğ‘¥ğ‘¡)ğ‘ ğ‘–âˆ—ğ‘Ÿ(ğ‘¤ğ‘–,ğ‘),andğ¼ğ‘†unr=âˆ‘ï¸ âŸ¨ğ‘¤ğ‘–,ğ‘ ğ‘–âŸ©âˆˆğ‘’(ğ‘“,ğ‘¥ğ‘¡)ğ‘ ğ‘–âˆ—(1âˆ’ğ‘Ÿ(ğ‘¤ğ‘–,ğ‘)) ğ·(ğ‘“,ğ‘¥ğ‘¡,ğ‘,ğ‘’)=ğ¼ğ‘†relâ‰¥ğ¼ğ‘†unr(6) TOKI leverages the explanation method ğ‘’(Â·,Â·)to extract decision-contributing words for the pre- diction to the given input ğ‘¥ğ‘¡. For each word ğ‘¤, TOKI identifies the most similar word in the word poolğ‘Šğ‘of the predicted class ğ‘, which is constructed in the first pipeline, by measuring the cosine similarity between their embeddings. The semantic relatedness between each word and the class is determined by checking whether the most similar word is a keyword. Next, TOKI computes the total importance scores ğ¼ğ‘†relandğ¼ğ‘†unrfor semantically related and unrelated words, respectively. Based on the difference between them, the trustworthiness oracle ğ·finally assigns a trustworthiness label (trustworthy or untrustworthy) to the prediction. Word embeddings themselves can be biased due to their training data [ 57], potentially affecting the ability to measure semantic relatedness. To mitigate this, TOKI applies ensemble learning [ 29] by employing different word embedding methods. We use both static embedding methods [ 4,5,46,53], which produce a single output for each word and contextual embedding methods [ 10,14], which generate different vectors for the same word based on its context. Each method has a different way of vectorizing words, resulting in different similarity measurements between them. In the first pipeline, this affects the computation of ğœƒrelate and the keyword identification. In the second pipeline, different embedding methods identify different similar words in the word pool, leading to different decisions about how the word is related to the class. Decisions made by all embedding methods are combined using plurality voting [29] in both pipelines. 4 Targeted Adversarial Attacks on Trustworthiness Vulnerabilities We introduce a novel adversarial attack method guided by TOKI. The key idea is to weaken valid cor- relations by replacing words with similar ones that are weakly correlated to the original class, while strengthening spurious correlations by injecting unrelated words strongly correlated to other classes.

[PÃ¡gina 10]
10 Lam et al. Table 1. Comparing TOKI-guided attack method (Ours) and A2T [61] Components TOKI-guided attack method (Ours) A2T [61] Word Ranking Method Gradient-based Word Importance Gradient-based Word Importance Source of Synonyms Trust worthiness Oracle Counter-fitted Embedding Word Substitution Word Embedding + Word -Class Correlation Word Embedding ConstraintsModification Rate DistilBERT Cosine Similarity Part-of-Speech ConsistencyModification Rate DistilBERT Cosine Similarity Part-of-Speech Consistency While the story does seem pretty unbelievable at times, itâ€™ s awfully fun to watch. While the story does seem pretty unbelievable at times, itâ€™ s awfully humorous to watch.A2T TrularWhile the story does seem pretty unbelievable at times, itâ€™ s awfully entertaining to watch.Replaced wor d: entertaining Replaced wor d: unbelievable While the story does seem pretty extraordinary at times, itâ€™s awfully fun to watch. failed success successOriginal text TOKI Fig. 5. Adversarial examples generated by TOKI (Ours) and A2T. Words highlighted in orange or blue show their contribution to the negative or positive class, with the shade indicating their level of importance. Table 1 compares TOKI-guided attack method with existing adversarial attack methods, exemplified by the state-of-the-art A2T [ 61]. A2T uses the gradient of the loss to determine the substitution order of words based on their importance scores in the prediction. It then iteratively replaces each word with synonyms generated from a counter-fitted word embedding model [ 41]. A modification rate sets the maximum number of perturbations allowed in the original text. The generated texts are subsequently filtered to ensure part-of-speech consistency and semantic preservation by evaluating the cosine similarity between the sentence encodings of the original and perturbed texts. A2T has been validated and demonstrated to be a strong adversarial attack method [66]. Our method follows the same architecture as A2T. The key difference is that it finds synonyms in word pools ğ‘Šconstructed by TOKI, based on word-class correlations measured by the averaged importance scores ğ‘ . To replace a word, TOKI-guided attack method checks whether the word is related to the original predicted class. If it is, the word is replaced with a similar keyword of that class that has a low importance score . Otherwise, it is replaced with a similar non-keyword from other classes that has a high importance score . This mechanism tricks the model into using unrelated words as cues for predictions to cause misclassification, increasing the likelihood of successful attacks while reducing the number of perturbations. Figure 5 compares adversarial examples generated by TOKI and A2T on the same input, showing that A2T requires more perturbations to succeed. The text is initially predicted as positive mainly based on the words â€œentertainingâ€ and â€œunbelievableâ€. A2T first substitutes â€œentertainingâ€ with â€œfunâ€, a synonym identified by the counter-fitted word embedding model. However, this change fails to attack the model, as the positive clues â€œfunâ€ and â€œunbelievableâ€ still dominate. A2T then replaces â€œunbelievableâ€ with its synonym â€œextraordinaryâ€. This time, the overall impact of positive clues is reduced, allowing the negative clue â€œawfullyâ€ to dominate. A2T substitutes words based only on the semantic meaning, requiring two substitutions to create a successful adversarial example. In contrast, TOKI-guided attack method replaces â€œentertainingâ€ with a similar word â€œhumorousâ€. This word is deemed weakly correlated to the positive class by TOKI in this case, allowing the negative clues to dominate and alter the prediction with just one substitution.

[PÃ¡gina 11]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 11 Table 2. Trustworthiness benchmark DatasetData Statistics Model Under Test Number of top wordsImportance Type Trust Untrust Total Model Type Accuracy Dongâ€™s [15]movie311 47 358MLP 0.83210, 20importance 20news MLP 0.939 equivalent Garg et al.â€™s [20]CAMS 1,206 739 1,945 mentalbert-base-uncased 0.397 10importance equivalent Mathew et al.â€™s [36]HateXplain 3,002 304 3,306 bert-base-uncased 0.797 10importance equivalent Oursamazon_polarity 226 19 245roberta-base-cased 0.960 5, 10, 20importance differentag_news bert-base-uncased 0.934 rotten_tomatoes distilbert-base-uncased 0.841 yahoo_answers_topics bert-base-uncased 0.750 imdb distilbert-base-uncased 0.928 emotion distilbert-base-uncased 0.926 5 Trustworthiness Benchmark Trustworthiness oracles aim to determine whether a prediction is trustworthy, as described in Section 2. Evaluating them requires datasets containing model predictions, corresponding explana- tions, and trustworthiness labels (trustworthy or untrustworthy). We call them â€œtrustworthiness datasetsâ€ to distinguish them from â€œdatasetsâ€ used to train and validate ML models. However, limited trustworthiness datasets are available for such evaluation [51]. Dongâ€™s trustworthiness dataset. The human-based evaluation conducted by Dong [ 15] is one of the most relevant studies. This evaluation uses two datasets: a subset [ 49] of the 20 newsgroups (20news ) differentiating Christianity from Atheism, and movie reviews with sentiment labels. In the evaluation, text inputs, highlighting the top words identified by the explanation method, are shown to crowdworkers. They then guess the systemâ€™s output and state their confidence on a five-point Likert scale, ranging from â€œstrongly disagreeâ€ to â€œstrongly agreeâ€. To derive a trustworthiness dataset, only answers, where the model correctly predicts the output, are selected from the original crowdworkersâ€™ responses. The answers, where crowdworkers either guess incorrectly with high confidence (4â€“5) or correctly with low confidence (1â€“2), are deemed untrustworthy. Other answers, where the crowdworkers guess correctly with high confidence (4â€“5), are trustworthy. Trustworthi- ness labels from the crowdworkers for the same prediction are combined using plurality voting [ 29] to determine the final trustworthiness label. CAMS and HateXplain. CAMS [ 20] is a corpus for classifying mental health issues from social media posts, while HateXplain [ 36] is a dataset for hate speech detection. Both datasets provide ground- truth explanations for each instance. In CAMS, annotations highlight phrases used as inferences for predictions. In HateXplain, tokens are labeled as 0 or 1 to indicate whether they are part of the explanation. Trustworthiness datasets are created by explaining the models and comparing these explanations with the ground-truth ones. To measure the plausibility of the reasoning behind predictions, we follow Zini et al. [ 19] and use explanation precision =|ğ¸âˆ©ğº|/|ğ¸|, whereğ¸is the model explanation and ğºis the ground-truth explanation. High precision suggests that the model explanation is unlikely to provide a word not in the ground-truth explanation. A prediction is then considered trustworthy if its explanation precision â‰¥0.5. Importance-different trustworthiness dataset. We create an additional trustworthiness dataset in a more straightforward manner, with the distribution of importance scores among words considered. Six datasets and corresponding models are selected from Huggingface: amazon_polarity ,ag_news , rotten_tomatoes ,yahoo_answers_topics ,imdb , and emotion . We randomly sample 1,000 predictions

[PÃ¡gina 12]
12 Lam et al. and have three trained participants annotating their trustworthiness. For each prediction, the participants first guess the output of the text input. They then review the model prediction and its explanation generated by LIME [ 49], a state-of-the-art method known for its effectiveness and faithfulness [ 35,65]. Finally, the participants label the prediction as trustworthy or untrustworthy. Only predictions where both the model and participants guess the correct output are considered. The trustworthiness label for each prediction is determined by plurality voting [ 29] based on all annotations. It is observed that the participants often deem a prediction untrustworthy if unrelated words receive significantly higher importance scores in its explanation. The final trustworthiness benchmark is summarised in Table 2. Explanation method. Three explanation methods are used to extract decision-contributing words. â€¢LIME approximates the model locally with an interpretable model on perturbed samples created around the input [49]. Our experiments use 5,000 perturbed samples. â€¢Word omission [31] estimates the contribution of individual words by deleting them and measuring the change in probability for the predicted class [15]. â€¢Gradient computes the gradient of the output with respect to the input [38]. Models under test. For Dongâ€™s, CAMS and HateXplain datasets, models provided by the authors [ 15, 20,36] are used. For the remaining datasets, popular fine-tuned models from Huggingface are chosen. Table 2 presents all the models under test and their corresponding accuracies. 6 Evaluation This section describes a series of experiments conducted to evaluate how well TOKI addresses the trustworthiness oracle problem and adversarial attacks text classifiers. 6.1 Baselines Trustworthiness assessment. We adopt a naive approach, named Naive , which is based on model confidence. Naive considers a prediction untrustworthy if its confidence is lower than a threshold ğœƒconf. For an ablation study, we use a TOKIâ€™s variant, called TOKI (â€“K.I) , which directly measures the relatedness between decision-contributing words and class names without identifying keywords. Adversarial attack. We compare TOKI-guided attack method with A2T [61], a state-of-the-art adversarial attack method that has been validated to be strong and effective [66]. 6.2 Research Questions We answer three research questions to investigate the trustworthiness problem and evaluate TOKI. RQ1. Does a predictionâ€™s uncertainty reflect its trustworthiness? RQ1.1. How are prediction uncertainty and trustworthiness related? RQ1.2. What is the optimal configuration for ğœƒconfof Naive? RQ2. How effective and efficient is TOKI in trustworthiness assessment? RQ2.1. What is the optimal configuration for ğœƒdist? RQ2.2. How effective is TOKI compared to Naive? RQ2.3. How does identifying keywords affect TOKIâ€™s effectiveness and efficiency? RQ2.4. What is the impact of different explanation methods on TOKI? RQ3. How effective is TOKI-guided attack method compared to A2T? 6.3 Metrics Trustworthiness assessment. We frame trustworthiness assessment as a binary classification task with trustworthy and untrustworthy labels. The effectiveness is measured by: overall accuracy ;

[PÃ¡gina 13]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 13 precision ,sensitivity , and F1-score , which evaluate performance in detecting trustworthy predic- tions; specificity measures performance in detecting untrustworthy predictions; and geometric mean ( G-mean =âˆšï¸ sensitivityÃ—specificity ), which balances the classification performance on both labels. The efficiency of trustworthiness oracles is measured using their processing time in seconds. Adversarial attack. We report the attack success rate ( ASR ), defined as# of successful attacks # of total attacks, and the number of perturbations ( NP) to evaluate the effectiveness. We also use Bert [14] and USE [10] scores to assess the cosine similarity between original and adversarial examples. 6.4 Experimental Setup We used six different word embedding methods for the word embedding ensemble model of TOKI. â€¢NNLM [4] learns embeddings and language models using a feedforward neural network. â€¢GloVe [46] generates word embeddings from corpus word-to-word co-occurrence matrices. â€¢Swivel [53] generates low-dimensional embeddings from feature co-occurrence matrices. â€¢FastText [5] represents each word as a bag of character n-grams, capable of embedding misspelled, rare, or out-of-vocabulary words. â€¢USE [10] encodes sentences into embeddings for transfer learning to other tasks. â€¢Bert [14] is the first deeply bidirectional, unsupervised language representation, pre-trained on a large text corpus to condition both left and right contexts of each word. The trustworthiness benchmark described in Section 5 is used to evaluate trustworthiness oracles. For our datasets, 2,000 training instances are randomly sampled for explanations in the first pipeline of TOKI, while all training instances are used in the remaining datasets. Trustworthiness oracles employing LIME, omission, and gradient explanations are denoted by the suffixes â€œ-limeâ€, â€œ-omisâ€, and â€œ-gradâ€, respectively. Regarding adversarial attacks, we implement our method using TextAttack [ 40], which already includes the implementation of A2T [ 61], and use A2Tâ€™s default settings for both methods. We then attack random samples of up to 8,000 instances from each dataset listed in Table 2. All experiments are run 10 times on a Macbook M3 Pro with 12-core CPU, 18-core GPU, 18GB RAM, and 512GB SSD. The final results are averaged across these runs. 6.5 Results RQ1 : Relation between prediction uncertainty and trustworthiness. We use model confidence as a metric to assess the uncertainty of ML predictions. Figure 6 addresses RQ1.1 by illustrating the distribution of model confidence for two trustworthiness labels on the benchmark shown in Table 2. The majority of high confidence predictions are trustworthy. Highly confident (0.9â€“1.0) predictions that are trustworthy account for 76%â€“94% of predictions with highly strong confidence. Similarly, trustworthy predictions with high confidence (0.8â€“0.9) represent 49%â€“91% of high confidence predictions. While predictions with strong confidence are likely to be trustworthy, several confident predictions are still untrustworthy. Confident (0.8â€“1.0) but untrustworthy predictions make up 6%â€“29% of all confident predictions. In contrast, predictions with low confidence ( <0.8) can also be trustworthy, with 51%â€“91% of low confidence predictions being trustworthy. We now address RQ1.2 , finding the optimal value of ğœƒconffor Naive, which relies solely on model confidence. Figure 7 shows receiver-operating characteristic (ROC) curves of Naive across trustworthiness datasets. All ROC curves are near the random curve, indicating model confidence is limited to distinguish between trustworthy and untrustworthy predictions. Figure 6 further shows that increasing ğœƒconfon the y-axis decreases the false positive rate, where untrustworthy predictions are labeled as trustworthy, but also increases the false negative rate, where trustworthy predictions are labeled as untrustworthy. In other words, increasing ğœƒconfenhances specificity but

[PÃ¡gina 14]
14 Lam et al. untrustworthy trustworthy0.0 0.3 0.5 0.7 0.8 0.9 1.0confidence0 0 0 0 7 23 3 13 6 15 31 260 0100200 (a) Dongâ€™s [15] untrustworthy trustworthy0.0 0.3 0.5 0.7 0.8 0.9 1.0confidence22 18 171 152 175 179 63 98 110 104 198 655200400600 (b) CAMS [20] untrustworthy trustworthy0.0 0.3 0.5 0.7 0.8 0.9 1.0confidence0 0 0 0 85 270 49 279 50 518 120 1934 050010001500 (c) HateXplain [36] untrustworthy trustworthy0.0 0.3 0.5 0.7 0.8 0.9 1.0confidence0 0 0 1 1 4 0 5 1 7 17 209 0100200 (d) Ours Fig. 6. The distribution of predictionsâ€™ confidence across trustworthiness labels. 0.0 False Positive Rate0.0True Positive RateDataset: Dong's CAMS HateXplain Ours Random 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate0.00.20.40.60.81.0True Positive RateRandom Fig. 7. ROC curve of Naive. 0.0 0.2 0.4 0.6 0.8 1.0 conf 0.00.20.40.60.81.0G-mean Fig. 8. Naive with different ğœƒconf. 0.0 0.2 0.4 0.6 0.8 1.0 dist 0.00.20.40.60.81.0G-meanFig. 9. TOKI with different ğœƒdist. reduces sensitivity. Hence, we choose to maximise G-mean as the criteria to find ğœƒconf. Figure 8 illustrates Naiveâ€™s effectiveness in G-mean with different values of ğœƒconf. As the value of 0.9 achieves the best G-mean for Naive, we set ğœƒconfto 0.9 for the remaining experiments. Answer to RQ1: Uncertainty metrics of ML predictions, such as model confidence, are not suitable for assessing their trustworthiness. Although high confidence predictions tend to be trustworthy, relying on these metrics easily overlooks both trustworthy low confidence predictions and untrustworthy high confidence predictions. RQ2 : Effectiveness and Efficiency of TOKI. We also address RQ2.1 by measuring the effectiveness of TOKI-lime in G-mean with various values of ğœƒdist. The result shown in Figure 9 reveals that the optimal value of ğœƒdistranges from 0.2 to 0.4, depending on the models under test. Across all four trustworthiness datasets, when ğœƒdistexceeds 0.6, G-mean drops significantly. In this case, TOKI considers all words as keywords and biases for labeling predictions as trustworthy. In the subsequent experiments, we set ğœƒdistto 0.3, as it balances the effectiveness across the benchmark. Thenext experiment compares TOKI, Naive, and TOKI (â€“K.I) with different explanation methods against the benchmark outlined in Table 2. The experimental results are presented in Table 3. â€¢RQ2.2 :Comparing TOKI and Naive . TOKI consistently outperforms Naive across all datasets. Naive shows good precision but relatively low specificity and G-mean, especially on Dongâ€™s and our trustworthiness datasets. This indicates that Naive is biased toward labeling predic- tions as trustworthy, resulting in misclassifying untrustworthy predictions as trustworthy. â€¢RQ2.3 :Ablation study â€“ the effect of keywords . TOKI (â€“K.I) underperforms compared to TOKI with low accuracy and G-mean across all datasets. On Dongâ€™s and HateXplain trust- worthiness datasets, TOKI (â€“K.I) shows high specificity and precision, but other metrics remain low. This suggests a bias against labeling predictions as trustworthy, leading to misclassifying trustworthy predictions as untrustworthy. In terms of efficiency, TOKI (â€“K.I) and TOKI have similar processing times for computing trustworthiness labels, indicating that most of the time is spent on explaining predictions. Although keyword identification is time-consuming, it is only run once to assess the trustworthiness of a batch of predictions.

[PÃ¡gina 15]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 15 Table 3. Comparison between TOKI (Ours), Naive, and a TOKIâ€™s variant that is without keyword identification (indicated by the suffix â€“K.I) against the trustworthiness benchmark Dataset Method Acc (â†‘) Pre(â†‘) Sen(â†‘) F1(â†‘) Spec (â†‘)G-mean (â†‘)Training InstancesKeyword Identification Time (â†“s)Test InstancesTrustworthiness Label Computa- tion Time (â†“s)Dongâ€™s [15]TOKI-lime (Ours) 0.930 0.947 0.974 0.960 0.638 0.789 2,5094,866 3581,053 TOKI-omis (Ours) 0.882 0.912 0.956 0.934 0.392 0.612 98 379 TOKI-grad (Ours) 0.853 0.944 0.891 0.917 0.387 0.587 41 704 Naive 0.771 0.893 0.836 0.864 0.340 0.533 âœ— âœ— âœ— TOKI-lime (â€“K.I) 0.302 0.985 0.209 0.345 0.915 0.437 âœ— âœ—1,051 TOKI-omis (â€“K.I) 0.285 0.956 0.192 0.320 0.902 0.416 377 TOKI-grad (â€“K.I) 0.233 1.000 0.170 0.290 0.968 0.406 702CAMS [20]TOKI-lime (Ours) 0.878 0.872 0.941 0.905 0.775 0.854 2,158198,359 1,945224,540 TOKI-omis (Ours) 0.735 0.840 0.622 0.715 0.864 0.733 30,030 31,968 TOKI-grad (Ours) 0.539 0.888 0.499 0.639 0.719 0.599 2,366 3,116 Naive 0.615 0.768 0.543 0.636 0.732 0.631 âœ— âœ— âœ— TOKI-lime (â€“K.I) 0.422 0.648 0.343 0.449 0.551 0.435 âœ— âœ—224,435 TOKI-omis (â€“K.I) 0.442 0.557 0.232 0.327 0.683 0.398 31,912 TOKI-grad (â€“K.I) 0.388 0.836 0.356 0.500 0.531 0.435 3,071HateXplain [36]TOKI-lime (Ours) 0.933 0.966 0.961 0.963 0.661 0.797 2,364352,721 3,306464,720 TOKI-omis (Ours) 0.855 0.928 0.904 0.916 0.538 0.697 4,630 4,913 TOKI-grad (Ours) 0.684 0.375 0.385 0.380 0.785 0.550 5,334 7,303 Naive 0.640 0.942 0.645 0.765 0.605 0.624 âœ— âœ— âœ— TOKI-lime (â€“K.I) 0.103 0.953 0.014 0.027 0.984 0.117 âœ— âœ—464,606 TOKI-omis (â€“K.I) 0.137 0.879 0.010 0.020 0.975 0.099 4,818 TOKI-grad (â€“K.I) 0.737 0.586 0.020 0.039 0.979 0.140 7,271OursTOKI-lime (Ours) 0.922 0.977 0.938 0.957 0.737 0.831 12,000 1,126,453 24523,892 Naive 0.861 0.925 0.925 0.925 0.105 0.312 âœ— âœ— âœ— TOKI-lime (â€“K.I) 0.408 0.968 0.403 0.569 0.474 0.437 âœ— âœ— 23,849 â€¢RQ2.4 :The impact of explanation methods . Overall, TOKI demonstrates the best effectiveness with the state-of-the-art LIME. TOKI-lime achieves higher accuracy and G-mean compared to TOKI-omis and TOKI-grad, while other metrics show no significant differences, except for the CAMS dataset. Explanation methods also impact TOKIâ€™s efficiency. TOKI-lime is significantly slower due to LIMEâ€™s sampling of 5,000 neighbors, while omission and gradient are more lightweight. The model complexity also affects TOKIâ€™s efficiency, as the fine-tuned models consume more time than the two simpler MLPs in Dongâ€™s dataset. Answer to RQ2: TOKI outperforms the naive baseline based solely on model confidence, achieving the best effectiveness with LIME explanations. Without identifying keywords, TOKIâ€™s ability to measure semantic relatedness is limited, which can misclassify trustworthy predictions as untrustworthy. In terms of efficiency, TOKI depends on the explanation methods and the model complexity, as most of its processing time is spent on explaining predictions. RQ3 : Effectiveness of TOKI-guided attack method. Table 4 compares the effectiveness of TOKI-guided attack method and A2T [ 61]. Overall, TOKI-guided attack method outperforms A2T, achieving a higher ASR and lower NP across all models. Adversarial examples generated by TOKI also have higher Bert and USE scores than those by A2T, except for the USE score on the ag_news dataset, demonstrating TOKIâ€™s ability to preserve the semantic meaning of the original texts. Table 5 displays several adversarial examples generated by both methods. It is observed that TOKI can effectively find synonyms to successfully attack with fewer perturbed words, while A2T requires more perturbations and sometimes fails.

[PÃ¡gina 16]
16 Lam et al. Table 4. Experimental results comparing TOKI-guided attack method and A2T [61] Dataset InstancesTOKI-guided attack method (Ours) A2T SAR (â†‘)Avg. NP (â†“)Bert (â†‘) USE (â†‘) SAR (â†‘)Avg. NP (â†“)Bert (â†‘) USE (â†‘) movies 728 0.994 11.399 0.994 0.996 0.994 11.531 0.994 0.996 20news 926 0.458 21.058 0.977 0.979 0.456 21.089 0.976 0.979 CAMS 4,316 0.812 4.807 0.983 0.968 0.765 5.500 0.981 0.966 HateXplain 3,846 0.351 1.887 0.952 0.933 0.325 1.915 0.951 0.931 amazon_polarity 8,000 0.362 5.976 0.974 0.969 0.187 7.316 0.970 0.963 ag_news 7,600 0.192 2.481 0.965 0.949 0.151 2.668 0.963 0.952 rotten_tomatoes 1,066 0.500 1.626 0.952 0.921 0.334 1.898 0.950 0.919 yahoo_answers_topics 8,000 0.372 4.107 0.955 0.954 0.307 4.726 0.953 0.952 imdb 8,000 0.800 7.473 0.983 0.985 0.650 9.786 0.981 0.982 emotion 2,000 0.643 1.152 0.961 0.927 0.517 1.235 0.957 0.926 Table 5. Several adversarial examples: blue words indicate perturbations in the text, âœ“andâœ—represent successful and failed attacks, respectively Original [toxic]: If I am a moderate muzzy and do not stand against the extremists within my religion, I am a worthless dog who deserves the title of muzzy terrorist period.Original [medication]: When I manage to get a small victory against depression and â€œwake upâ€, I still have to fight against the pain that doesnâ€™t let me move without moaning. A2T [normal âœ“]:If I am a moderate muzzy and do not stand against the extremists within my cults, I am a point lessdog who meritsthe title of muzzy terrorist periods.A2T [jobs and careers âœ“]:When I manage to get a small winagainst down turn and â€œwake upâ€, I still have to fight against the grief that doesnâ€™t let me move without whin ing. TOKI [normal âœ“]:If I am a mild muzzy and do not stand against the extremists within my religion, I am a worthless dog who deserves the title of muzzy terrorist period.TOKI [jobs and careers âœ“]:When I manage to get a small victory against stressorsand â€œwake upâ€, I still have to fight against the pain that doesnâ€™t let me move without moaning. Original [joy]: He is old enough to no longer feel that I am the only acceptable answer in the dark.Original [negative]: Despite its visual virtuosity, â€œNaqoyqatsiâ€ is banal in its message and the choice of material to convey it. A2T [sadness âœ“]:He is longtime enough to no longer feel that I am the only agree able answer in the gloomy .A2T [negative âœ—]:Though its visual virtuosity, â€œNaqoyqatsiâ€ is banal in its messagingand the choices ofmaterialsto convey it. TOKI [love âœ“]:He is old enough to no longer feel that I am the only accepted answer in the dark.TOKI [positive âœ“]:Despite its visual virtuosity, â€œNaqoyqatsiâ€ is insipid in its message and the choice of material to convey it. Answer to RQ3: The adversarial attack method guided by TOKI outperforms the state-of-the- art A2T. Our method achieves a higher success rate with fewer perturbations than A2T while preserving the semantic meaning of the original texts. 7 Discussion 7.1 Implications Experimental results show that highly confident predictions are more likely to be trustworthy. However, high confidence does not guarantee trustworthiness, and low confidence predictions can still be trustworthy. Relying on prediction uncertainty is ineffective for trustworthiness assessment, overlooking both trustworthy low and untrustworthy high confidence predictions. Prior work has a complex debate over the impact of prediction uncertainty, mainly on human trust in ML models. Several studies [ 7,42] suggest that prediction uncertainty has a limited impact on human trust [ 44,48]. Other studies [ 64] show that prediction certainty can improve human trust in ML models and increase the willingness to rely on high confidence predictions. However, we argue that whether the certainty of a modelâ€™s predictions increases human trust or not, it does not translate into improving the modelâ€™s trustworthiness. Model certainty can lead to overreliance on ML decisions without reflecting the true trustworthiness of the model. The model may make decisions based on spurious correlations, being confident in them rather than valid correlations in the training data. This fits the distinction between trust and trustworthiness defined by KÃ¤stner et al. [27], where people can still trust an untrustworthy model.

[PÃ¡gina 17]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 17 In contrast, TOKI shows its superior effectiveness compared to the baselines. TOKI identifies keywords for each class and uses them to measure the semantic relatedness between words and the class. This approach better captures the characteristics of classes, outperforming its variant that does not use keywords to measure relatedness, which is biased for untrustworthy predictions. Regarding adversarial attacks, examples generated by TOKI are more effective in attacking models than those created by A2T. By leveraging correlations between words and classes, our method generates adversarial examples likely to trigger trustworthiness vulnerabilities. This enables TOKI- guided attack method to achieve a higher attack success rate with fewer perturbations than existing adversarial attack methods. This finding highlights the negative impact of trustworthiness issues and the need for trustworthiness oracles, which remain underexplored in the research community. 7.2 Threats to Validity Threats to internal validity can be related to the faithfulness of explanations, which refers to how accurately the explanations reflect a modelâ€™s reasoning [ 35]. To assess this, we evaluate TOKI using three explanation methods, including LIME, omission, and gradient. Another threat to internal validity is the bias in word embedding methods due to their training data. We mitigate this by using ensemble learning with multiple word embedding methods. To reduce threats to external validity , we evaluate TOKI on approximately 6,000 predictions across 10 models and datasets in various domains, including topic classification, sentiment analysis, clinical mental text classification, and hate speech detection. We also employ TOKI with various explanation methods to assess its performance. Threats to construct validity can arise from the trustworthiness benchmark. We derive trustwor- thiness datasets from existing studies, which do not directly label predictions as trustworthy or not, by measuring the explanationsâ€™ plausibility. To mitigate the threats, we collect an additional, more straightforward trustworthiness dataset. The bias in human annotations is addressed by combining all human-annotated trustworthiness labels for each prediction using plurality voting [29]. 8 Related Work 8.1 Machine Learning Testing Various testing techniques have emerged to assess different aspects of ML models. For example, test input generation methods [ 23,45,56] are used detect defects, guided by test adequacy criteria [ 34, 45]. Several methods aim to debug and repair models [ 18,26]. Other methods, such as A2T [ 61], focus on adversarial attacks [ 40,63] to assess model robustness. The oracle problem [ 3] is also a significant area of interest. Two main approaches are primarily used to address the oracle problem: metamorphic testing [ 47,59] and cross-referencing [ 23,45]. Considerable efforts have been made to test various properties of ML models, such as accuracy, relevance, efficiency, robustness, fairness, and interpretability [ 62]. However, testing the trustworthiness of ML models remains a significant challenge. Several metrics have been proposed to quantify the concept of trustworthiness [ 13,25]. However, different from other properties, much of the work on trustworthiness testing relies on human interpretation within their systems. 8.2 Making Use of Explanations Explanations have served as a valuable tool for testing and improving ML models [ 65]. Various studies have used explanations to understand and debug models. Ribeiro et al. [ 49] demonstrated the usefulness of local explanations in various tasks, such as model comparison and trust assessment. Lapuschkin et al. [ 28] introduced a semi-automated approach that characterises and validates classification strategies. Thomas et al. [ 55] used explanations to uncover input-prediction patterns.

[PÃ¡gina 18]
18 Lam et al. Several studies [ 11,61,63] have applied explanations in adversarial attacks, primarily to determine word substitution order based on word importance. To the best of our knowledge, this paper introduces the first approach that leverages explanations to make perturbations, specifically, by weakening valid correlations and strengthening spurious ones. Explanations have also been inte- grated into the learning process to improve model performance and reliability. Andrew et al. [ 50] proposed an approach to discover multiple models for the same task with different classification strategies, allowing domain experts to choose the best one. Chen et al. [ 12] used explanations to im- prove the adversarial robustness of language models. Other studies [ 22,52] leveraged explanations to make models right for the right scientific reasons. Similarly, Du et al. [ 17] developed a framework to mitigate shortcuts focusing on stop words, punctuation, and numbers. Recent studies [ 33] have also applied explanation regularisation to mitigate the impact of spurious correlations. 9 Conclusion This paper investigates the trustworthiness oracle problem of text classifiers. Statistical evaluation reveals that while highly confident predictions are more likely to be trustworthy, some still lack trustworthiness due to reliance on spurious correlations. We propose TOKI, an automated trust- worthiness oracle generation method. Experiments are conducted to compare the effectiveness and efficiency of TOKI, TOKIâ€™s ablation variant and the naive approach against the trustworthiness benchmark on ten models and datasets. Results show that TOKI outperforms other approaches. We also introduce a TOKI-guided adversarial attack method, which proves to be more effective than the state-of-the-art A2T. In addition, several works can be explored in the future, including adapting TOKI for other data types and conducting a larger human-based evaluation to gain more insights into the trustworthiness problem. Data Availability The trustworthiness benchmark and the replicate package are available on Zenodo [2]. References [1]Amina Adadi and Mohammed Berrada. 2018. Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). IEEE Access 6 (2018), 52138â€“52160. https://doi.org/10.1109/ACCESS.2018.2870052 [2]Anonymous Anonymous. 2024. Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers. https://doi.org/10.5281/zenodo.13751580 [3]Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2015. The Oracle Problem in Software Testing: A Survey. IEEE Transactions on Software Engineering 41, 5 (2015), 507â€“525. https://doi.org/10.1109/TSE.2014. 2372785 [4]Yoshua Bengio, RÃ©jean Ducharme, and Pascal Vincent. 2000. A Neural Probabilistic Language Model. In Advances in Neural Information Processing Systems , Vol. 13. MIT Press. [5]Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching Word Vectors with Subword Information. Transactions of the Association for Computational Linguistics 5 (06 2017), 135â€“146. https://doi.org/10. 1162/tacl_a_00051 [6]Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics 32, 1 (03 2006), 13â€“47. https://doi.org/10.1162/coli.2006.32.1.13 [7]Adrian Bussone, Simone Stumpf, and Dympna Oâ€™Sullivan. 2015. The Role of Explanations on Trust and Reliance in Clinical Decision Support Systems. In 2015 International Conference on Healthcare Informatics . 160â€“169. https: //doi.org/10.1109/ICHI.2015.26 [8]GÃ¼rol Canbek, Tugba Taskaya Temizel, and Seref Sagiroglu. 2022. PToPI: A Comprehensive Review, Analysis, and Knowledge Representation of Binary Classification Performance Measures/Metrics. SN Computer Science 4, 1 (16 Oct 2022), 13. https://doi.org/10.1007/s42979-022-01409-1 [9]Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission. In The 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD â€™15) . Association for Computing Machinery, New York, USA, 1721â€“1730. https://doi.org/10.1145/2783258.2788613

[PÃ¡gina 19]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 19 [10] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo- Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. 2018. Universal Sentence Encoder for English. In2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . Association for Computational Linguistics, Brussels, Belgium, 169â€“174. https://doi.org/10.18653/v1/D18-2029 [11] Yidong Chai, Ruicheng Liang, Sagar Samtani, Hongyi Zhu, Meng Wang, Yezheng Liu, and Yuanchun Jiang. 2023. Additive Feature Attribution Explainable Methods to Craft Adversarial Attacks for Text Classification and Text Regression. IEEE Transactions on Knowledge and Data Engineering 35, 12 (2023), 12400â€“12414. https://doi.org/10.1109/ TKDE.2023.3270581 [12] Hanjie Chen and Yangfeng Ji. 2022. Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation. The AAAI Conference on Artificial Intelligence 36, 10 (Jun. 2022), 10463â€“10472. https://doi.org/10. 1609/aaai.v36i10.21289 [13] Mingxi Cheng, Shahin Nazarian, and Paul Bogdan. 2020. There Is Hope After All: Quantifying Opinion and Trustwor- thiness in Neural Networks. Frontiers in Artificial Intelligence 3 (2020). https://doi.org/10.3389/frai.2020.00054 [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies , Vol. 1. Association for Computational Linguistics, Minnesota, 4171â€“4186. https://doi.org/10.18653/v1/N19-1423 [15] Nguyen Dong. 2018. Comparing Automatic and Human Evaluation of Local Explanations for Text Classification. In The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, Hyatt Regency, New Orleans, 1069â€“1078. https: //doi.org/10.18653/v1/N18-1097 [16] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. 2023. Shortcut Learning of Large Language Models in Natural Language Understanding. Commun. ACM 67, 1 (dec 2023), 110â€“120. https://doi.org/10.1145/3596490 [17] Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu. 2021. Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models. In The 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, Online, 915â€“929. https://doi.org/10.18653/v1/2021.naacl-main.71 [18] Saikat Dutta, Wenxian Zhang, Zixin Huang, and Sasa Misailovic. 2019. Storm: program reduction for testing and debugging probabilistic programming systems. In The 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Tallinn, Estonia) (ESEC/FSE 2019) . Association for Computing Machinery, New York, USA, 729â€“739. https://doi.org/10.1145/3338906.3338972 [19] Julia El Zini, Mohamad Mansour, Basel Mousi, and Mariette Awad. 2022. On the Evaluation of the Plausibility and Faithfulness of Sentiment Analysis Explanations. In Artificial Intelligence Applications and Innovations , Ilias Maglogiannis, Lazaros Iliadis, John Macintyre, and Paulo Cortez (Eds.). Springer International Publishing, Cham, 338â€“349. [20] Muskan Garg, Chandni Saxena, Sriparna Saha, Veena Krishnan, Ruchi Joshi, and Vijay Mago. 2022. CAMS: An Annotated Corpus for Causal Analysis of Mental Health Issues in Social Media Posts. In Proceedings of the Thirteenth Language Resources and Evaluation Conference , Nicoletta Calzolari, FrÃ©dÃ©ric BÃ©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, HÃ©lÃ¨ne Mazo, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 6387â€“6396. https://aclanthology.org/2022.lrec-1.686 [21] Robert Geirhos, JÃ¶rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence 2, 11 (01 Nov 2020), 665â€“673. https://doi.org/10.1038/s42256-020-00257-z [22] Bhavya Ghai, Q. Vera Liao, Yunfeng Zhang, Rachel Bellamy, and Klaus Mueller. 2021. Explainable Active Learning (XAL): Toward AI Explanations as Interfaces for Machine Teachers. Proc. ACM Hum.-Comput. Interact. 4, CSCW3, Article 235 (jan 2021), 28 pages. https://doi.org/10.1145/3432934 [23] Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. DLFuzz: differential fuzzing testing of deep learning systems. In The 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Lake Buena Vista, FL, USA) (ESEC/FSE 2018) . Association for Computing Machinery, New York, USA, 739â€“743. https://doi.org/10.1145/3236024.3264835 [24] Keith Harrigian, Carlos Aguirre, and Mark Dredze. 2020. Do Models of Mental Health Based on Social Media Data Generalize?. In Findings of the Association for Computational Linguistics: EMNLP 2020 , Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 3774â€“3788. https://doi.org/10.18653/v1/2020.findings- emnlp.337 [25] Davinder Kaur, Suleyman Uslu, Arjan Durresi, Sunil Badve, and Murat Dundar. 2021. Trustworthy Explainability Acceptance: A New Metric to Measure the Trustworthiness of Interpretable AI Medical Diagnostic Systems. In Complex,

[PÃ¡gina 20]
20 Lam et al. Intelligent and Software Intensive Systems . Springer International Publishing, Cham, 35â€“46. [26] Sanjay Krishnan and Eugene Wu. 2017. PALM: Machine Learning Explanations For Iterative Debugging. In The 2nd Workshop on Human-In-the-Loop Data Analytics (Chicago, IL, USA) (HILDA â€™17) . Association for Computing Machinery, New York, USA, Article 4, 6 pages. https://doi.org/10.1145/3077257.3077271 [27] Lena KÃ¤stner, Markus Langer, Veronika Lazar, Astrid SchomÃ¤cker, Timo Speith, and Sarah Sterz. 2021. On the Relation of Trust and Explainability: Why to Engineer for Trustworthiness. In IEEE 29th International Requirements Engineering Conference Workshops . 169â€“175. https://doi.org/10.1109/REW53955.2021.00031 [28] Sebastian Lapuschkin, Stephan WÃ¤ldchen, Alexander Binder, GrÃ©goire Montavon, Wojciech Samek, and Klaus-Robert MÃ¼ller. 2019. Unmasking Clever Hans predictors and assessing what machines really learn. Nature Communications 10, 1 (11 Mar 2019), 1096. https://doi.org/10.1038/s41467-019-08987-4 [29] Florin Leon, Sabina-Adriana Floria, and Costin BÄƒdicÄƒ. 2017. Evaluating the effect of voting methods on ensemble-based classification. In 2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA) . 1â€“6. https://doi.org/10.1109/INISTA.2017.8001122 [30] Piyawat Lertvittayakumjorn and Francesca Toni. 2021. Explanation-Based Human Debugging of NLP Models: A Survey. Transactions of the Association for Computational Linguistics 9 (12 2021), 1508â€“1528. https://doi.org/10.1162/ tacl_a_00440 [31] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding Neural Networks through Representation Erasure. CoRR abs/1612.08220 (2016). http://dblp.uni-trier.de/db/journals/corr/corr1612.html#LiMJ16a [32] Q. Vera Liao and Jennifer Wortman Vaughan. 2024. AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap. Harvard Data Science Review Special Issue 5 (feb 29 2024). https://hdsr.mitpress.mit.edu/pub/aelql9qy. [33] Lorenz Linhardt, Klaus-Robert MÃ¼ller, and GrÃ©goire Montavon. 2024. Preemptively pruning Clever-Hans strategies in deep neural networks. Information Fusion 103 (2024), 102094. https://doi.org/10.1016/j.inffus.2023.102094 [34] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. DeepGauge: multi-granularity testing criteria for deep learning systems. InThe 33rd ACM/IEEE International Conference on Automated Software Engineering (Montpellier, France) (ASE â€™18) . Association for Computing Machinery, New York, USA, 120â€“131. https://doi.org/10.1145/3238147.3238202 [35] Ettore Mariotti, Anna Arias-Duart, Michele Cafagna, Albert Gatt, Dario Garcia-Gasulla, and Jose Maria Alonso-Moral. 2024. TextFocus: Assessing the Faithfulness of Feature Attribution Methods Explanations in Natural Language Processing. IEEE Access (2024), 1â€“1. https://doi.org/10.1109/ACCESS.2024.3408062 [36] Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021. HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 14867â€“14875. [37] George A. Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (nov 1995), 39â€“41. https: //doi.org/10.1145/219717.219748 [38] Hosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. 2021. Exploring the Role of BERT Token Rep- resentations to Explain Sentence Probing Results. In The 2021 Conference on Empirical Methods in Natural Lan- guage Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 792â€“806. https://doi.org/10.18653/v1/2021.emnlp-main.61 [39] Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical Semantic Relations. In The Computational Lexical Semantics Workshop . Association for Computational Linguistics, Boston, USA, 46â€“51. https://aclanthology.org/W04-2607 [40] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP. In The 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . Association for Computational Linguistics, Online, 119â€“126. https://doi.org/10.18653/v1/2020.emnlp-demos.16 [41] Nikola MrkÅ¡iÄ‡, Diarmuid Ã“ SÃ©aghdha, Blaise Thomson, Milica GaÅ¡iÄ‡, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-fitting Word Vectors to Linguistic Constraints. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , Kevin Knight, Ani Nenkova, and Owen Rambow (Eds.). Association for Computational Linguistics, San Diego, California, 142â€“148. https://doi.org/10.18653/v1/N16-1018 [42] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . 427â€“436. https://doi.org/10.1109/CVPR.2015.7298640 [43] Frank Nielsen. 2016. Hierarchical Clustering . Springer International Publishing, Cham, 195â€“211. https://doi.org/10. 1007/978-3-319-21903-5_8 [44] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshmi- narayanan, and Jasper Snoek. 2019. Can you trust your model 's uncertainty? Evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems , Vol. 32. Curran Associates.

[PÃ¡gina 21]
Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers 21 [45] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Automated Whitebox Testing of Deep Learning Systems. In 26th Symposium on Operating Systems Principles (Shanghai, China) (SOSP â€™17) . Association for Computing Machinery, New York, USA, 1â€“18. https://doi.org/10.1145/3132747.3132785 [46] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. InThe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Doha, Qatar, 1532â€“1543. https://doi.org/10.3115/v1/D14-1162 [47] Manikandasriram Srinivasan Ramanagopal, Cyrus Anderson, Ram Vasudevan, and Matthew Johnson-Roberson. 2018. Failing to Learn: Autonomously Identifying Perception Failures for Self-Driving Cars. IEEE Robotics and Automation Letters 3, 4 (2018), 3860â€“3867. https://doi.org/10.1109/LRA.2018.2857402 [48] Amy Rechkemmer and Ming Yin. 2022. When Confidence Meets Accuracy: Exploring the Effects of Multiple Perfor- mance Indicators on Trust in Machine Learning Models. In Human Factors in Computing Systems (LA, USA) (CHI â€™22) . Association for Computing Machinery, NY, USA, Article 535, 14 pages. https://doi.org/10.1145/3491102.3501967 [49] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In The 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD â€™16) . Association for Computing Machinery, New York, USA, 1135â€“1144. https://doi.org/10.1145/2939672.2939778 [50] Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. 2017. Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations. In The Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17 . 2662â€“2670. https://doi.org/10.24963/ijcai.2017/371 [51] Viktor Schlegel, Erick Mendez Guzman, and Riza Batista-Navarro. 2022. Towards Human-Centred Explainability Benchmarks For Text Classification. In 16th International AAAI Conference on Web and Social Media . AAAI, USA, 1. [52] Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao, Hans- Georg Luigs, Anne-Katrin Mahlein, and Kristian Kersting. 2020. Making deep neural networks right for the right scientific reasons by interacting with their explanations. Nature Machine Intelligence 2, 8 (01 Aug 2020), 476â€“486. https://doi.org/10.1038/s42256-020-0212-3 [53] Noam Shazeer, Ryan Doherty, Colin Evans, and Chris Waterson. 2016. Swivel: Improving Embeddings by Noticing Whatâ€™s Missing. CoRR abs/1602.02215 (2016), 9 pages. arXiv:1602.02215 http://arxiv.org/abs/1602.02215 [54] Thabet Slimani. 2013. Description and Evaluation of Semantic Similarity Measures Approaches. International Journal of Computer Applications 80, 10 (October 2013), 25â€“33. https://doi.org/10.5120/13897-1851 [55] Armin W Thomas, Hauke R Heekeren, Klaus-Robert MÃ¼ller, and Wojciech Samek. 2019. Analyzing Neuroimaging Data Through Recurrent Deep Learning Models. Front Neurosci 13 (Dec. 2019), 1321. [56] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: automated testing of deep-neural-network- driven autonomous cars. In The 40th International Conference on Software Engineering (Sweden) (ICSE â€™18) . Association for Computing Machinery, USA, 303â€“314. https://doi.org/10.1145/3180155.3180220 [57] FranÃ§ois Torregrossa, Robin Allesiardo, Vincent Claveau, Nihel Kooli, and Guillaume Gravier. 2021. A survey on training and evaluation of word embeddings. International Journal of Data Science and Analytics 11, 2 (01 Mar 2021), 85â€“103. https://doi.org/10.1007/s41060-021-00242-8 [58] Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not Explanation. In The 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 11â€“20. https://doi.org/10.18653/v1/D19-1002 [59] Xiaoyuan Xie, Zhiyi Zhang, Tsong Yueh Chen, Yang Liu, Pak-Lok Poon, and Baowen Xu. 2020. METTLE: A METamor- phic Testing Approach to Assessing and Validating Unsupervised Machine Learning Systems. IEEE Transactions on Reliability 69, 4 (2020), 1293â€“1322. https://doi.org/10.1109/TR.2020.2972266 [60] Wenqian Ye, Guangtao Zheng, Xu Cao, Yunsheng Ma, and Aidong Zhang. 2024. Spurious Correlations in Machine Learning: A Survey. arXiv:2402.12715 [cs.LG] https://arxiv.org/abs/2402.12715 [61] Jin Yong Yoo and Yanjun Qi. 2021. Towards Improving Adversarial Training of NLP Models. In Findings of the Association for Computational Linguistics: EMNLP 2021 , Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic, 945â€“956. https://doi.org/10.18653/v1/2021.findings-emnlp.81 [62] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2022. Machine Learning Testing: Survey, Landscapes and Horizons. IEEE Transactions on Software Engineering 48, 1 (2022), 1â€“36. https://doi.org/10.1109/TSE.2019.2962027 [63] Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol. 11, 3, Article 24 (apr 2020), 41 pages. https://doi.org/10.1145/3374217 [64] Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In The 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* â€™20) . Association for Computing Machinery, New York, USA, 295â€“305. https://doi.org/10.

[PÃ¡gina 22]
22 Lam et al. 1145/3351095.3372852 [65] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024. Explainability for Large Language Models: A Survey. ACM Trans. Intell. Syst. Technol. 15, 2, Article 20 (feb 2024), 38 pages. https://doi.org/10.1145/3639372 [66] Huichi Zhou, Zhaoyang Wang, Hongtao Wang, Dongping Chen, Wenhan Mu, and Fangyuan Zhang. 2024. Evaluating the Validity of Word-level Adversarial Attacks with Large Language Models. In Findings of the Association for Compu- tational Linguistics ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand and virtual meeting, 4902â€“4922. https://aclanthology.org/2024.findings-acl.292