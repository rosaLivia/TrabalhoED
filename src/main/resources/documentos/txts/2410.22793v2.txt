
[Página 1]
Less is More: DocString Compression in Code Generation GUANG YANG, YU ZHOU∗, WEI CHENG, and XIANGYU ZHANG, Nanjing University of Aeronautics and Astronautics, China XIANG CHEN, Nantong University, China TERRY YUE ZHUO, Monash University, Australia KE LIU, National University of Defense Technology, China XIN ZHOU and DAVID LO, Singapore Management University, Singapore TAOLUE CHEN†,Birkbeck, University of London, UK The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings which capture user re- quirements for the code and used as the prompt for LLMs, often contains redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc , dedicated to DocString compression for code generation. Our extensive experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25∼40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this research is to improve efficiency and reduce the cost while maintaining the quality of the generated code, especially when calling third-party APIs, and is able to reduce the token processing cost by 25 ∼40%. CCS Concepts: •Software and its engineering ;•Computing methodologies →Artificial intelligence ; Additional Key Words and Phrases: DocString Compression, Code Generation, Large Language Model ACM Reference Format: Guang Yang, Yu Zhou, Wei Cheng, Xiangyu Zhang, Xiang Chen, Terry Yue Zhuo, Ke Liu, Xin Zhou, David Lo, and Taolue Chen. . Less is More: DocString Compression in Code Generation. 0, 0, Article 0 ( ), 27 pages. ∗Corresponding author. †Corresponding author. Authors’ addresses: Guang Yang, novelyg@outlook.com; Yu Zhou, zhouyu@nuaa.edu.cn; Wei Cheng, chengweii@nuaa.edu. cn; Xiangyu Zhang, zhangx1angyu@nuaa.edu.cn, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Xiang Chen, xchencs@ntu.edu.cn, Nantong University, Nantong, China; Terry Yue Zhuo, terry.zhuo@monash.edu, Monash University, Australia; Ke Liu, liuke23@nudt.edu.cn, National University of Defense Technology, ChangSha, China; Xin Zhou, xinzhou.2020@phdcs.smu.edu.sg; David Lo, davidlo@smu.edu.sg, Singapore Management University, Singapore; Taolue Chen, t.chen@bbk.ac.uk, Birkbeck, University of London, London, UK. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ©Association for Computing Machinery. XXXX-XXXX//0-ART0 $15.00 https://doi.org/ , Vol. 0, No. 0, Article 0. Publication date: .arXiv:2410.22793v2 [cs.SE] 31 Oct 2024

[Página 2]
0:2 G. Yang, et al. 1 INTRODUCTION Large Language Models (LLMs) have emerged as a key tool in today’s software development, owning to their powerful language understanding and generation capabilities [ 30,41]. In particular, they have shown great potential in a variety of SE tasks such as code generation [ 23], automated testing [ 7] and documentation [ 11]. However, with the wide deployment of LLMs, it has become a pressing challenge to improve the inference efficiency and to reduce the ever increasing demand of computational resources [33, 42, 52]. 1def add_numbers (a, b): 2 """ 3 Adds two numbers and returns the result . 4 Parameters : 5 a ( int or float ): The first number to add . 6 b ( int or float ): The second number to add . 7 Returns : 8 int or float : The sum of a and b. 9 Raises : 10 TypeError : If 'a'or'b'is not a number . 11 Example : 12 >>> add_numbers (2, 3) 13 5 14 """ 15 if not (isinstance (a, ( int,float ))and isinstance (b, ( int,float ))): 16 raise TypeError (" Both arguments must be numbers ") 17 return a + b Listing 1. The Example of DocString in Code Generation DocString standing for "documentation string" is a critical component that can significantly enhance the readability, maintainability and usability of the code. It plays a pivotal role in the code generation process, typically positioned at the onset of a function or method. DocStrings serve to articulate the code’s functionality, parameters, return values and potential exceptions that may arise [ 8]. Listing 1 presents an example to illustrate how DocString (Lines 2-14) elucidates the purpose of the function, its parameters, the type of return value it yields, the exceptions it might trigger, and offers a practical example showing how to invoke the function. These texts not only facilitate an intuitive grasp of the code for developers but also act as a direct reflection of user requirements. In the realm of code generation leveraging LLMs, DocStrings are typically used as prompts. They guide the model to generate code that aligns with the specified requirements and expectations [ 26]. However, existing DocString varies significantly in quality [ 37]. Some DocString may be overly lengthy and contain a large amount of redundancy, which may prevent the model from accurately understanding user requirements. Although these redundant docstring do not exceed the length of the maximum context that LLM can now handle, they may also increase the computational cost of the model training and inference, thus reduce their efficiency. In addition, when users generate code by invoking APIs from third-party LLMs, shorter prompt can greatly reduce their financial cost. Therefore, optimizing the quality of DocString is key for improving the efficiency of code generation and reducing costs. Naturally, one may apply existing prompt compression techniques (e.g., Selective_Context [ 21] and LLMLingua [15]) from NLP to DocString. It turns out that there are several challenges: (1) Compression efficacy : these compression methods show limited effectiveness in compress- ing DocStrings. When we attempted to increase the compression rate above 10%, there was a , Vol. 0, No. 0, Article 0. Publication date: .

[Página 3]
Less is More: DocString Compression in Code Generation 0:3 significant decrease in the quality of the generated code. We posit that this is largely because they fail to extract the code-related semantic information in DocStrings, leading to significant information loss. (2) Inflexibility : existing methods require manual setting of the compression ratio, making it difficult to adapt to various code generation scenarios. Developers must adjust compression param- eters based on specific situations, which requires specialized knowledge and is time-consuming. Moreover, it is hard to find the optimal balance of compression ratio and model efficacy (specifically Pass@1). Our work. We aim to develop prompt compression methods dedicated to DocStrings, optimizing the efficiency of LLMs in code generation tasks. We introduce ShortenDoc , a novel method that dynamically adjusts DocString compression based on the importance of individual tokens. Instead of using a fixed compression ratio, ShortenDoc analyzes the significance of each token, ensuring that essential information is preserved while removing redundant content. Initially, we decompose a given prompt into its signature and docstring components. The doc- string undergoes preprocessing to enhance its quality based on empirical findings. Each token in the docstring is assigned an importance score, and tokens are ranked accordingly. We then construct a search space of tokens eligible for compression, considering predefined constraints to ensure minimal semantic distortion. Tokens are iteratively compressed until the constraint is no longer satisfied. The final compressed docstring is obtained by integrating the selected tokens with the original sequence. This approach ensures that ShortenDoc balances efficiency with accuracy, minimizing computational overhead while maintaining the quality of the generated code across different programming languages and contexts. We evaluate the performance of ShortenDoc across six datasets and six LLMs, demonstrating its superior ability to preserve essential information and reduce inference costs compared to baseline methods. To explore the generalization of ShortenDoc , we evaluate the performance of ShortenDoc across four more programming languages. Additionally, we conduct a human study to further assess the practical impact of DocString compression, evaluating both informativeness and comprehensibility from the perspective of human evaluation. Our contributions can be summarized as follows. •We empirically show that the feasibility and limitations of DocString compression for code generation tasks. •We design a novel method ShortenDoc , which is an adaptive compression method for com- pressing DocString and has better compression results compared to existing methods. •We delve into the insights gained from DocString compression techniques and give relevant insights. Structure. The rest of the paper is organized as follows. Section 2 provides preliminary knowledge related to our study. Section 3 confirms and analyzes the feasibility and limitations of existing DocString compression methods for code generation tasks. Section 4 describes the key components ofShortenDoc . Section 5 present the research questions and the result analysis, which is followed by the discussion in Section 6. Section 7 reviewes the related work. Section 8 concludes our study and outlines future directions. 2 BACKGROUND Code generation, in a nutshell, refers to automated generation of code from specifications and under certain constraints, which plays a pivotal role in software development [ 20]. In this context, , Vol. 0, No. 0, Article 0. Publication date: .

[Página 4]
0:4 G. Yang, et al. signatures and DocStrings (in natural language) are typically used as inputs to the model and are converted into executable code. LetTdenote the set of signatures, Ddenote the set of DocStrings and Ydenote the set of executable code. In general, signatures define the name of the function/method, input parameters, return type and possible side effects. DocStrings provide a natural language description of the function or method, including its purpose, behavior, parameter explanations, and return values. Typically, code generation can be formalized as a function 𝑓:T×D→Y . In practice, most of the current code generation models follow Transformer’s decoder architec- ture [ 44]. The process can be broken down into several steps, including word embedding, layer transformations and vocabulary mapping. Embedding. The input signature 𝑇∈T and DocString 𝐷∈D are tokenized into a sequence of words𝑤1,...,𝑤𝑛. Each word is then converted into a vector representation through word embedding. LetWbe the vocabulary of tokens and E∈R|W|×𝑑embedbe the embedding matrix, where𝑑embed is the dimensionality of the embedding space. The word embedding function Emb : W→ R𝑑embedmaps each token to its corresponding vector representation, viz., Emb(𝑤𝑖)is the embedding vector for the 𝑖-th word. Transformer. The embedded sequence is then fed into a stack of 𝐿transformer layers to produce a sequence of hidden states H=[h1,h2,..., h𝐿], where h𝑖is the hidden state vector for the 𝑖-th layer output and each Transformer-Block computes the hidden state through Self-Attention and Feed-Forward, i.e., h𝑖+1=Transformer-Block (h𝑖)=Self-Attention(h𝑖)+Feed-Forward(h𝑖) Probability. The final hidden state vector h𝐿of the last layer is mapped to a vocabulary space using a linear transformation, followed by a softmax function to obtain a probability distribution over the vocabulary: z=Wh𝐿+b, where Wis the weight matrix and bis the bias vector for the vocabulary mapping. The probability of generating the next code token 𝑦is computed using the softmax function: 𝑃(𝑦)=softmax(z)=exp(z𝑦) Í|W| 𝑗=1exp(z𝑗) where z𝑦is the score for the word 𝑦in the vocabulary, and |W| is the size of the vocabulary. 𝑃(𝑌|𝑇,𝐷)=𝑚Ö 𝑖=1𝑃(𝑦𝑖|𝑇,𝐷,𝑦 1,𝑦2,···,𝑦𝑖−1) The model generates a sequence of code tokens 𝑦1,𝑦2,...,𝑦𝑚by sampling from the probability dis- tributions𝑃(𝑦1|𝑇,𝐷),𝑃(𝑦2|𝑇,𝐷,𝑦 1),...,𝑃(𝑦𝑚|𝑇,𝐷,𝑦 1,...,𝑦𝑚−1), where each token is conditioned on the previous tokens in the sequence. Prompt Compression. Prompt compression is a technique aimed at reducing the length of input prompts while preserving essential information. This method is particularly beneficial in scenarios where computational resources are constrained or where rapid response time is critical. In a nutshell, the goal of prompt compression is to construct a function 𝑔, which, given the original prompt 𝑃, outputs the compressed prompt 𝑔(𝑃)such that the it contains less tokens but retains the critical information. 3 EMPIRICAL STUDY In this section, we conduct an empirical study to explore the applicability of existing prompt compression methods to code generation tasks. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 5]
Less is More: DocString Compression in Code Generation 0:5 Fig. 1. The length distribution of DocString in these datasets. 3.1 Experiment Setup Datasets. We experiment on six diverse and widely adopted datasets which can simulate real-world code generation tasks. •HumanEval [ 6]:this dataset contains 164 well-designed hand-written programming prob- lems for Python, each of which includes an average of 7.7 test cases per problem. •CodeHarmony [ 12]:this dataset contains 16,153 high quality Python code samples syn- thesized by LLMs, which is extracted from existing open-source datasets, such as the Evol dataset and OSS dataset. Each problem consists of 3 test cases. In our study, we choose CodeHarmony’s test set containing 153 samples. •MBPP [ 2]:This benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming funda- mentals, standard library functionality, and so on. Each problem consists of 3 automated test cases. In our study, we choose MBPP’s test set containing 500 samples. •Subtle [ 50]:this dataset contains 100 programming problems for Python based on Hu- manEval, which makes a subtle and minor change to the original problem such as inverting or replacing a requirement. Each problem consists of an average of 10.3 test cases per problem. •Creative [ 50]:this dataset contains 100 programming problems for Python based on Hu- manEval, which generates a more creative problem compared to the original through the use of stories or uncommon narratives. Each problem consists of an average of 43.1 test cases per problem. •BigCodeBench [ 55]:A set of 1,140 programming problems for Python which evaluates LLMs with practical and challenging programming tasks. Each problem consists of an average of 5.6 test cases. In our study, we choose BigCodeBench’s hard set containing 148 samples. Fig. 1 shows the length distribution of DocString in these datasets. The different lengths distri- bution and diversity in these datasets ensures that our findings are applicable to a broad range of code generation scenarios, from simple script generation to complex software development tasks. Study Models. We select a diverse set of LLMs that are representative of the current state-of- the-art in code generation, which include DeepSeekCoder-1.3b, DeepSeekCoder-6.7b [ 14], Code- Qwen1.5 [3], CodeGeeX4 [54] and Llama3.1 [10]. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 6]
0:6 G. Yang, et al. •DeepSeekCoder: The 1.3b and 6.7b versions of DeepSeekCoder are trained on an extensive dataset, consisting of 87% code and 13% natural language in both English and Chinese. These models undergo pre-training on a project-level code corpus with a window size of 16K tokens, enhanced by a fill-in-the-blank task designed to bolster their project-level code completion and infilling capabilities. •CodeQwen1.5: This model boasts 7.3 billion parameters and supports an impressive 92 programming languages, managing contexts up to 64K tokens with ease. It has demonstrated remarkable proficiency in code generation, long sequence modeling, code modification, and SQL capabilities. •CodeGeeX4: With 9.4 billion parameters at its disposal, CodeGeeX4 is tailored for a myriad of AI software development scenarios. Its strengths lie in code completion, interpretation, web search, function calling, and repository-level question-and-answer interactions. •Llama3.1: The 8.0 billion parameter version of Llama3.1 is an open-source powerhouse, trained on a grand scale with the ability to handle context lengths up to 128K tokens. Among these, DeepSeekCoder-1.3b, DeepSeekCoder-6.7b, CodeQwen1.5 and CodeGeeX4 are spe- cialized for coding tasks, while Llama3.1, being a general-purpose model, has proven its versatility and potential for applications in diverse coding scenarios. Existing Prompt Compression Methods. We explore two state-of-the-art prompt compression methods: Selective_Context [ 21] and LLMLingua2 [ 36].Selective_Context employs a base lan- guage model (such as GPT-2) to compute the self-information of lexical units such as sentences, phrases or tokens, which is then used to evaluate the informativeness. LLMLingua2 is a task- agnostic prompt compression technique, i.e., it does not depend on the information entropy of the prompt. It utilizes data distillation to create an effective prompt compression dataset and trains a classification model as a way to select tokens that can be compressed. Evaluation Metrics. For code generation Pass@1 and Compression Ratio are considered. Pass@1 indicates the percentage of tasks for which the model produces a code snippet that successfully passes all the associated test cases on the first try. This metric is a direct reflection of the model’s accuracy and reliability in code generation scenarios. Compression Ratio quantifies the efficiency of the compression technique. It is defined as the percentage reduction in the length of the prompt after compression relative to the original length, i.e., 1−∥𝐷′∥ ∥𝐷∥. A higher compression ratio indicates that more of the original prompt has been removed, which can lead to improvements in inference speed and reduced computational costs. Implementation. All LLMs and their corresponding tokenizers are loaded from the official Hug- gingface repository, ensuring that the up-to-date and optimized versions are used in our experiments. To ensure a fair comparison, we maintain the hyperparameters of all models consistent throughout our study. For the backend service, we utilize the VLLM [ 17], which provides a unified interface for model inference. In addition, we employ Greedy Search as the inference strategy for all models. This method is chosen for its simplicity and effectiveness in generating sequential text, which is a common requirement in code generation tasks. Finally, the maximum output length for all models is set to 512 tokens. 3.2 Empirical Findings In Table 1, we show the efficacy comparison of different LLMs after applying Selective_Context and LLMLingua2 compression techniques. In particular, for each model on different datasets, it shows the efficacy with the original prompts (0% compression) as well as the change in efficacy with increasing compression (10%, 20%, 30%, 40%). , Vol. 0, No. 0, Article 0. Publication date: .

[Página 7]
Less is More: DocString Compression in Code Generation 0:7 Table 1. Efficacy Comparison of Existing Compression Methods during Different Ratios Model DatasetSelectiveContext (%) LLMLingua2 (%) 0 10 20 30 40 10 20 30 40 100 DS-1.3BHumanEval 63.42 54.88 50.00 44.51 51.22 53.05 51.22 42.68 37.20 21.95 CodeHarmony 59.48 58.82 56.21 54.90 52.29 58.82 58.82 54.25 50.33 39.22 MBPP 36.40 36.60 35.40 35.60 34.20 35.60 36.80 34.20 31.80 25.00 Subtle 53.00 53.00 44.00 45.00 40.00 50.00 46.00 44.00 32.00 15.00 Creative 23.00 21.00 20.00 17.00 14.00 20.00 22.00 17.00 14.00 2.00 BigCodeBench 6.10 4.10 2.00 0.00 2.00 4.10 4.10 2.00 2.00 0.70 Avg. 40.23 38.07 34.60 32.84 32.29 36.93 36.49 32.36 27.89 17.20 DS-6.7BHumanEval 71.95 70.12 64.63 61.59 57.32 73.17 62.20 54.27 46.34 25.61 CodeHarmony 64.36 66.67 67.97 67.32 59.48 67.32 66.67 67.97 63.40 45.75 MBPP 46.20 47.60 46.60 46.20 45.00 45.80 44.80 46.20 42.00 30.60 Subtle 61.00 58.00 55.00 55.00 47.00 58.00 60.00 45.00 39.00 13.00 Creative 34.00 34.00 31.00 31.00 23.00 33.00 37.00 31.00 27.00 4.00 BigCodeBench 12.20 8.80 4.70 6.10 6.10 12.20 9.50 6.10 5.40 0.00 Avg. 48.29 47.53 44.98 44.54 39.65 48.25 46.70 41.76 37.19 19.83 CQ-7.3BHumanEval 77.44 76.22 73.78 70.12 67.07 82.32 73.78 65.24 57.32 31.10 CodeHarmony 60.78 60.78 58.82 59.48 58.82 64.05 63.40 62.75 58.82 49.67 MBPP 59.00 58.20 57.80 57.40 55.60 58.20 56.20 53.00 51.60 35.60 Subtle 63.00 54.00 54.00 52.00 51.00 65.00 57.00 55.00 48.00 14.00 Creative 38.00 32.00 36.00 33.00 23.00 34.00 33.00 32.00 25.00 6.00 BigCodeBench 13.50 8.80 6.10 6.10 5.40 9.50 8.80 6.10 7.40 0.00 Avg. 51.95 48.33 47.75 46.35 43.48 52.18 48.70 45.68 41.36 22.73 CG-9.4BHumanEval 60.37 62.20 54.88 56.10 49.39 62.80 61.59 52.44 37.20 20.73 CodeHarmony 64.71 59.48 60.78 60.78 58.17 61.44 63.40 57.52 57.52 39.22 MBPP 46.60 45.80 43.40 44.00 42.40 45.20 45.60 42.80 40.80 29.60 Subtle 64.00 57.00 53.00 52.00 46.00 66.00 59.00 51.00 40.00 15.00 Creative 36.00 33.00 31.00 32.00 28.00 36.00 39.00 32.00 21.00 3.00 BigCodeBench 14.20 4.10 4.70 8.10 6.10 9.50 7.40 8.80 4.70 0.00 Avg. 47.65 43.60 41.29 42.16 38.34 46.82 46.00 40.76 33.54 17.93 LA-8.0BHumanEval 57.32 58.54 54.88 53.05 47.56 56.71 54.27 45.12 35.37 20.73 CodeHarmony 59.48 58.17 56.86 58.17 58.17 62.09 62.75 62.75 54.90 43.79 MBPP 41.40 42.20 39.80 41.60 37.00 41.00 41.20 38.40 35.20 24.20 Subtle 56.00 57.00 59.00 50.00 47.00 58.00 53.00 47.00 35.00 16.00 Creative 34.00 29.00 30.00 24.00 21.00 38.00 34.00 28.00 25.00 2.00 BigCodeBench 10.80 1.40 3.40 2.70 4.70 8.10 7.40 5.40 4.10 0.00 Avg. 43.17 41.05 40.66 38.25 35.91 43.98 42.10 37.78 31.60 17.79 (1)Effectiveness . As can be seen from the table, at lower compression rates, such as 10%, the efficacy degradation of most models is not significant. Taking the efficacy of CodeGeeX4 model on HumanEval dataset as an example, the original efficacy is 60.37%, the efficacy even can up to 62.20% at 10% compression rate using Selective_Context. Moreover, the efficacy can up to 62.80% at 10% compression rate using LLMLingua2. This shows that after compressing away some of the redundant information, the model still understands the task requirements better and generates , Vol. 0, No. 0, Article 0. Publication date: .

[Página 8]
0:8 G. Yang, et al. Table 2. The top-10 most frequent tokens in each dataset DataSet Rank-1 Rank-2 Rank-3 Rank-4 Rank-5 Rank-6 Rank-7 Rank-8 Rank-9 Rank-10 Selective_Context HE of 2, Output: ⇒ to than [1, 1, [5, 3, CH of to with on Additionally, returns otherwise. by be string. MBPP of to not. list. string. array. tuples. in from number. Subtle of 2, order. descending than to 3, [5, order by Creative 3: to Output: on than 2: 2, ascending 4, (1, BCB be on of Notes: not ValueError: which [1, matplotlib. 2, LLMLingua2 HE the that a and are The an You of is CH the a The are that and an This where which MBPP the a that to an are and is of which Subtle the a that and are an The of is any Creative The that the an a are and there which where BCB The a the that an then A and are there correct code. This verifies that there is indeed redundant information in DocString that can be compressed without affecting the correctness of the generated code too much. (2)Limitations . As the compression rate increases further, the efficacy degradation trend becomes obvious. For example, the efficacy of the CodeGeeX4 model on the HumanEval dataset decreases to 49.39% at a compression rate of 40% using Selective_Context, and to 37.20% at a compression rate of 40% using LLMLingua2. This suggests that existing compression methods, while removing more information, may also remove semantic information that is critical for the model to generate correct code. (3)Code generation after removing DocString . Interestingly, even in extreme cases, such as in the MBPP dataset, at 100% compression (i.e., DocString is completely removed), the models are still able to generate a certain percentage of correct code. This may indicate that in addition to DocString, other elements in the function signature, such as method name, carry important semantic information that is sufficient to guide the model in generating code that functional correct [ 9,51]. This phenomenon is explored further in Section 6, as it relates to the deeper mechanisms of how the model utilizes different types of cue information to generate code. Compressed Token Analysis. In addition to analyzing the efficacy impact of prompt compression methods, we examine the specific tokens that are being removed. By reviewing the most frequent tokens that are compressed away, we can gain insights into the types of information that are considered redundant or less critical by the compression algorithms. To this end, we extract the top-10 tokens with the highest frequency of occurrence for the two methods on the 10% compression ratio on the six datasets, respectively, and the results are shown in Table 2. We can find that the comparison between Selective_Context and LLMLingua2 reveals differences in the tokens each method targets for compression. This variation could be due to the distinct algorithms and criteria each method uses to evaluate the importance of tokens. Specifically, Selective_Context, which is based on information entropy, focuses more on the semantic coherence of the text. It tends to prioritize the retention of tokens that contribute to the logical flow and understanding of the code, thus it might be less likely to compress away tokens that are crucial for the structure or functionality of the code, even if they are not the most frequent. On the other hand, LLMLingua2, which utilizes a pre-trained classifier, is more attuned to the significance of information. It is particularly sensitive to articles and prepositions such as "the" and "a," which are common in English but may not carry substantial meaning in the context of code. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 9]
Less is More: DocString Compression in Code Generation 0:9 Fig. 2. Framework of ShortenDoc. This method might prioritize compressing these tokens as they are less likely to affect the overall functionality or readability of the code. The divergence in the tokens targeted by each method underscores the importance of the underlying algorithm’s design in determining what constitutes "redundant" information. It also highlights the need for a nuanced approach to prompt compression, where the method must balance the reduction of redundancy with the preservation of critical information necessary for code comprehension and execution. 4 APPROACH Building upon insights from the empirical research, we propose a novel compression technique ShortenDoc . Our approach consists of three main stages: Token Importance Sort, Search Space Formation and Constraint Specification. Given a prompt consisting of a signature 𝑇and a docstring 𝐷, our goal is to compress the docstring into 𝐷′while preserving the overall semantics and effectiveness in guiding LLMs for code generation. We formalize this as an optimization problem aiming to find the optimal compressed docstring 𝐷′ that satisfies semantic similarity constraints C(which we discuss in Section 4.4). The optimization task is defined as: minimize 𝐷′∥𝐷′∥ subject toC,and𝐷′⊑𝐷, where∥𝐷′∥denotes the length (number of tokens) of the compressed docstring 𝐷′, and𝐷′⊑𝐷 asserts that 𝐷′is asubsequence of the original docstring 𝐷. 4.1 Pre-processing Inspired by the previous study [ 45], we posit that the removal of line breaks and tabs from DocString does not significantly affect the efficacy of the code generation model. (An analysis will be discussed in Section 6.) Moreover, upon examining the tokens that frequently undergo compression, as indicated in Table 2, we observe that the tokens compressed by LLMLingua2 exhibit regular patterns (such as ’the’, ’a’), and the model’s efficacy remains largely intact when compression is applied at a rate of , Vol. 0, No. 0, Article 0. Publication date: .

[Página 10]
0:10 G. Yang, et al. 10%. This insight suggests that the compression of these tokens does not adversely affect model’s efficacy, leading us to categorize them as stop-words. To minimize semantic distortion in the compressed DocString, we calculate the semantic similar- ity between the original and the compressed versions for each stop word we attempt to compress. This ensures that the meaning of the DocString remains closely aligned with the original, even after compression. Specifically, we choose the CodeGPT-py-adapted [ 24] and adopt the methodology proposed by OpenAI [ 31] for extracting semantic vectors corresponding to the text, followed by the computation of semantic similarity using cosine similarity. Compression is performed only for those tokens where the similarity metric exceeds a threshold of 0.999, ensuring the preservation of DocString’s semantic integrity. 4.2 Token Importance Sort Following the preprocessing phase, we obtain the token sequence 𝐷=[𝑑1,...,𝑑𝐿]. To determine the importance of each token in this sequence, we draw upon principles from information theory. Specifically, the significance of each token 𝑑𝑡is quantified by its contribution to the conditional entropy of the sequence, which is quantified by its negative log-probability given its preceding context: Information(𝑑𝑡)=−log𝑃(𝑑𝑡|𝑑1,𝑑2,···,𝑑𝑡−1) This measure reflects how informative the token 𝑑𝑡is, given the tokens that come before it. A higher value indicates that the token carries more information and is thus more important to the sequence’s overall meaning. To calculate the distribution 𝑃, we also adopt CodeGPT-py-adapted [ 24] as the foundational language model for assessing token significance and formulating subsequent constraints. This choice is justified by the model’s relatively modest parameter count and minimal GPU memory requirements, which facilitate efficient computation. Furthermore, as demonstrated in LLMLin- gua [ 15], smaller language models such as GPT2-small can maintain comprehension of compressed prompts. The perplexity of the entire DocString 𝐷serves as a measure of its overall information content, calculated as the average conditional entropy across all tokens: Perplexity(𝐷)=−1 𝐿𝐿∑︁ 𝑖=1log𝑃(𝑑𝑖|𝑑1,𝑑2,···,𝑑𝑖−1) This value serves as a measure of the sequence’s overall unpredictability or information content. To elucidate the individual informational contribution of each token, we compute the perplexity of𝐷−𝑖, obtained by excluding a specific token 𝑑𝑖from𝐷, viz.,𝐷−𝑖=(𝑑1,𝑑2,...,𝑑𝑖−1,𝑑𝑖+1,...,𝑑𝐿) The importance of each token 𝑑𝑖is then calculated as the difference between the perplexity of 𝐷 and that of𝐷−𝑖, formulated as Importance(𝑑𝑖)=Perplexity(𝐷)−Perplexity(𝐷−𝑖) We then sort the tokens in ascending order based on their importance scores, i.e., 𝐷′=[𝑑𝑖1,···,𝑑𝑖𝐿] where𝑖1,𝑖2,...,𝑖𝐿represent the index of each token in 𝐷. This approach ensures that tokens whose removal leads to a significant increase in the sequence’s conditional entropy are identified as highly important. These tokens are critical for maintaining the semantic integrity of the docstring and are therefore prioritized for retention during the compression process. Conversely, tokens whose removal results in minimal changes to the conditional entropy are considered less important. These tokens are prime candidates for compression, as their absence is unlikely to adversely affect the model’s understanding of the docstring’s content. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 11]
Less is More: DocString Compression in Code Generation 0:11 4.3 Search Space Construction After ranking the tokens by their importance scores, we proceed to construct the search space S which comprises candidate tokens (sequences) that may be removed. To efficiently explore the search space and avoid suboptimal solutions due to local optima, we employ a Top-N strategy, i.e., to select the top N least important tokens [𝑑𝑖1,𝑑𝑖2,...,𝑑𝑖𝑁]. Our empirical observations in Table 1 indicate that, in certain scenarios, a higher compression rate may yield enhanced efficacy. Taking the DS-6.7B model as an example, on the CodeHarmony dataset, the efficacy of LLMLingua2 under 20% compression outperforms that under 10% compression. We hypothesize that the simultaneous compression of consecutive tokens could provide unexpected benefits over compressing individual tokens. As a result, for each 1≤𝑘≤𝑁we consider 𝑘-gram, i.e., G𝑘={[𝑑𝑖𝑗,𝑑𝑖𝑗+1,...,𝑑𝑖𝑗+𝑘−1]|1≤𝑗≤𝑁−𝑘+1} EachG𝑘represents the set of all possible consecutive k-grams within the Top-N tokens. The complete search space Sis the union of allG𝑘: S=𝑁Ø 𝑘=1G𝑘 This approach considers both individual low-importance tokens and sequences of such tokens, thereby exploring a richer set of compression candidates. In terms of computational complexity, the number of combinations for each 𝑘-gram is𝑁−𝑘+1, so the size ofSis bounded by𝑁(𝑁+1) 2. For example, let us assume that 𝑁=5andI=[2,15,3,10,13], then the search space can be defined as •1-grams:G1={[2],[15],[3],[10],[13]} •2-grams:G2={[2,15],[15,3],[3,10],[10,13]} •3-grams:G3={[2,15,3],[15,3,10],[3,10,13]} •4-grams:G4={[2,15,3,10],[15,3,10,13]} •5-grams:G5={[2,15,3,10,13]} By including N-gram combinations, we capture the potential benefits of compressing sequences of tokens that may collectively have a low impact on the semantic content but offer greater compression efficiency when removed together. This comprehensive search space allows us to explore various compression strategies and select the one that best satisfies our optimization objectives and constraints. 4.4 Constraint Definition In the context of code generation, the probability of generating a sequence of code 𝑌by given a signature𝑇and a docstring 𝐷is defined as: 𝑃(𝑌|𝑇,𝐷)=𝑚Ö 𝑖=1𝑃(𝑦𝑖|𝑇,𝐷,𝑦 1,𝑦2,...,𝑦𝑖−1) Here,𝑇is considered constant for different compressions, implying that the variability in 𝑃(𝑌|𝑇,𝐷) is attributed to changes in 𝐷. We denote the compressed version of 𝐷as𝐷′, and the corresponding probability as: 𝑃(𝑌|𝑇,𝐷′)=𝑚Ö 𝑖=1𝑃(𝑦𝑖|𝑇,𝐷′,𝑦1,𝑦2,...,𝑦𝑖−1) , Vol. 0, No. 0, Article 0. Publication date: .

[Página 12]
0:12 G. Yang, et al. Our goal is to minimize the difference between 𝑃(𝑌|𝑇,𝐷)and𝑃(𝑌|𝑇,𝐷′), ensuring that the com- pression does not significantly alter the model’s output. 𝑃(𝑌|𝑇,𝐷)≈𝑃(𝑌|𝑇,𝐷′) However, directly computing the divergence between these two distributions over all possible code sequencesYis computationally infeasible. To address this, we approximate the requirement by focusing on the model’s output distribution for the next token. Given the sequential nature of code generation, where each token depends on the previous tokens, we simplify the comparison to the probability of generating the first token 𝑦1: 𝑃(𝑦1|𝑇,𝐷)≈𝑃(𝑦1|𝑇,𝐷′) The probability of the first token is given by the softmax function of the model’s output: 𝑃(𝑦1|𝑇,𝐷):=softmax(𝑧(𝑇,𝐷)) Thus, the constraint can be expressed as the proximity between the softmax inputs for the original and compressed sequences: softmax(𝑧(𝑇,𝐷))≈ softmax(𝑧(𝑇,𝐷′)) To ensure that the model’s behavior remains consistent after compression, we impose a constraint on the similarity between the logits 𝑧(𝑇,𝐷)and𝑧(𝑇,𝐷′). We use the cosine similarity metric to quantify the closeness between these two vectors: CosineSimilarity(𝑧(𝑇,𝐷),𝑧(𝑇,𝐷′))=𝑧(𝑇,𝐷)·𝑧(𝑇,𝐷′) ∥𝑧(𝑇,𝐷)∥·∥𝑧(𝑇,𝐷′)∥ Our constraint is then formalized as: CosineSimilarity(𝑧(𝑇,𝐷),𝑧(𝑇,𝐷′))≥𝜏 Here,𝜏is the hyper-parameter and we also set 0.999 in our study. Integrating this constraint into our overall optimization framework, we aim to find the com- pressed docstring 𝐷′that minimizes its length while satisfying the semantic similarity constraint: minimize 𝐷′∥𝐷′∥ subject to 𝐷′⊑𝐷, CosineSimilarity(𝑧(𝑇,𝐷),𝑧(𝑇,𝐷′))≥𝜏 5 EVALUATION We evaluate the effectiveness of our proposed approach from three perspectives. 5.1 RQ1: What is the effectiveness of our proposed ShortenDoc compared to existing prompt compression methods? (Efficacy Comparison) We aim to evaluate the effectiveness of ShortenDoc in reducing DocString length without compro- mising the efficacy of code generation models. To ensure the robustness of our findings, we also include a closed-source state-of-the-art model GPT-4o in our analysis. Additionally, we introduce the Random method as a baseline. The focus of our analysis is on the efficacy metrics and the compression ratio across a spectrum of datasets and models, as presented in Table 3. Our results demonstrate that ShortenDoc consistently outperforms alternative methods across all datasets and models, particularly at higher compression rates. For instance, on the HumanEval dataset, ShortenDoc achieves the highest efficacy across all models, effectively maintaining or even improving code generation quality despite a 30% reduction in DocString length. This indicates that , Vol. 0, No. 0, Article 0. Publication date: .

[Página 13]
Less is More: DocString Compression in Code Generation 0:13 Table 3. Efficacy Comparison of Different Methods Across Various Datasets and Models DataSet Method Ratio DS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B GPT-4o HumanEvalNone 0 63.42 71.95 77.44 60.37 57.32 85.37 Random 30 32.32 38.41 60.37 36.59 35.37 50.61 SelectiveContext 30 44.51 61.59 70.12 56.10 53.05 75.00 LLMLingua2 30 42.68 54.27 65.24 52.44 45.12 67.68 ShortenDoc 30 57.93 72.56 78.66 64.34 56.10 83.54 CodeHarmonyNone 0 59.48 64.36 60.78 64.71 59.48 62.09 Random 25 49.63 60.78 51.63 52.29 56.21 54.25 SelectiveContext 25 54.90 67.32 56.86 59.48 59.48 61.44 LLMLingua2 25 54.25 67.97 61.40 59.48 62.09 61.44 ShortenDoc 25 55.56 66.01 63.40 60.78 61.44 62.09 MBPPNone 0 36.40 46.20 59.00 46.60 41.40 55.00 Random 38 30.20 39.40 48.20 38.60 32.00 46.20 SelectiveContext 38 34.60 44.80 55.40 42.00 37.60 50.00 LLMLingua2 38 31.00 42.00 52.40 42.20 37.20 50.00 ShortenDoc 38 36.60 43.80 55.40 46.20 41.40 51.40 SubtleNone 0 53.00 61.00 63.00 64.00 56.00 81.00 Random 30 28.00 35.00 40.00 35.00 34.00 42.00 SelectiveContext 30 45.00 55.00 52.00 52.00 50.00 64.00 LLMLingua2 30 44.00 45.00 55.00 51.00 47.00 60.00 ShortenDoc 30 54.00 56.00 62.00 66.00 56.00 79.00 CreativeNone 0 23.00 34.00 38.00 36.00 34.00 54.00 Random 25 9.00 16.00 18.00 21.00 21.00 29.00 SelectiveContext 25 19.00 28.00 29.00 32.00 28.00 49.00 LLMLingua2 25 23.00 33.00 29.00 31.00 35.00 51.00 ShortenDoc 25 23.00 36.00 37.00 35.00 35.00 58.00 BigCodeBenchNone 0 6.10 12.20 13.50 14.20 10.80 27.70 Random 34 0.00 5.40 4.70 5.40 1.40 12.20 SelectiveContext 34 2.00 6.10 6.10 8.10 2.70 15.50 LLMLingua2 34 2.00 5.40 6.10 8.80 4.10 15.50 ShortenDoc 34 2.00 6.10 10.10 9.50 4.70 20.30 Table 4. FLOPs Comparison of Across Various Datasets and Models, the brackets indicate the FLOPs needed for the ShortenDoc calculation. DataSetDS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B Compress Raw Compress Raw Compress Raw Compress Raw Compress Raw HumanEval 0.28(+0.02) 0.34 1.43(+0.02) 1.74 1.43(+0.02) 1.79 1.70(+0.02) 2.02 1.46(+0.02) 1.73 CodeHarmony 0.12(+0.01) 0.15 0.62(+0.01) 0.75 0.63(+0.01) 0.78 0.75(+0.01) 0.88 0.65(+0.01) 0.75 MBPP 0.03(+0.01) 0.04 0.13(+0.01) 0.22 0.14(+0.01) 0.22 0.18(+0.01) 0.28 0.15(+0.01) 0.24 Subtle 0.25(+0.02) 0.30 1.31(+0.02) 1.55 1.29(+0.02) 1.59 1.53(+0.02) 1.77 1.31(+0.02) 1.52 Creative 0.65(+0.02) 0.75 3.37(+0.02) 3.89 3.35(+0.02) 3.90 3.97(+0.02) 4.41 3.39(+0.02) 3.75 BigCodeBench 0.70(+0.05) 0.85 3.59(+0.05) 4.39 3.53(+0.05) 4.44 4.00(+0.05) 4.67 3.41(+0.05) 3.98 ShortenDoc can sustain, and in some cases enhance, model efficacy while significantly compressing input prompts. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 14]
0:14 G. Yang, et al. Moreover, in several cases, ShortenDoc surpasses the uncompressed baseline. For instance, on the HumanEval dataset, DeepSeekCoder-6.7B, CodeQwen, and CodeGeeX4 can even achieve a higher Pass@1 value when compressed by ShortenDoc . This surprising result suggests that ShortenDoc not only preserves essential information but may also eliminate redundant or distracting content from the DocStrings, thereby improving model’s focus on relevant information during code generation. Consistently, across other datasets such as CodeHarmony, MBPP, Subtle, Creative and Big- CodeBench, ShortenDoc either matches or exceeds the competing methods, especially under aggressive compression scenarios. The uniform superiority of ShortenDoc across diverse datasets and models underscores its versatility and robustness. These results suggest that ShortenDoc effectively balances compression and information retention, making it a potent tool for optimizing prompt inputs in code generation tasks. Furthermore, the fact that ShortenDoc maintains high efficacy across models of varying sizes and architectures indicates its generalizability. It effectively aids both open-source and closed-source models, including state-of-the-art systems like GPT-4o. This broad applicability enhances the practical value of ShortenDoc in real-world code generation applications. Efficiency: FLOPs (Floating Point Operations) refers to the number of floating-point operations, an indicator used to measure the complexity of a model, reflecting the total number of floating-point operations required during its execution. FLOPs is closely related to the input length and structure of the model. The purpose of docstring compression is to reduce computational costs and enhance model efficiency. Therefore, we utilize the FLOPs metric to evaluate the computational load of different models before and after compression across various datasets. The results in Table 4 indicate that the computational load of the models generally decreases after compression. It can be observed that the FLOPs values for the compressed models (Com- press) are lower than those of the original models (Raw) across all datasets. For instance, on the HumanEval dataset, the FLOPs decreased from 0.34 to 0.28 after compression, which is a reduction of approximately 17.6%. On the CodeHarmony dataset, the FLOPs decreased from 0.15 to 0.12 after compression, which is a reduction of about 20%. This reduction in computational load due to compression means that under the same hardware conditions, the model can process data more quickly, or process more data in the same amount of time. Summary of RQ1 ShortenDoc consistently outperforms existing compression methods across multiple datasets and models, effectively reducing DocString length without compromising, and sometimes even enhancing code generation efficacy. 5.2 RQ2: What is the effectiveness of our proposed ShortenDoc in other program languages? (Generalization Capability) Considering that the datasets chosen in RQ1 are all in Python, to explore the generalizability of ShortenDoc , we select four other programming languages, i.e., C++, Go, Java, JavaScript in the HumanEval-X [ 54] dataset for our experiments. Since these four datasets share DocString with HumanEval, we directly migrate the compressed DocString obtained for HumanEval in RQ1 to these datasets. The detailed experimental results are shown in Table 5. The results reveal that ShortenDoc maintains its superior performance across all four program- ming languages when compared to baseline methods (e.g., Random compression, SelectiveContext, , Vol. 0, No. 0, Article 0. Publication date: .

[Página 15]
Less is More: DocString Compression in Code Generation 0:15 Table 5. Generalization Capability of ShortenDoc in Different Programming Languages DataSet Method Ratio DS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B Avg. CPP- 0 47.56 63.41 65.85 59.76 43.90 56.10 Random 30 26.22 37.20 41.46 34.15 29.27 33.66 SelectiveContext 30 39.63 52.44 57.32 43.29 37.80 46.10 LLMLingua2 30 33.54 50.00 55.49 43.90 31.71 42.93 ShortenDoc 30 42.07 58.54 60.37 58.54 39.02 51.71 Go- 0 46.95 61.59 58.54 52.44 37.80 51.46 Random 30 21.95 32.32 35.37 27.44 22.56 27.93 SelectiveContext 30 44.51 53.05 59.76 40.24 27.44 45.00 LLMLingua2 30 34.76 42.07 55.49 36.59 26.22 39.02 ShortenDoc 30 39.63 56.10 64.63 46.95 31.10 47.68 Java- 0 57.93 59.15 77.44 60.37 57.93 62.56 Random 30 37.20 44.51 54.27 37.80 31.10 40.98 SelectiveContext 30 55.49 58.54 71.95 57.32 45.73 57.80 LLMLingua2 30 46.95 51.83 67.68 54.27 34.15 50.98 ShortenDoc 30 53.66 67.68 75.00 60.37 48.17 60.98 JavaScript- 0 57.93 64.63 71.34 59.15 40.85 58.78 Random 30 27.44 43.90 49.39 35.37 26.22 36.46 SelectiveContext 30 46.34 55.49 57.32 43.29 46.34 49.76 LLMLingua2 30 45.12 48.17 66.46 47.56 31.71 47.80 ShortenDoc 30 56.10 67.07 65.24 49.39 41.46 55.85 and LLMLingua2). Despite the structural and syntactical differences among these languages, Short- enDoc consistently achieves higher performance metrics, demonstrating its generalization ability to retain the critical information in DocStrings. Notably, in the C++ and Java datasets, ShortenDoc significantly outperformed other methods by maintaining closer alignment with the uncompressed baseline, indicating that its approach to token importance and compression strategy is not tightly coupled to any specific programming language. The JavaScript dataset, which tends to have more varied DocString usage, still saw considerable gains with ShortenDoc , reinforcing its versatility. The Go dataset showed a similar trend, where ShortenDoc consistently outperformed the baselines and adapted well to the more concise and functionally focused nature of Go’s syntax. At the same time, we observe a slight degradation in model performance when directly migrating the compressed DocStrings obtained in RQ1 (Python) to other programming languages such as C++, Go, Java, and JavaScript. While ShortenDoc consistently outperformed baseline methods, its performance did not always fully match the results achieved with the original uncompressed DocStrings in these new languages. This performance degradation can be attributed to the fact that the base language model used in our approach, CodeGPT, is specifically trained and optimized for Python. The model’s understanding of the syntax, structure, and semantics of Python-based code influences how DocStrings are compressed. When these compressed DocStrings are applied to other programming languages, certain language-specific nuances may not be as effectively preserved, leading to a small drop in code generation performance. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 16]
0:16 G. Yang, et al. Despite this limitation, ShortenDoc still demonstrates strong generalization capability, retaining the critical information needed for code generation tasks across multiple programming languages. This suggests that while language-specific models might yield the best results when paired with ShortenDoc , the method remains highly effective even when applied across different languages with minimal adjustments. Summary of RQ2 While ShortenDoc generalizes well across programming languages, a slight performance degradation is observed when migrating compressed DocStrings from Python to other languages. This can be attributed to the base model, CodeGPT, being trained specifically for Python, which may not fully capture the nuances of other languages. Nonetheless, the generalization capability of ShortenDoc was thoroughly demonstrated by its performance across C++, Go, Java, and JavaScript in the HumanEval-X dataset. By directly migrating compressed DocStrings from Python datasets, ShortenDoc consistently outperformed baseline methods, proving its adaptability and robustness across multiple programming languages. Fig. 3. Case Study of Different DocString Compression Methods 5.3 RQ3: What is the impact of compressed prompts on developers? (Human Study) RQ1 and RQ2 rely on automated evaluation metrics to compare the performance of different methods. In RQ3, we aim to gain more insights by conducting a case study to assess the qualitative impact of DocString compression using human evaluation. To begin with, we present a case study to visually contrast the compression effects of our tool with other methods. Fig. 3 illustrates this by showcasing a DocString from the Llama3.1 model within the MBPP dataset. We display the original DocString alongside its compressed versions, processed by four distinct methods. The original DocString led to semantically incorrect code generation by the Llama3.1 model. The Random compression method resulted in irrelevant code. Both Selective_Context and LLMLingua2 maintained the model’s output, yet failed to rectify the , Vol. 0, No. 0, Article 0. Publication date: .

[Página 17]
Less is More: DocString Compression in Code Generation 0:17 semantic inaccuracy. In stark contrast, our tool’s compression successfully steered the model towards semantically correct code generation. To further evaluate the quality of the compressed DocStrings, we conducted a human study using three groups of DocStrings compressed by different methods (Selective_Context, LLMLingua2, and ShortenDoc ). By involving human evaluators, we sought to provide a more comprehensive assessment of the compressed DocStrings and to better understand their practical implications. In our human study, we used two key evaluation criteria: Informativeness and Comprehensibility. •Informativeness . This criterion measures whether the compressed DocString retains the key information conveyed in the original DocString. A higher score indicates that the essential content is preserved after compression. •Comprehensibility . This criterion evaluates how easily a human can understand the com- pressed DocString. It measures whether the DocString, despite being shortened, remains understandable. Fig. 4. A Sample Questionnaire Used in Human Study To conduct the human study, we recruited three evaluators with a strong background in Python programming (all Ph.D. candidates). We selected all samples from the six datasets used in the previ- ous experiments as the evaluation subjects. For each sample, we collected the original DocString along with the three compressed DocStrings generated by three methods. To ensure comprehensive evaluation, the samples were divided into six groups according to the dataset, and each group was evaluated by the three evaluators. Each DocString was assessed anonymously in terms of both Informativeness and Comprehensibility. The evaluation scale ranged from 0 to 4 points, where a higher score reflected higher quality. The final score for each criterion was the average of the scores from all three evaluators. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 18]
0:18 G. Yang, et al. Table 6. The average score and kappa score (in parentheses) of human study. Aspect Method HumanEval CodeHarmony MBPP Subtle Creative BigCodeBench InformativenessSelective_Context 2.83 (0.4) 3.37 (0.5) 3.22 (0.2) 2.85 (0.3) 3.55 (0.6) 3.60 (0.5) LLMLingua2 2.85 (0.5) 3.32 (0.4) 2.83 (0.5) 2.80 (0.5) 3.55 (0.5) 3.35 (0.5) ShortenDoc 3.83 (0.7) 3.75 (0.7) 3.13 (0.5) 3.77 (0.7) 3.97 (0.9) 3.96 (0.9) ComprehensibilitySelective_Context 1.79 (0.2) 2.44 (0.1) 2.86 (0.1) 1.77 (0.2) 1.87 (0.1) 1.90 (0.0) LLMLingua2 2.30 (0.4) 2.92 (0.3) 2.74 (0.3) 2.24 (0.4) 2.85 (0.2) 2.68 (0.2) ShortenDoc 3.48 (0.3) 3.50 (0.3) 3.61 (0.2) 3.33 (0.3) 3.67 (0.2) 3.66 (0.1) The questionnaire used for the study is illustrated in the Fig. 4. To maintain the quality of the human evaluation, the compressed DocStrings were presented in random order, ensuring that evaluators were unaware of which method was used to generate each DocString. Additionally, the evaluators were allowed to use the internet to look up unfamiliar concepts, ensuring that they had access to relevant information. To prevent evaluator fatigue and maintain focus, we limited each evaluator to assessing no more than 20 samples per half-day session. The results presented in Table 6 from the human study clearly demonstrate that the proposed ShortenDoc method outperforms the other two methods, Selective_Context and LLMLingua2, in both Informativeness and Comprehensibility. This superiority indicates that ShortenDoc is more effective in retaining key information and maintaining the understandability of DocStrings after compression. Furthermore, to appraise the consistency among the three evaluators, we computed Fleiss’ Kappa statistic [ 13,40]. Fleiss’ Kappa is a generalization of Cohen’s Kappa that is used to measure the reliability of categorical data when there are multiple raters. It provides a measure of the degree to which the evaluators agree beyond what would be expected by chance. A Kappa value of 1 indicates perfect agreement, while a value of 0 suggests no agreement beyond chance. Negative values indicate agreement less than what would be expected by chance. In our study, a Kappa coefficient was calculated to ensure that the evaluation scores were consistent and reliable across the evaluators. In terms of Informativeness, the ShortenDoc method consistently scores higher than the other two methods across most datasets. This suggests that ShortenDoc is adept at preserving the essential information within the DocStrings even after compression. In addition, the high Fleiss’ Kappa scores, ranging from 0.7 to 0.9, indicate a strong consensus among the evaluators regarding the Informativeness of the compressed DocStrings. This high agreement likely stems from the objective nature of this metric, where evaluators can clearly discern whether the compressed DocString retains the necessary information from the original. In terms of Comprehensibility, ShortenDoc also shows superiority than the other two methods in Comprehensibility, indicating that the compressed DocStrings remain understandable to human readers. While the lower Fleiss’ Kappa scores for Comprehensibility, ranging from 0.1 to 0.3, suggest that there is less consistency among the evaluators in this metric compared to Informativeness. This variability could be attributed to several factors: (1) Subjectivity. Comprehensibility is inher- ently more subjective than Informativeness, as it depends on the evaluator’s personal ability to understand the compressed information. (2) Complexity of Docstrings. Some DocStrings, despite being compressed, may still contain complex information or jargon that is more challenging for some evaluators to grasp. (3) Diversity in Evaluator Background. The professional background and experience of the evaluators can influence their assessment of DocString comprehensibility. For instance, evaluators with specific domain knowledge might find certain terms or concepts more comprehensible. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 19]
Less is More: DocString Compression in Code Generation 0:19 Table 7. Efficacy comparison within Different Method Name Quality DataSet Method DS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B Avg. HumanEvalOriginal 63.42 71.95 77.44 60.37 57.32 66.10 ShortenDoc 57.93 72.56 78.66 64.34 56.10 65.92 ShortenDoc w Foo 50.61 71.95 68.90 54.27 53.05 59.96 CodeHarmonyOriginal 59.48 64.36 60.78 64.71 59.48 61.76 ShortenDoc 55.56 66.01 62.75 60.78 61.44 61.31 ShortenDoc w Foo 48.37 60.13 58.82 51.63 54.25 54.64 MBPPOriginal 36.40 46.20 59.00 46.60 41.40 45.92 ShortenDoc 36.60 43.80 55.40 46.20 41.40 44.68 ShortenDoc w Foo 30.80 38.60 47.20 37.40 37.20 38.24 SubtleOriginal 53.00 61.00 63.00 64.00 56.00 59.40 ShortenDoc 54.00 56.00 62.00 66.00 56.00 58.80 ShortenDoc w Foo 39.00 48.00 58.00 53.00 54.00 50.40 CreativeOriginal 23.00 34.00 38.00 36.00 34.00 33.00 ShortenDoc 23.00 36.00 37.00 35.00 35.00 33.20 ShortenDoc w Foo 22.00 33.00 39.00 36.00 25.00 31.00 BigCodeBenchOriginal 6.10 12.20 13.50 14.20 10.80 11.36 ShortenDoc 2.00 6.10 10.10 9.50 4.70 6.48 ShortenDoc w Rename 2.00 7.40 10.80 12.20 8.10 8.10 Summary of RQ3 The human study confirms that ShortenDoc excels in both Informativeness and Compre- hensibility compared to SelectiveContext and LLMLingua2. The strong performance of ShortenDoc across both dimensions demonstrates that it not only compresses DocStrings effectively but also preserves critical information and maintains clarity, making it a practical solution for real-world applications. 6 DISCUSSION In this section, we delve into the insights gained from the DocString compression techniques and explore the patterns that emerged from our experiments. 6.1 The Impact of Method Name Quality In Section 3, we observed an intriguing phenomenon: models were capable of generating some correct code without relying on DocStrings, indicating that the method signature itself, particularly the method name, carries substantial information. This observation led us to hypothesize that the quality of method names plays a pivotal role in the success of DocString compression. To empirically validate this hypothesis, we designed a controlled experiment. We systematically replaced high-quality method names with a generic placeholder, such as ‘foo’ [ 51], across various datasets including HumanEval, CodeHarmony, MBPP, Subtle, and Creative. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 20]
0:20 G. Yang, et al. Conversely, we enhanced the BigCodeBench dataset by upgrading the low-quality method names to more descriptive and informative alternatives. We observed that all method names in the dataset were generic, lacking any meaningful information, such as ‘func_task’. To address this, we manually replaced each sample with high-quality method names that we crafted to provide better context and clarity. The outcomes of this experiment are encapsulated in Table 7. The data clearly demonstrate that method names wield a significant influence on the efficacy of DocString compression. Across the board, models exhibited a notable decline in efficacy when high-quality method names were replaced with the generic ‘foo’. This finding suggests that a portion of the information compressed byShortenDoc is redundant with the information already present in the method name. Conversely, when low-quality method names were enhanced, we observed a consistent uptick in model efficacy. This improvement underscores the notion that enriching the quality of method names not only aids in the comprehension of the code but also bolsters the robustness of Short- enDoc ’s compression capabilities. In light of these findings, we advocate for a conscientious approach to method name quality during the DocString compression process. By elevating the quality of method names, we can potentially enhance the clarity of the code and the effectiveness of the compression algorithm. This strategy may yield unexpected benefits, such as more efficient code generation and improved maintainability of the codebase. 6.2 The Impact of DocString Style The style of DocStrings is a critical yet often overlooked aspect of code documentation. In our investigation, we explored the impact of DocString style on the performance of our compression tool and the models’ ability to generate accurate and understandable code. Our analysis led to several key findings and considerations. Experiments with Newline and Tab Characters. In Section 4, during the preprocessing phase, we made an assumption that the removal of newline characters and tabs would not substantially affect the model’s code generation capabilities. This assumption was based on the idea that these characters, while useful for formatting, might not contribute to the semantic content that the model uses to understand the code structure and purpose. To test this, we conducted controlled experiments where we systematically removed these characters from the DocStrings and compared the model’s efficacy against a control group where these elements were retained. Impact of Stop Words. We also investigated the role of stop words in DocStrings. In our study, we define the stop words are commonly considered to be filler words that do not carry much meaning, which are chosen by LLMLingua2 used in a 10% ratio we discussed in Section 3. Our experiments involved removing stop words from DocStrings to assess if the model’s efficacy would change. The comparison between the efficacy with and without stop words helped us understand the contribution of these words to the overall comprehension of different LLMs. Impact of Redundant Intruction. Our analysis of the MBPP dataset revealed a prevalent pattern of redundant instructions, specifically the phrase "write a function to." Intuitively, we suspected that such repetitive and generic instructions might not be beneficial for the model in understanding the specific requirements of the code it is tasked to generate. To analyze the impact of these instructions, we designed a set of controlled experiments where we removed these phrases from the DocStrings. The comparison of the model’s output with and without these redundant instructions allowed us to evaluate their utility in the context of code generation. Findings and Implications. The findings from these experiments shed light on the importance of various elements within DocStrings. The removal of newline characters and tabs did not significantly impair the model’s efficacy, suggesting that these elements are more stylistic than functional in , Vol. 0, No. 0, Article 0. Publication date: .

[Página 21]
Less is More: DocString Compression in Code Generation 0:21 Table 8. Efficacy Comparison whitin Different DocString Styles DataSet Method DS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B Avg. HumanEvalOriginal 63.42 71.95 77.44 60.37 57.32 66.10 Remove New Line & Tab 57.32 76.83 83.84 66.46 60.37 68.96 Remove Stop Words 54.88 66.46 76.83 56.10 56.10 62.07 CodeHarmonyOriginal 59.48 64.36 60.78 64.71 59.48 61.76 Remove New Line & Tab 60.78 67.32 62.75 61.44 61.44 62.75 Remove Stop Words 57.52 67.32 60.14 60.78 58.17 60.79 MBPPOriginal 36.40 46.20 59.00 46.60 41.40 45.92 Remove New Line & Tab 37.80 47.20 59.00 47.80 43.00 46.96 Remove Stop Words 34.00 45.80 54.40 44.20 41.20 43.92 SubtleOriginal 53.00 61.00 63.00 64.00 56.00 59.40 Remove New Line & Tab 54.00 58.00 62.00 68.00 60.00 60.40 Remove Stop Words 46.00 56.00 61.00 62.00 55.00 56.00 CreativeOriginal 23.00 34.00 38.00 36.00 34.00 33.00 Remove New Line & Tab 22.00 31.00 33.00 34.00 30.00 30.00 Remove Stop Words 20.00 29.00 29.00 30.00 31.00 27.80 BigCodeBenchOriginal 6.10 12.20 13.50 14.20 10.80 11.36 Remove New Line & Tab 1.40 8.80 12.20 7.40 6.80 7.32 Remove Stop Words 2.0 8.10 9.50 10.80 6.10 7.30 Table 9. Efficacy comparison of Redundant Intruction DataSet Method DS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B Avg. MBPPOriginal 36.40 46.20 59.00 46.60 41.40 45.92 w/o ‘Write a python function to’ 37.80 45.60 57.40 47.60 42.80 46.24 the context of code generation. The impact of stop words was also nuanced, with some instances showing a slight efficacy change, indicating that even seemingly inconsequential words might play a role in the model’s comprehension process. The removal of redundant instructions led to a modest improvement in efficacy, supporting our hypothesis that such phrases do not contribute positively to the model’s understanding and may even introduce noise. These experiments underscore the importance of a balanced approach to DocString style. While it is beneficial to maintain clear and concise documentation, the removal of certain elements should be done with consideration of their potential impact on the model’s comprehension. 6.3 Hyper-Parameter Analysis (1) The hyperparameter 𝜏significantly influences the performance of our method. As demonstrated in Table 10, different values of 𝜏result in varying performance metrics across the CodeHarmony dataset. The optimal value of 𝜏that maximizes performance may differ depending on the specific dataset and model architecture. For instance, a higher 𝜏value like 0.999 seems to work better for the CodeQwen, CodeGeeX4, and Llama3.1 model, while a lower value like 0.985 could be more , Vol. 0, No. 0, Article 0. Publication date: .

[Página 22]
0:22 G. Yang, et al. Table 10. Efficacy Comparison of Different 𝜏in CodeHarmony DataSet 𝜏 Ratio DS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B Avg. CodeHarmony- 0 59.48 64.36 60.78 64.71 59.48 61.76 0.985 32 58.82 62.09 59.48 56.86 56.21 58.69 0.990 29 56.21 64.05 62.10 59.48 58.17 60.00 0.995 26 56.86 66.67 62.10 59.48 60.78 61.18 0.999 25 56.21 66.01 63.40 60.78 61.44 61.44 Table 11. Efficacy Comparison of Different Base Models in CodeHarmony DataSet Base Model DS-1.3B DS-6.7B CQ-7.3B CG-9.4B LA-8.0B Avg. CodeHarmonyCodeGen-350M 52.94 62.75 57.52 54.25 52.94 56.08 GPT2 41.83 45.75 47.71 43.14 44.44 44.57 CodeGPT-py-adapted 56.21 66.01 63.40 60.78 61.44 61.44 effective for the DeepSeekCoder-1.3B model. Given the good balance or optimal efficacy for the models and datasets, we set the hyperparameter 𝜏to 0.999. (2) The choice of base model also plays a critical role in the performance of our method. As shown in Table 11, different base models have different baseline performances. For the CodeHarmony dataset, the CodeGPT-py-adapted base model outperforms the other two models across all metrics, indicating that it is better suited for this specific dataset. This suggests that choosing the suitable base model is especially important for code generation tasks. 6.4 Threats to Validity In this subsection, we analyze potential threats to the validity of our empirical study. Threats to Internal Validity. The first internal threat is the possibility of implementation errors inShortenDoc . To alleviate this, we conducted a thorough code inspection of the implementation and utilized mature libraries. The second internal threat is the implementation correctness of the considered baselines. To mitigate this threat, we implemented all baselines by running their open-source code directly or reimplementing them according to the original studies. Threats to External Validity. The main external threat lies in the choice of datasets and models used in our study. To alleviate this, we selected six diverse datasets with high reputations to reflect the complexity of real-world scenarios. Importantly, our approach can be applicable to different programming languages. In terms of the choice of models, we selected six state-of-the-art models in code generation including DeepSeekCoder-1.3b, DeepSeekCoder-6.7b, CodeQwen1.5, CodeGeeX4, Llama3.1, and GPT-4o. Threats to Construct Validity. Construct threats concern the performance metrics used to evaluate ShortenDoc and baselines. To evaluate the performance of models, we utilized Pass@1 metric which is commonly used in in the previous studies of code generation. Additionally, we used Compression Ratio, which is also widely used in similar studies, to measure the proportion of reduction in the length of the prompt after compression. Furthermore, we conducted a human study to analyze the qualitative impact of DocString compression. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 23]
Less is More: DocString Compression in Code Generation 0:23 7 RELATED WORK 7.1 Code Generation Code generation aims to produce code snippets from given natural language descriptions or requirements. Some studies [ 22,28] use sequence-based models, which treat the source code as a sequence of tokens and utilize neural networks to generate the source code token by token based on the input description. Other studies [ 43,53] use tree-based models, .ie., construct a parse tree of the program from the natural language description and subsequently convert it into corresponding code. In recent years, researchers have gradually utilized pre-trained models for code generation tasks, which have outperformed conventional sequence-based and tree-based methods. These models are pre-trained on massive data of source code and then fine-tuned on code generation task. For example, models like CodeGPT [ 24], PLBART [ 1], and CodeT5 [ 47] leverage the GPT, BART, and T5 architectures of language models pre-trained on code corpora. However, these models are more suitable for fine-tuning code generation tasks, as their parameter numbers are not large enough to demonstrate emergent capabilities in zero-shot scenarios. With the development of LLM research, LLMs with over a billion parameters have been employed for zero-shot code generation tasks. Current Code LLMs can be divided into two categories: standard language models and instruction-tuned models [18]. Standard Language models are pre-trained on the raw corpus with the next-token prediction. With the success of GPT series [ 4,38,39] in NLP, Chen et al. [ 6] adapted similar ideas into the domain of source code and fine-tunes GPT models on code to produce closed-source CodeX, which is pre-trained on GitHub code with 12 billion model parameters. To replicate its success, Nijkamp et al. [ 32,32] proposed CodeGen and CodeGen2, which are large language models for code with multi-turn program synthesis. Zheng et al. [ 54] proposed CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters. CodeGeex is pre-trained on a large code corpus of over 20 programming languages and has good performance for generating executable programs in several mainstream programming languages like Python, C++, Java, JavaScript, Go, etc. Li et al. [19] proposed StarCoder, a 15.5 billion parameter model whose training data incorporates more than 80 different programming languages as well as text extracted from GitHub issues and commits and from notebooks. Differing from the aforementioned decoder-only model, Wang et al. [ 46] proposed CodeT5+, a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Instruction-tuned models are fine-tuned using instruction tuning [ 48]. Instruction tuning helps models follow user’s instructions. OpenAI’s ChatGPT [ 34] is trained by Reinforcement Learning with Human Feedback (RLHF) [ 35], making it capable of programming tasks. However, it is of closed-source. For the open-source models, luo et al. [ 25] introduce Wizardcoder by fine-tuning StarCoder [ 19] with Evol-Instruct and ChatGPT feedback with Code Alpaca’s dataset as seed dataset. wang et al. introduce InstructCodeT5+ [ 46] by fine-tuning CodeT5+ [ 46] on Code Alpaca’s [ 5] dataset. In this article, our main goal is to compress docstrings within code prompts without losing their semantic integrity in the field of code generation. We have observed that the model can still understand the task requirements and generate correct code after removing some of the redundant information in docstrings. This observation has motivated us to further investigate the compression of docstrings , an essential component of prompt. To achieve the goal, we have conducted an empirical study on code generation task. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 24]
0:24 G. Yang, et al. 7.2 Prompt Compression Prompt Compression attempts to shorten the original prompts without losing essential information. Prompt Compression methods [ 16] can be grouped into three main categories : Token pruning and token merging, Soft prompt tuning methods and Information-entropy-based approaches. Token pruning and token merging need model fine-tuning or intermediate results during inference and have been used with BERT-scale models. For instance, Modarressi et al.[ 27] introduced AdapLeR which dynamically eliminates less contributing tokens through layers to achieve shorter lengths. Soft prompt tuning requires LLMs’ parameter fine-tuning to make them suitable for specific domains but not applicable to black-box LLMs. Wingate et al. [ 49] used the framework of soft prompts to manipulate prompt compression. Mu et al. [ 29] proposed GIST, a manner to compresse arbitrary prompts into a smaller set of Transformer activations on top of virtual “gist” tokens. However, these methods are task-aware and usually tailored for specific tasks and compression ratios, which may limit their generalizability in real-world scenarios. Information-entropy-based approaches use a small language model to calculate the self-information or perplexity of each token in the original prompt and then remove tokens with lower perplexities. For example, Li et al. [ 21] introduced Selective Context, which employs self-information to filter out less informative content, resulting in the efficiency of the fixed context length. Jiang et al. [ 15] proposed LLMLingua, a coarse-to-fine prompt compression method to reduce the length of original prompts while preserving essential information. Based on LLMLingua, Pan et al.[ 36] proposed a data distillation procedure to derive knowledge from an LLM (GPT-4) to compress the prompts without losing crucial information. All of these approaches are task-agnostic prompt compression methods and have better generalizability and efficiency compared with task-aware methods. In contrast to the previous studies, we present a novel adaptive compression approach targeting code generation. This compression approach emphasizes the understanding of code semantics and removes redundant content by analyzing the importance of individual tokens. To improve the efficiency of prompt compression, we employ a Top-N strategy, which optimize the compression process and greatly preserve the semantic integrity. 8 CONCLUSION AND FUTURE WORK In our study, we focus on DocString compression and avoiding the loss of essential information in docstring. Thereby, we propose a novel compression method ShortenDoc . This compression method dynamically adjusts the compression rate and retains greater informativeness and comprehensibility in compressed docstrings. By implementing this compression method, our goal is to improve model efficiency and reduce the computational resources cost. While our current research has focused primarily on function level code generation, we recognize that the complexity of code generation increases as the code structure expands. Therefore, future work will extend to class level code generation, which will involve more complex logic and larger code structures. We believe that ShortenDoc ’s approach can accommodate these higher-level code structures while maintaining its compression efficiency and output quality. In addition, we plan to explore code generation in a multi-language environment. With the diversification of global software development, docstring compression tools that support multiple programming languages will be of higher utility. We plan to train a multilingual base language model, which will enhance the applicability and performance of ShortenDoc in different programming languages. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 25]
Less is More: DocString Compression in Code Generation 0:25 ACKNOWLEDGMENTS This work was partially supported by the National Natural Science Foundation of China (NSFC, No. 61972197 and No. 62372232), the Natural Science Foundation of Jiangsu Province (No. BK20201292), the Collaborative Innovation Center of Novel Software Technology and Industrialization, and the Short-term Visiting Program of Nanjing University of Aeronautics and Astronautics for Ph.D. Students Abroad (No. 240501DF16). T. Chen is partially supported by an oversea grant from the State Key Laboratory of Novel Software Technology, Nanjing University (KFKT2022A03). REFERENCES [1]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program understanding and generation. arXiv preprint arXiv:2103.06333 (2021). [2]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al .2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021). [3]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al . 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023). [4] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [5]Sahil Chaudhary. 2023. Code alpaca: An instruction-following llama model for code generation. GitHub repository (2023). [6]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al .2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [7]Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, and Jianwei Yin. 2024. Chatunitest: A framework for llm-based test generation. In Companion Proceedings ofthe32nd ACM International Conference ontheFoundations ofSoftware Engineering. 572–576. [8] Nicola Dainese, Alexander Ilin, and Pekka Marttinen. 2024. Can docstring reformulation with an LLM improve code generation?. In Proceedings ofthe18th Conference oftheEuropean Chapter oftheAssociation forComputational Linguistics: Student Research Workshop. 296–312. [9]Xi Ding, Rui Peng, Xiangping Chen, Yuan Huang, Jing Bian, and Zibin Zheng. 2024. Do Code Summarization Models Process Too Much Information? Function Signature May Be All That Is Needed. ACM Transactions onSoftware Engineering andMethodology 33, 6 (2024), 1–35. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al .2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [11] Shubhang Shekhar Dvivedi, Vyshnav Vijay, Sai Leela Rahul Pujari, Shoumik Lodh, and Dhruv Kumar. 2024. A comparative analysis of large language models for code documentation generation. In Proceedings ofthe1stACM International Conference onAI-Powered Software. 65–73. [12] Flab-Pruner. 2024. Flab-Pruner: Towards Greener Yet Powerful Code Intelligence via Structural Pruning. https: //github.com/Flab-Pruner/Flab-Pruner. [13] Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin 76, 5 (1971), 378. [14] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, YK Li, et al .2024. DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence. arXiv preprint arXiv:2401.14196 (2024). [15] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. In Proceedings ofthe2023 Conference onEmpirical Methods in Natural Language Processing. 13358–13376. [16] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839 (2023). [17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. InProceedings oftheACM SIGOPS 29th Symposium onOperating Systems Principles. [18] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2024. AceCoder: An Effective Prompting Technique Specialized in Code Generation. ACM Transactions onSoftware Engineering andMethodology (2024). , Vol. 0, No. 0, Article 0. Publication date: .

[Página 26]
0:26 G. Yang, et al. [19] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al .2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023). [20] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al .2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092–1097. [21] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023. Compressing Context to Enhance Inference Efficiency of Large Language Models. In Proceedings ofthe2023 Conference onEmpirical Methods inNatural Language Processing . 6342–6353. [22] Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočisk `y, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744 (2016). [23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances inNeural Information Processing Systems 36 (2024). [24] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al .[n.d.]. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. In Thirty-fifth Conference onNeural Information Processing Systems Datasets andBenchmarks Track (Round 1). [25] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568 (2023). [26] Antonio Valerio Miceli-Barone and Rico Sennrich. 2017. A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation. In Proceedings oftheEighth International Joint Conference onNatural Language Processing (Volume 2:Short Papers). 314–319. [27] Ali Modarressi, Hosein Mohebbi, and Mohammad Taher Pilehvar. 2022. Adapler: Speeding up inference by adaptive length reduction. arXiv preprint arXiv:2203.08991 (2022). [28] Lili Mou, Rui Men, Ge Li, Lu Zhang, and Zhi Jin. 2015. On end-to-end program generation from user intention by deep neural networks. arXiv preprint arXiv:1510.07211 (2015). [29] Jesse Mu, Xiang Li, and Noah Goodman. 2024. Learning to compress prompts with gist tokens. Advances inNeural Information Processing Systems 36 (2024). [30] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In Proceedings oftheIEEE/ACM 46th International Conference onSoftware Engineering . 1–13. [31] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al .2022. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 (2022). [32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022). [33] Changan Niu, Ting Zhang, Chuanyi Li, Bin Luo, and Vincent Ng. 2024. On Evaluating the Efficiency of Source Code Generated by LLMs. In Proceedings ofthe2024 IEEE/ACM First International Conference onAIFoundation Models andSoftware Engineering. 103–107. [34] OpenAI. 2022. ChatGPT. https://openai.com/blog/chatgpt. [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022. Training language models to follow instructions with human feedback. Advances inneural information processing systems 35 (2022), 27730–27744. [36] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. 2024. LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression. In Findings oftheAssociation forComputational Linguistics ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand and virtual meeting, 963–981. https://aclanthology.org/2024.findings-acl.57 [37] Bibek Poudel, Adam Cook, Sekou Traore, and Shelah Ameli. 2024. DocuMint: Docstring Generation for Python using Small Language Models. arXiv preprint arXiv:2405.10243 (2024). [38] Alec Radford. 2018. Improving language understanding by generative pre-training. (2018). [39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al .2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. , Vol. 0, No. 0, Article 0. Publication date: .

[Página 27]
Less is More: DocString Compression in Code Generation 0:27 [40] Justus J Randolph. 2005. Free-Marginal Multirater Kappa (multirater K [free]): An Alternative to Fleiss’ Fixed-Marginal Multirater Kappa. Online submission (2005). [41] Sanka Rasnayaka, Guanlin Wang, Ridwan Shariffdeen, and Ganesh Neelakanta Iyer. 2024. An empirical study on usage and perceptions of llms in a software engineering project. In Proceedings ofthe1stInternational Workshop onLarge Language Models forCode. 111–118. [42] Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo. 2024. Greening large language models of code. In Proceedings ofthe46th International Conference onSoftware Engineering: Software Engineering inSociety . 142–153. [43] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020. Treegen: A tree-based transformer architecture for code generation. In Proceedings oftheAAAI conference onartificial intelligence , Vol. 34. 8984–8991. [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances inNeural Information Processing Systems 30:Annual Conference onNeural Information Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998–6008. https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html [45] Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, et al .2023. ReCode: Robustness Evaluation of Code Generation Models. In The61st Annual Meeting OfTheAssociation ForComputational Linguistics. [46] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023). [47] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder- decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021). [48] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021). [49] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162 (2022). [50] Chunqiu Steven Xia, Yinlin Deng, and Lingming Zhang. 2024. Top Leaderboard Ranking= Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM. arXiv preprint arXiv:2403.19114 (2024). [51] Guang Yang, Yu Zhou, Wenhua Yang, Tao Yue, Xiang Chen, and Taolue Chen. 2024. How important are good method names in neural code generation? a model robustness perspective. ACM Transactions onSoftware Engineering and Methodology 33, 3 (2024), 1–35. [52] Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, and David Lo. 2024. Robustness, security, privacy, explainability, efficiency, and usability of large language models for code. arXiv preprint arXiv:2403.07506 (2024). [53] Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. arXiv preprint arXiv:1704.01696 (2017). [54] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X. In Proceedings ofthe29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 5673–5684. [55] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al .2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877 (2024). , Vol. 0, No. 0, Article 0. Publication date: .