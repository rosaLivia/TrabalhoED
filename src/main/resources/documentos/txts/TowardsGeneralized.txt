
[Página 1]
PROJECT MPG: TOWARDS A GENERALIZED PERFORMANCE BENCHMARK FOR LLM CAPABILITIES Lucas Spangher1,4Tianle Li2William F. Arnold3 Nick Masiewicki1Xerxes Dotiwalla1Rama Parusmathi1Peter Grabowski1 Eugene Ie1Dan Gruhl1 1Google Research2UC Berkeley3KAIST4MIT Plasma Science and Fusion Center ABSTRACT There exists an extremely wide array of LLM benchmarking tasks, whereas often- times a single number is the most actionable for decision-making, especially by non-experts. No such aggregation schema exists that is not Elo-based, which could be costly or time-consuming. Here we propose a method to aggregate performance across a general space of benchmarks, nicknamed Project ”MPG,” dubbed Model Performance and Goodness, additionally referencing a metric widely understood to be an important yet inaccurate and crude measure of car performance. Here, we create two numbers: a ”Goodness” number (answer accuracy) and a ”Fast- ness” number (cost or QPS). We compare models against each other and present a ranking according to our general metric as well as subdomains. We find signif- icant agreement between the raw Pearson correlation of our scores and those of Chatbot Arena, even improving on the correlation of the MMLU leaderboard to Chatbot Arena. 1 I NTRODUCTION Miles Per Gallon (MPG) has long been useful as a standardized, one-dimensional measure of vehicle fuel efficiency. Although the limitations of MPG are well documented — particularly its inability to capture the full spectrum of a vehicle’s performance and environmental impact—its utility lies in providing a single, generalized measure that simplifies comparisons between vehicles while re- taining some accuracy, making MPG a standard bearer in consumer decision-making and various regulatory contexts. We endeavor to apply the same sort of principled aggregation techniques to Large Language Models (LLMs). There exist a vast array of benchmarks for LLMs (i.e. logic (Kil et al., 2024), math (Liu et al., 2024), law (Guha et al., 2024), linguistic understanding (Narayan et al., 2018), factual recall (Hendrycks et al., 2020), general performance ((bench authors, 2023), etc.) yet in many cases, decision-makers require a single, unified metric to facilitate model selection. In this paper, we introduce a novel aggregation approach, dubbed Project MPG – which nods both to Miles Per Gallon and also Model Performance and Goodness, a more accurate description of our focus. Project MPG generates two primary metrics: a “Goodness” score, representing general answer ac- curacy, and a “Performance” score, reflecting queries per second (QPS). These metrics are derived from the aggregation of various open benchmarks, designed to: (1) be representative of a general- ized, real-world use cases by focusing on key domains where benchmarks correlate, (2) maintain relational distances between models, similar to those captured by existing intelligence and latency evaluations, and (3) be quick to compute and financially efficient. Please see Figure 1 for the calcu- lation of Goodness vs Performance that we will further define throughout our paper. Our target audience includes resource-constrained developers - such as engineers at smaller com- panies or universities — who lack access to human evaluations, large-scale compute, or public 1arXiv:2410.22368v1 [cs.SE] 28 Oct 2024

[Página 2]
Figure 1: Outcome of our MPG benchmark applied to thirteen publicly facing language models. Here, the x axis is the “Performance” (Queries Per Second), which we express on the log scale, and the y axis is “Goodness” (our benchmark’s outcome). The error is 95% confidence intervals described in Section 3.3. ratings. By providing a lightweight evaluation approach, we enable these users to select models that align with their specific requirements for quality and latency. Additionally, our approach may be of interest to teams that need to rapidly evaluate internal model versions to quantify incremental improvement, or test that fine-tuning efforts have not caused general capabilities to decrease. This framework would serve many developing or deploying large language models. To our knowledge, we are the first to attempt to systematically reduce different benchmarks into one interpretable number while also focusing on computational and financial efficiency of evalua- tion. We evaluate fourteen models considered state of the art, selected for disjointedness, that are currently supported for production on easy to access platforms, providing a comprehensive view of their generalized intelligence. Furthermore, we provide metrics correlating our aggregation to gold standard evaluation aggregations, and find a strong correlation. 2 R ELATED WORK Evaluating Large Language Models (LLMs) has become increasingly important as their usage ex- pands across diverse applications. One approach that has gained traction is LLM as a judge (Zheng et al., 2023; Dubois et al., 2024b), where models are used to evaluate other models by scoring gener- ated outputs. Indeed, several benchmarks which employs LLM-as-Judges, such as Arena-Hard-Auto (Li et al., 2024) and AlpacaEval 2.0 (Dubois et al., 2024a). LLM-as-a-judge raises questions about biases and objectivity, as LLM judges may have similar myiopias to the LLMs that they are judging. Most non-LLM-as-judge benchmarks are thus static and ground-truth-based (e.g., multi-choice question answering). They cover a wide range of domains, including math, science, coding, and reasoning. Common ones include MMLU Hendrycks et al. (2020), MATH Hendrycks et al. (2021), GSM-8K Cobbe et al. (2021), HumanEval Chen et al. (2021), BigBench bench authors (2023), Hel- laSwag Zellers et al. (2019), AGIEval Zhong et al. (2023), as well as comprehensive collection such as HELM Liang et al. (2023). However, one recent development is LLM-Sys Elo ratings inspired by the Elo rating system used in competitive games. This method evaluates LLMs by having them compete in pairwise comparisons, allowing models to be ranked dynamically based on their performance against others in specific tasks, and has been implemented on many scales (Luo et al., 2024). However, there are critiques to Elo rankings (Boubdir et al., 2023). Namely, (1) there is difficulty representing a suitable breadth 2

[Página 3]
of questions; as different model matchups are served different questions, rankings are created in opaque and non-standard ways. (2) Each matchup’s winner isn’t actually reflective of good quality: one matchup featuring two similarly bad responses may look the same to the ranking as a matchup featuring similarly good responses. (3) These flaws may only be resolved with rather extreme com- putational or human cost, with Chatbot Arena featuring O(10k) votes per top model. (4) An Elo ranking thus has difficulty in comparing a model’s change over time; a fixed benchmark may be run more routinely, is less opaque, and is better for understanding. The backbone of the well known Chatbot Arena, although competition based, is Bradley Terry (Chiang et al., 2024). DyVal 2 , or Dynamic Evaluation, both proposes a grouping of benchmark questions into different psychometric domains and a method by which benchmark questions may be kept uncontaminated through heuristic strategies, like shuffling multiple choice answers or adding incorrect answers – strategies that meaningfully test whether the LLM is memorizing order or wording (Zhu et al., 2024; Lin et al., 2024). Together, these approaches represent a shift toward more nuanced and adaptive methods for evaluating LLMs, highlighting the need for evaluation systems that keep pace with the rapid advancements in model development. 3 B ENCHMARK METHODOLOGY 3.1 B ENCHMARK SELECTION To determine which benchmarks to assign under specific hierarchies, we ensure comprehensive coverage LLM benchmark domains as measured in the work of Ilic 2023 (Ili ´c & Gignac, 2024). Ilic et al. highlight that the primary benchmarks in Chatbot Arena show varying degrees of cross- correlation; a model’s strong performance in certain benchmarks often predicts success in related ones. By analyzing distinct clusters within their pairwise correlation matrix, we selected represen- tative benchmarks from each cluster: the MMLU-redux global facts, MMLU college mathematics and computer science, BigBench ambiguous and disambiguous benchmarks in sexuality, race, and socioeconomic status, and ARC-C-Challenge. We included some additional benchmarks beyond those in the cross correlation matrix for the sakes of representing famous benchmarks: SQuAD-2 (Rajpurkar et al., 2018), BoolQ (Clark et al., 2019), OpenBookQA (Mihaylov et al., 2018), and Cli- mate Fever (Diggelmann et al., 2020). This targeted selection captures a broad spectrum of LLM capabilities while minimizing redundancy. Having selected benchmarks, we move on to scoring and aggregating them. For multiple choice questions, which compromise the majority of our dataset, we prepare the prompt in the following way: You are a succinct and smart LLM who answers questions parsimoniously. Here is your question: ... And here are your options: (A:..., B:..., C:..., D:...). Please answer with the letter corresponding to the choice, only! We score multiple choice questions by performing an 1-gram lookup of the correct letter. For boolean questions, we prepare the prompt with the same prefix: You are a succinct and smart LLM who answers questions parsimoniously. Here is your question:... Answer in a True/False only! And simply score the answer using an XOR with the correct response. Please see Figure 6 for a description of the relevant benchmark domains. 3.2 B ENCHMARK GROUPING In line with psychometric traditions, we categorize our MPG subdomains into three primary areas: 1.Factual Recall : This subdomain assesses the model’s domain knowledge, particularly in relation to global facts, science, and climate change, which are known to correlate with other factual datasets. The benchmarks used in this category include BoolQ (developed 3

[Página 4]
by the Google AI Language team) (Clark et al., 2019), the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2018), MMLU Global Facts (Hendrycks et al., 2020), and the ClimateFever dataset (Diggelmann et al., 2020). We omit the context from the SQuAD questions in order to present a more pure recall task for the models. 2.Linguistic Capability and Social Understanding : This area focuses on the model’s sensi- tivity to social biases. Specifically, we evaluate the model using BigBench’s benchmarks on sensitivity to LGBT identity and race, which are known to be cross-correlated with broader social sensitivities (bench authors, 2023). 3.Problem Solving : This subdomain tests the model’s ability to solve complex problems. We employ the MMLU College-level Computer Science and Math to evaluate problem-solving skills. Under each subtree, we group all of the benchmarks associated with them and perform a Bayesian posterior sampling as described in Section 3.3. MPG Factual KnowledgeSocial SensitivityProblem Solving SQuAD Global Facts BoolQ Climate Change Info OpenBook QA ARC-CSexuality AmbigSexuality Disambig Race AmbigRace Disambig SES AmbigSES DisambigMMLU College CSMMLU College Math Figure 2: Hierarchical structure of MPG metrics. Please note that each of the six leaf nodes of “Factual Knowledge” and “social sensitivity” are treated as equal leaf nodes; we drew fewer arrows only to simplify the figure. 3.3 S CORE AGGREGATION We consider each node iin this tree as a beta distribution with shape Beta (αi, βi), and each collec- tion of children under a parent to be overlapping samples from a similar space. Thus, our goal in aggregation is to use observed data from the leaf nodes to resolve the latent posterior beta distribu- tions representing a model’s capabilities on subdomains that we do not observe directly. The mean and 95% coverage of these latent aggregates become the scores that we present in Figure 1 and 4. The score of the model’s answers on each benchmark question is an observation which can be mod- eled by a binomial likelihood function. As a reminder to the reader, a beta distribution is conjugate with a binomial likelihood function; therefore, when defining the prior to be non-informative; that is, alima,b→0Beta(a, b), the posterior beta distributions is computed by setting the distributions’ parameters to Beta (#scores , Ni−#scores ). Here, Niis the number of questions in each bench- mark. We propose a Monte-Carlo Markov Chain (MCMC) to simulate latent questions from the aggregate beta distributions, in which we draw a probability from each child posterior to simulate a single latent “score” from a Bernoulli distribution. Specifically, here is the above in pseudocode: 1:Initialization: 2:LetN=PNi∀nodes i 3:Letxibe a scored question, Xithe set of scored questions on each question from leaf node i 4:Letzkbe a sample, Zkthe set of samples from the binomial likelihood for each non-child node 5:LetDbe the space of subdomains with d∈Dreferring to each second-level (subdomain) node 4

[Página 5]
6: 7:Leaf (Measured Benchmarks) Layer: 8:foreach leaf node ido 9: Sample pi∼Beta(αi, βi)where αi=Pxiandβi=Ni−Pxi 10: fork= 1toNddo 11: Sample zk∼Bernoulli (pi) 12: end for 13:end for 14: 15:Second (Subdomains) Layer: 16:foreach subdomain d∈Ddo 17: Compute the posterior of the parent node summarizing each subdomain: 18: Beta(Pzd, Nd−Pzd) 19: Sample pd∼Beta(Pzd, Nd−Pzd) 20: fork= 1toNdo 21: Sample zk∼Bernoulli (pd) 22: end for 23:end for 24: 25:Final Layer: 26:Compute the posterior of the root node as: 27: Beta(PZ, N−PZ) 4 M ODEL EVALUATION In order to evaluate models, we used a RunPod console to inference six open source models on A100 GPUs: yi-1.5-34b-chat, llama-3.1-70b-Instruct, quen2-72b-Instruct, phi-3-small-8k-instruct, gemma-2-9b-it, gemma-2-27b-it, and qwen2-72b-instruct, and the following eight proprietary mod- els on their own public facing APIs: GPT-4o-2024-05-13, Gemini 1.5 Pro 001 05-24, Gemini 1.5 Pro 08-27, Gemini 1.5 Flash 08-27, GPT-4-01-preview (Strawberry), Mistral-large 2, Claude 3.5 Sonnet 2024-06-20, and Claude 3 Opus 2024-02-29. We measured an average Queries-per-Second (QPS) by simply timing the response rate of every prompt that was sent to the external servers for our specific benchmark questions. Please note that another set of benchmark questions, including longer and multimodal questions, may have garnered a different QPS ordering. 5 R ESULTS 5.1 M ODEL RANKING For our main figure, please see Figure 1. Here we see a clear distinction between the proprietary models and the open source models in terms of IQ and QPS. Gemini-Pro-001, from mid May, was the furthest along on the pareto frontier that the line created. Many models are within the error bar distributions of other models. Furthermore, please see the Appendix for a full page figure showing the rankings between the mod- els, broken down into their subdomains, i.e. Figure 4. We do see a significant difference in the rankings of how different models perform on subdomains, indicating some degree of heterogeneity. GPT-4o leads the factual recall subdomain, whereas Mistral leads the social sensitivity subdomain and Gemini-Pro leads the problem solving by a sizeable margin. We note in Figure 3 that a clustered taxonomy of our individual benchmarks that the models’ perfor- mance aligns as we would expect: the factuality and problem solving benchmarks form a correlated cluster, and the social sensitivities form another larger cluster, although with more variance within. Please see an ordering of the LLMs that we studied in Figure 4. We note that models have different strengths, with some excelling more at problem solving than others. 5

[Página 6]
Figure 3: Taxonomy of subject groupings for the benchmark. 6

[Página 7]
5.2 C ORRELATION TO CHATBOT ARENA We calculate the raw score correlation and the rank number correlation of MPG to the Chatbot Arena score and rank, respectively. Additionally, we calculate the raw and rank score correlation of the MMLU rating to the Chatbot Arena score rating. We find significant correlations: Table 1: Correlation coefficients and p-values for pairwise comparisons Comparison Raw Pearson corr p-value Rank Pearson corr p-value MPG vs Arena Score 0.9157 0.0004 0.6868 0.0095 MPG vs MMLU 0.8326 0.0015 0.7182 0.0128 Arena Score vs MMLU 0.7721 0.0033 0.8462 0.0005 We note that MPG raw scores are slightly more correlated to the output of Chatbot Arena than MMLU raw scores are. The improvement in correlation is especially notable given the MMLU leaderboard includes an order of magnitude more questions than the MPG benchmark. Thus, if one’s goal were to estimate the Chatbot Arena ranking of a new model quickly, our benchmark may produce a higher probability estimate with less compute than another leading benchmark. Please see Figure 5 for correlation plot. 5.3 S OCIAL SENSITIVITIES In the social sensitivity benchmarks, LLMs are presented with two individuals who have different social characteristics. They are then asked questions, some of which are intentionally ambiguous, where no specific answer is expected, while others include clear factual details, and the goal is for the LLM to accurately recognize and respond to those details. (As a reminder to the reader, these questions are part of a classic benchmark, BigBench (bench authors, 2023).) We found a substantial difference in the probability that a model would answer ambiguous questions correctly relative to unambiguous. We read this finding in the context of responsible AI develop- ment, finding that many major language models have improved in this ratio relative to the original BigBench findings. For example, the Gemini Pro, Claude Sonnet and Opus, and Phi-3 models avoided generating harmful responses 100% of the time. However, we caution to the reader that more further study is warranted. We note as well that the pattern of consistent differences between scores is some evidence against data contamination. Were these datasets fully contaminated, we would expect the most competent models to get all or most questions correct evenly across ambiguous and disambiguous domains. Instead, we often find quite consistently lower performance on types of questions. 5.4 L IMITATIONS Any attempt to aggregate many capabilities into a single number will create problems, and we hope to list some here. First, in manually grouping the benchmarks, we assume that different measures within a sub-domain measure the same underlying construct (e.g., we assume that MMLU global facts tests the same recall skills as Squad 2 without context.) Treating domains as equivalent ob- servations may potentially misinterpret model capabilities. Second, this metrics doesn’t account for varying difficulty and reliability across different benchmark. Third, our decision to use non- informative priors obscures a bias of the type of questions – largely multiple choice – and how they may not directly line up with the way in which humans actually interface with LLMs. 6 C ONCLUSION In this work, we introduce IQ, a benchmarking framework that aggregates a minimal set of bench- marks in order to efficiently generalize an agent’s capabilities. Our approach prioritizes factual, falsifiable questions, such as “What is the height of the Eiffel Tower?” over more subjective prompts like “compose a beautiful haiku.” We intend our focus on factuality to ensure reproducibility and 7

[Página 8]
Model Race SO SES claude-3-opus-20240229 1.00 1.00 0.99 gpt-4o-2024-08-06 1.00 1.00 1.00 gemini-1.5-pro-experimental 1.00 1.00 1.00 gemini-1.5-pro-001 1.00 1.00 1.00 claude-3-5-sonnet-20240620 1.00 1.00 1.00 phi-3-small-8k-instruct 1.00 1.00 1.00 gemma-2-9b-it 0.99 1.00 1.00 yi-1.5-34b-chat 0.89 0.87 1.00 qwen2-72b-instruct 0.75 1.00 1.00 o1-preview-2024-09-12 0.37 0.88 0.05 llama-3.1-70b-instruct 0.35 0.99 0.03 mistral-large-2407 0.11 1.00 0.01 gemma-2-27b-it 0.01 0.99 0.42 gemini-1.5-flash-experimental 0.01 0.50 0.01 Table 2: This table displays the probability that a model’s posterior distribution of success on am- biguous social questions is higher than its posterior distribution of success on unambiguous social questions. A probability close to 0.5 indicates the model is equally likely to answer both types of questions correctly, while a probability close to 1 suggests the model is almost certain to perform better on ambiguous questions. For brevity, ”Sexual Orientation” is abbreviated as ”SO,” and ”So- cioeconomic Status” as ”SES.” enable objective, quantifiable evaluation metrics, with an eye towards consistent performance as- sessments. Our target audience includes resource-constrained stakeholders, such as modeling managers at smaller companies or universities, who may lack access to extensive human evaluations, large-scale testing, or public ratings like those solicited in Chatbot Arena. By providing a lightweight evaluation approach, we enable such users to select models that align with their specific requirements in terms of quality and latency. Additionally, this framework serves as a guide for those in the early stages of developing or deploying large language models (LLMs), offering a practical tool for navigating trade-offs between different models. We recognize that various applications will have different performance sensitivities—some prioritize latency, while others may emphasize accuracy or price. Our benchmark offers a flexible framework that can be adapted to reflect meaningful constraints in specific use cases, encouraging users to tailor evaluations to their unique needs and better understand the trade-offs inherent in selecting one model over another. In addition, we recognize that our framework has several limitations. First, the focus on multiple choice questions appears an idiosyncratic choice given how little they resemble the ways users ac- tually engage with LLMs. While this limitation is mitigated by the strong correlation we see with Chatbot Arena, it still raises questions about the generalizability across use cases. Furthermore, our benchmark does not include any direct tests of linguistic skills or sentiment analysis. In the future, we aim to extend this benchmark to cover multimodal tasks and more complex lin- guistic skills, such as text summarization. Additionally, we plan to incorporate dynamic, evolving benchmarks to mitigate the risks of dataset contamination, further improving the robustness and relevance of future evaluations. 8

[Página 9]
Figure 4: Orderings of the LLMs we studied. 9

[Página 10]
Figure 5: Raw score correlation between MPG and LMSys Chatbot Arena scores. We find a signif- icant correlation between the two. 10

[Página 11]
REFERENCES BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research , 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj . Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model evaluation. arXiv preprint arXiv:2311.17295 , 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo- tios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc- Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132 , 2024. Christopher Clark, Lee Kenton, Chang Ming-Wei, Kwiatkowski Tom, Collins Michael, and Toutanova Kristina. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL , 2019. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168 . Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims, 2020. Yann Dubois, Bal ´azs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators, 2024a. URL https://arxiv. org/abs/2404.04475 . Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2024b. URL https://arxiv.org/abs/2305. 14387 . Neel Guha, Julian Nyarko, Daniel Ho, Christopher R ´e, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: A collabo- ratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems , 36, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS , 2021. 11

[Página 12]
David Ili ´c and Gilles E. Gignac. Evidence of interrelated cognitive-like capabilities in large language models: Indications of artificial general intelligence or achievement? Intelligence , 106:101858, September 2024. ISSN 0160-2896. doi: 10.1016/j.intell.2024.101858. URL http://dx.doi. org/10.1016/j.intell.2024.101858 . Jihyung Kil, Zheda Mai, Justin Lee, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, Arpita Chowdhury, and Wei-Lun Chao. Compbench: A comparative reasoning benchmark for multi- modal llms. arXiv preprint arXiv:2407.16837 , 2024. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gon- zalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, 2024. URL https://arxiv.org/abs/2406.11939 . Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R ´e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuk- sekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Hen- derson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yi- fan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2023. URL https://arxiv.org/abs/2211.09110 . Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. arXiv preprint arXiv:2406.04770 , 2024. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wen- wei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the theory and application proficiency of llms with a hierarchical mathematics benchmark. arXiv preprint arXiv:2405.12209 , 2024. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yan- song Tang, and Weizhu Chen. Arena learning: Build data flywheel for llms post-training via simulated chatbot arena. arXiv preprint arXiv:2407.10627 , 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP , 2018. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. ArXiv , abs/1808.08745, 2018. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822 , 2018. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma- chine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830 . Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685 . Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023. URL https://arxiv.org/abs/2304.06364 . Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, and Xing Xie. Dynamic evaluation of large language models by meta probing agents, 2024. URL https://arxiv.org/abs/2402. 14865 . 12

[Página 13]
Figure 6: Pairwise Correlations between benchmarks listed in LLMSys. APPENDIX Please see a cross correlation matrix between the main benchmarks included in LMSYS 6. Please see a breakdown of the main subdomains. Factuality TruthfulQA https://github.com/sylinrl/TruthfulQA Global Facts https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux (MMLU Redux) Climate-FEVER https://huggingface.co/datasets/tdiggelm/climate_fever ARC-Challenge https://huggingface.co/datasets/allenai/ai2_arc BoolQ https://huggingface.co/datasets/boolq SQuAD https://huggingface.co/datasets/rajpurkar/squad Social Sensitivity and Linguistics BBQ Lite https://github.com/google/BIG-bench/tree/main/bigbench XSum (Summarization) https://huggingface.co/datasets/EdinburghNLP/xsum Problem Solving MMLU College Math https://huggingface.co/datasets/cais/mmlu MMLU College CompSci. https://huggingface.co/datasets/cais/mmlu Table 3: Benchmarks Used in the Evaluation 13