
[Página 1]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs SHAN JIANG, The University of Texas at Austin, USA CHENGUANG ZHU, The University of Texas at Austin, USA SARFRAZ KHURSHID, The University of Texas at Austin, USA Software testing remains the most widely used methodology for validating quality of code. However, effec- tiveness of testing critically depends on the quality of test suites used. Test cases in a test suite consist of two fundamental parts: (1) input values for the code under test, and (2) correct checks for the outputs it produces. These checks are commonly written as assertions, and termed test oracles. The last couple of decades have seen much progress in automated test input generation, e.g., using fuzzing and symbolic execution. However, automating test oracles remains a relatively less explored problem area. Indeed, a test oracle by its nature requires knowledge of expected behavior, which may only be known to the developer and may not not exist in a formal language that supports automated reasoning. Our focus in this paper is automation of test oracles for clients of widely used Java libraries, e.g., java.lang andjava.util packages. Our key insight is that Javadocs that provide a rich source of information can enable automated generation of test oracles. Javadocs of the core Java libraries are fairly detailed documents that contain natural language descriptions of not only how the libraries behave but also how the clients must (not) use them. We use large language models as an enabling technology to embody our insight into a framework for test oracle automation, and evaluate it experimentally. Our experiments demonstrate that LLMs can generate oracles for checking normal and exceptional behaviors from Javadocs, with 98.8% of these oracles being compilable and 96.4% accurately reflecting intended properties. Even for the few incorrect oracles, errors are minor and can be easily corrected with the help of additional comment information generated by the LLMs. CCS Concepts: •Software and its engineering →Formal software verification ;Correctness ;Com- pleteness ;Software verification ;•Computing methodologies →Artificial intelligence ;•General and reference→Metrics ;Evaluation . Additional Key Words and Phrases: Software Testing, Automated Test Generation, Test Oracle Generation, Large Language Models ACM Reference Format: Shan Jiang, Chenguang Zhu, and Sarfraz Khurshid. 2018. Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs. J. ACM 37, 4, Article 111 (August 2018), 19 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction Software testing is the most widely used methodology for validating code quality[ 41]. However, effectiveness of testing critically depends on the quality of test suites used. A test suite consists of multiple test cases and each test case has two fundamental parts: (1) input values used to run the Authors’ Contact Information: Shan Jiang, The University of Texas at Austin, Austin, USA, shanjiang@utexas.edu; Chenguang Zhu, The University of Texas at Austin, Austin, USA, cgzhu@utexas.edu; Sarfraz Khurshid, The University of Texas at Austin, Austin, Texas, USA, khurshid@ece.utexas.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 1557-735X/2018/8-ART111 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.arXiv:2411.01789v1 [cs.SE] 4 Nov 2024

[Página 2]
111:2 Shan et al. Fig. 1. An overview of software testing workflow, the dashed box represents our work’s scope code under test, and (2) correctness checks run to validate the outputs it produces. These checks are commonly written as assertions, and serve as executable test oracles. In the last few decades, researchers have made much progress in automated test input generation, e.g., using fuzzing[ 46,49] and symbolic execution[ 3,18,20]. Generating test oracles is inherently challenging as it requires an understanding of the expected behavior, which often is known only to the developer and may not be documented in a formal language specification to automated reasoning. This problem occurs in different scenarios, including mobile applications, embedded systems, quantum systems, etc.[16,24,30]. Besides, existing test oracle generation techniques typically focus on verifying alignment between implementations and their project-specific documentation[ 7,13,28]. However, there remains a gap in research on verifying if implementations align with broader language contracts, such as JDK Javadocs. Software bugs, ranging from unexpected exceptions to incorrect outputs, frequently stem from mismatches between the intended behavior described in natural language and its actual implementa- tion in code [ 1,12]. This problem has intensified with the rise of LLM-assisted programming, where developers use large language models (LLMs) to generate code from natural language prompts [9,32]. In particular, when working with Javadoc, client developers might unintentionally create inconsistencies by overlooking or misinterpreting constraints specified in the documentation. Therefore, a testing approach is essential to identify discrepancies between Javadoc descriptions and the corresponding client code that either overrides standard Java classes or utilizes standard Java interfaces. Test oracles play a crucial role in identifying discrepancies by ensuring that software behavior aligns with its intended functionality [ 23,35]. Traditional specification mining techniques have largely focused on deriving oracles from structured code or constrained specifications [ 38], often neglecting the rich potential of natural language descriptions. In contrast, while neural generative models [ 7] offer a more flexible approach, they face challenges in generating precise oracles due to the vast range of possible assertions. Furthermore, their effectiveness can vary significantly across different real-world Java projects, making it difficult to develop universally applicable test oracles [ 14]. Ideally, a technique should accurately generate oracles that capture the developer’s intent, maintain a low false positive rate, and be generalizable across projects to effectively test discrepancies between Javadoc and its client implementations. In this paper, we explore the potential of LLMs as intermediaries between natural language documentation and Java test oracles. Recently, LLMs have shown strong capabilities in tasks that require both natural language comprehension and code-related functionalities [ 26,31]. Trained on extensive corpora, including large codebases and documentation, these models possess an J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 3]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:3 implicit understanding of programming syntax and semantics. Research suggests that LLMs can effectively align user inputs with their pre-existing knowledge [ 27], and they even display emergent abilities to handle tasks uncommon in their training data [ 33]. In our context, the richness of JDK Javadocs allows us to derive general contracts directly from documentation, without needing code implementation. These contracts, when formatted as oracles, provide essential guidelines for client developers to follow. We introduce a novel methodology that utilizes LLMs to automate test oracle generation directly from JDK Javadocs, aiming to address subtle conventions that Java projects must conform to within JDK contracts. As illustrated in Figure 1, our approach integrates into the broader software testing process without requiring any actual code implementation. Unlike traditional methods that generate test oracles from each project’s documentation for use within that project, our approach leverages JDK Javadocs as the primary input. We focus on the JDK because it is foundational to all Java applications, making its conventions essential for Java projects, and its documentation is meticulously maintained to ensure quality [ 22,37]. By extracting key properties from the JDK, our generated test oracles represent general contracts that all client Java projects should adhere to. Our experimental results highlight the effectiveness of LLMs in generating relevant and accurate test oracles. Specifically, our approach successfully produces 97% compilable test oracles without the need for additional compilation fix tools. Our technique also shows strong applicability and completeness: LLMs cover 90.3% of the properties outlined in JDK Javadocs, with 96.0% of the generated oracles accurately validating these properties. Additionally, we generate oracles for 98.9% of exceptions defined in JDK Javadocs, with 97.2% of these oracles effectively capturing the intended exceptions, demonstrating LLMs’ capability in creating comprehensive and precise exception oracles. With high-quality comments and clear naming conventions, the generated oracles are intuitive and straightforward, facilitating efficient testing. These promising results underscore the potential of LLMs in identifying misalignments between software specifications and their implementations. To summarize, this paper makes the following contributions: •Idea. We introduce the idea of test oracle automation using Javadocs for standard Java libraries in order to automate testing of the clients of these libraries. •Approach. We use large language models (LLMs) as an enabling technology to embody our insight into a technique for automating test oracles. We propose a prompt template which is flexible and allows generation of test oracles for a variety of JDK Javadocs. •Evaluation. We conducted an experimental evaluation using some of the most widely used JDK Java classes and interfaces from java.lang andjava.util (e.g., Object ,String ,Map,Set,List) and present the resulting data. Our findings confirm that LLMs can generate highly effective and applicable test oracles, capable of accurately capturing expected behaviors and intended exceptions within the JDK. •Artifact. We release our prompt template and the complete set of generated test oracles for reproducibility. This allows developers to adapt our approach to their specific needs, ensuring that Java projects conform to JDK contract with minimal manual intervention. 2 An Illustrative Example Inheritance promotes code reusability and modularity by allowing a subclass to inherit attributes and methods from a superclass. However, it also introduces challenges for software maintenance and testing [ 19]. In the context of the Java Development Kit, failing to adhere to a superclass’s specifications can lead subclasses to violate JDK contracts, potentially undermining software J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 4]
111:4 Shan et al. 1public class Point{ 2 private int x; 3 private int y; 4 5 public void Point(int x, int y) { 6 this.x = x; 7 this.y = y; 8 } 9 10 public boolean equals(Object o) { 11 if (! (o instanceof Point)) { return false; } 12 Point p = (Point) o; 13 return (this.x == p.x) && (this.y == p .y) 14 } 15} Fig. 2. Point Class1public class Point3D extends Point{ 2 private int z; 3 4 public void Point3D(int x, int y, int z) { 5 super(x,y); 6 this.z = z; 7 } 8 9 public boolean equals(Object o) { 10 if (! (o instanceof Point3D)) { return false; } 11 Point p = (Point) o; 12 return super(x, y) && (this.z == p.z) 13 } 14} Fig. 3. Point3D Class Fig. 4. Symmetric property in the specification of java.lang.Object equals(Object o) method 1public void testSymmetric(){ 2Point p = new Point(3, 4); 3Point3D p3 = new Point3D(3, 4, 5); 4 assertTrue(p.equals(p3) == p3.equals(p)); 5}1boolean checkSymmetric(Object x, Object y) { 2 if (x == null || y == null) return x == y; 3 return x.equals(y) == y.equals(x); 4} Fig. 5. A test case revealing the discrepancy (left) and an oracle method for checking symmetric property (right) integrity and reliability. To illustrate this, we use an example with two Java classes: Point and Point3D , highlighting a discrepancy caused by a subclass violating JDK contracts. Figure 2 shows the implementation of Point class. It represents a point in two-dimensional space, with attributes for the xandycoordinates. The equals method in this class is overridden to check equality based on these coordinates. After verifying that the object being compared is an instance of the Point class, it compares the xandycoordinates for equality. Figure 3 is the implementation of Point3D class, which is a subclass of Point . The Point3D class extends the Point to three-dimensional space by adding a z-coordinate attribute. Overriding the equals method in Point3D requires calling the equals method of its superclass Point to compare the xandycoordinates, and additionally comparing the z-coordinate. According to the specification of the java.lang.Object class, which sits at the root of the Java class hierarchy, all overridden equals methods must satisfy the symmetric property, as shown in Figure 4. This property requires that for any non-null reference values xandy,x.equals(y) should return true if and only if y.equals(x) also returns true. However, the implementation of Point and Point3D violates this contract. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 5]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:5 The test cases in Figure 5 reveal a bug where the symmetry is violated. In this test case, while p3.equals(p) returns false (as p3considers the z-coordinate), p.equals(p3) return true (as Point only checks xandycoordinates, ignoring the additional z-coordinate in Point3D ). This discrepancy breaches the symmetry requirement, highlighting a common oversight in software development where developers may unintentionally violate inherited behaviors. In more complex industry settings, the logic often becomes increasingly intricate, leading to more critical errors. 3 Test Oracle Generation To tackle the challenge of testing discrepancies between JDK-defined properties and client project implementations, we leverage LLMs as an enabling technology to transform our insights into a technique for automating test oracles. In contrast to test oracle generation methods that rely on manually defined oracle templates derived from existing test cases and program source code (e.g., TOGA [ 7]), our approach requires no oracle templates or source code. The only input is natural language documentation, specifically Javadocs. We adopt LLMs to directly extract properties in Javadocs and generate useful test oracles for described properties. Figure 6 shows the architecture of our work. We introduce some useful prompting techniques to boost the LLMs performance without any fine-tuning. Figure 9 shows the structure of our prompt template. The proposed methodology utilizes a structured prompt design to guide the LLMs in generating useful test oracles. Specifically, the prompting process can be divided into several key steps. Step 1: Javadocs Partition . The original Javadocs is partitioned at the method level to ensure relevant method relationships are captured without overwhelming the LLMs. This prevents the LLMs from forgetting or missing key dependencies between methods, such as the relationship between equals() andhashCode() . Step 2: Assistant Creation . The LLMs is framed as a "Software Testing Engineer" to generate test oracles. This persona helps the model focus its reasoning and output on software testing tasks, ensuring it aligns with professional standards. Step 3: Few-shot Learning . Provide examples that illustrate the desired test oracles for different method properties. By giving exemplar input/output scenarios, the LLMs learns the format and analysis depth required to generate accurate test oracles. Step 4: Chain of Thought Reasoning . The tasks are broken down into multiple interconnected steps, such as identifying features and generating test oracles. This sequential approach helps the LLMs focus on one sub-task at a time, improving accuracy and coherence. The details of prompt engineering are described in the following sections. 3.1 Javadocs Partitioning Partitioning Javadocs at the method level is essential for improving LLM performance in generating accurate test oracles. Using the entire JDK Javadocs as a prompt poses challenges due to the LLMs’ context window limit. A long prompt can cause LLMs to lose track of prior instructions, resulting in incomplete details and potential “hallucinations” – where incorrect test oracles are generated as method relationships blur [ 2,34]. Additionally, LLMs face constraints in both input and output token limits. When presented with unstructured, lengthy prompts, they may inefficiently reiterate method descriptions instead of producing specific, actionable test oracles. Partitioning Javadocs also addresses issues arising from method interdependencies. For instance, in the Java API, methods like equals() andhashcode() are tightly coupled, as illustrated in Figure 7; overriding one usually requires overriding the other. If only one method is included in the prompt, LLMs may overlook these crucial relationships, leading to incomplete or incorrect test oracles. By J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 6]
111:6 Shan et al. Fig. 6. Architecture of Test Oracle Generation Fig. 7. equals and hashCode method defined in java.lang.Object partitioning at the method level, LLMs can concentrate on specific, related subsets of the Javadocs, ensuring key method dependencies are captured and correctly managed. We propose a partitioning strategy that segments Javadocs at the method level, grouping related methods, especially those linked through “See Also” sections, in the same prompt. This approach helps LLMs maintain relevant context while minimizing the processing load of an entire class or interface. For instance, since method equals() references hashcode() in its Javadocs, both are included together, enabling the LLMs to grasp their interdependence and generate coherent test oracles, as shown in Figure 8. Algorithm 1 details the steps of the Javadocs partitioning process. Each method is treated as a standalone input, with related methods included to provide full context. These method descriptions are then passed as input to the LLMs. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 7]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:7 1boolean checkEqualsHashCodeConsistency(Object x, Object y) { 2 if (x != null && y != null && x.equals(y)) { 3 return x.hashCode() == y.hashCode(); 4 } 5 return true; 6} Fig. 8. Test oracle for checking equals() and hashCode() contracts Algorithm 1: Javadocs Partition Input: Javadocs 𝑗; Output: Descriptions 𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛𝑠 ; foreach m in j do 𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛←𝑚 foreach n in m.SeeAlso do ifn in j then 𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛←𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛+𝑛 end end 𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛𝑠←𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛𝑠+𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛 end return descriptions 3.2 Assistant Creation We employ assistant creation to guide LLMs in generating test oracles, a method proven effective in prior research [ 8,48]. As shown in the <context> section (Line 1-5) of Figure 9, we initiate the prompt by stating, "You are a software testing engineer," which frames the task within software testing. This role assignment directs the LLMs to focus on testing-related reasoning, such as identifying properties described in Javadocs and translating them into executable test oracles, thereby enhancing response relevance and performance. This approach’s effectiveness stems from narrowing the LLM’s broad capabilities to a focused, goal-oriented persona. In practice, this role-based framing influences how the LLM interprets the prompt and prioritizes information. Acting as a software testing engineer, the LLM, when given a Javadocs description, concentrates on key behavioral properties such as preconditions, postconditions, invariants, or expected exceptions – critical elements for test oracle generation. This ensures that the output not only verifies correctness but also addresses edge cases and potential failure points. Moreover, the assistant creation sets a consistent tone for interaction with the LLM [ 11]. Beyond role specification, it reinforces testing principles like accuracy, reliability, and completeness in test oracles. For example, the prompt directs the LLM to ensure coverage of all properties mentioned in the Javadocs, preventing omissions that could lead to incomplete test oracle coverage. The assistant creation process transforms the LLMs from a general-purpose model into a special- ized tool, making it an intermediary between natural language Javadocs and test oracle generation. By defining the assistant’s role early on, we ensure that each subsequent step—parsing method descriptions, identifying test cases, and generating assertions – is executed with a focused software testing perspective. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 8]
111:8 Shan et al. 3.3 Few-shot Learning Few-shot learning is crucial for improving the LLM’s ability to generate high-quality test oracles by providing carefully crafted examples that steer it toward the desired output [ 29,47]. This technique offers the LLM a small yet representative set of input-output examples, helping it understand the format, depth, and specificity required for effective test oracle generation. This minimizes ambiguity and enables the LLM to generalize from these examples to new, varied contexts. Our core idea behind few-shot learning is to “teach” the LLM how to interpret Javadocs descrip- tions and translate them into test oracles by presenting a few solved examples. For instance, in the <example> section (Line 7-31) of Figure 9, we provide a Javadocs description for a method likeequals(Object obj) from the Object class and illustrate how to generate a test oracle verifying properties including reflexivity, symmetry, and transitivity. By supplying the model with these well-defined examples, we establish a pattern it can emulate when processing similar method descriptions in new contexts. Few-shot learning also enables the LLM to handle a broad range of method types, from simple methods without dependencies (e.g., getClass ) to more complex methods with interrelated properties (e.g., equals andhashCode ). We also use few-shot learning to enhance the LLM’s ability to handle edge cases, particularly for methods that may exhibit exceptional behavior, such as those that throw exceptions under specific conditions. By providing examples, we can guide the LLM to generate oracles that account for these cases. For instance, we demonstrate how to create a test oracle that verifies a method correctly throws a NullPointerException when given a null input, which is an edge case common in Java programs. 3.4 Chain of Thought We use Chain of Thought, a technique empirically shown to be effective in complex tasks [ 5,44], to enhance the LLM’s performance and accuracy in test oracle generation. This method involves breaking down the task into smaller, logically connected steps that guide the LLM through the process in a structured, step-by-step manner. By dividing the task into manageable sub-tasks, prompt chaining helps the LLM stay focused, producing coherent and accurate outputs while avoiding the challenges of handling excessive information at once [6, 25]. Our first prompt-chaining step focuses on feature extraction. The LLM is instructed to analyze the Javadocs for a given method and identify all relevant properties or behaviors requiring testing. For example, if the method is equals(Object obj) , the LLM is guided to extract properties such as reflexivity, symmetry, transitivity, consistency, and null handling – each a feature essential for verifying the method’s behavior as specified in the Javadocs. The second step in the chain is generating test oracles for each identified feature. After success- fully extracting the key properties of a method, the LLM is then tasked with creating specific test oracles to verify each property. For example, once the reflexivity of equals(Object obj) is identified, the LLM generates a test oracle to confirm that any non-null object equals itself (i.e., x.equals(x) should return true). Finally, we extended prompt chaining to generate advanced test oracles involving exception handling or boundary conditions. For example, after identifying that a method throws a NullPoint- erException when passed a null argument, the next step in the chain prompts the LLM to generate a test oracle confirming that the exception is indeed thrown under these conditions. This step-by-step approach ensures that even complex oracles involving multiple conditions or exceptional behaviors are accurately handled. This prompt chaining based approach reduces the model’s cognitive load by allowing it to focus on one sub-task at a time, ensuring each step is completed before proceeding. It also improves J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 9]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:9 1<context> 2 You are a software testing engineer. You will be provided with Java method description in <class type 3 here>, and your task is to find all features in method descriptions and generate test oracles for all 4 features one by one. You do not need to generate the whole test cases, just focus on test oracles. 5</context> 6 7<examples> 8 <description> 9 public boolean equals(Object obj) 10 Indicates whether some other object is equal to this one. The equals method implements an equival- 11 -ence relation on non-null object references: 12 It is reflexive: for any non-null reference value x, x.equals(x) should return true. 13 It is symmetric: for any non-null reference values x and y, x.equals(y) should return true if 14 and only if y.equals(x) returns true. 15 It is transitive: for any non-null reference values x, y, and z, if x.equals(y) returns true and 16 y.equals(z) returns true, then x.equals(z) should return true. 17 It is consistent: for any non-null reference values x and y, multiple invocations of x.equals(y) 18 consistently return true or consistently return false, provided no information used in equals 19 comparisons on the objects is modified. 20 For any non-null reference value x, x.equals(null) should return false. 21 An equivalence relation partitions the elements it operates on into equivalence classes; all the 22 members of an equivale-nce class are equal to each other. Members of an equivalence class are subst- 23 -itutable for each other, at least for some purposes. 24 </description> 25 <oracle> 26 For reflexive, the test oracle is: 27 boolean checkReflexive(Object x) { 28 return x != null ? x.equals(x) : true; 29 } 30 </oracle> 31</examples> 32 33<instruction> 34 Use the following step-by-step method to generate test oracles. Remember that you need to generate a 35 test oracle that returns a boolean value rather than an entire test case that can be executed. If 36 necessary, you can use the try catch structure in test oracles to catch exception. Test oracles may 37 require some input, you need to determine the input as well, most time the input should be same as class 38 type. No matter in which cases, still return a boolean to indicate whether the feature is satisfied. 39 Step 1 - Find all properties and behaviors requiring testing in the Java method description. 40 Step 2 - Generate test oracles for each identified feature one by one. 41 Step 3 - Generate test oracles for exception handling and boundary conditions. 42 This is the Java method description you need to deal with: 43 <method description here> 44</instruction> Fig. 9. Our prompt template input to LLMs accuracy, as the LLM is less likely to miss essential features or produce incomplete or incorrect test oracles when working in a sequential, structured manner. Additionally, prompt chaining maintains logical consistency throughout the process by building on the work completed in the previous step, ensuring the test oracles remain relevant to the features identified. An added advantage of prompt chaining is that it provides implicit feedback at each stage. After feature extraction, the LLM is better equipped to generate meaningful test oracles because it has already developed a clear understanding of the method’s properties. This feedback loop reinforces the model’s focus and helps prevent it from deviating from the task. 4 Test Oracle Evaluation In this section, we evaluate the effectiveness of using Large Language Models (LLMs) for test oracle generation from Javadocs. Our primary goal is to assess the practicality of the generated test oracles and their ability to accurately validate software behavior without relying on project source code. Specifically, we investigate the following research questions: J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 10]
111:10 Shan et al. Class #Methods #Oracles #Compilable Oracles #Correct Oracles java.lang.Object 11 38 36(94.7%) 38(100%) java.lang.String 77 158 151(96.7%) 155(98.1%) java.util.Set 20 54 53(98.2%) 53(98.2%) java.util.List 32 87 86(98.9%) 86(98.9%) java.util.Map 25 88 87(98.9%) 88(100%) Total 165 428 418(97.7%) 423(98.8%) Table 1. RQ1: Evaluation of Compilability RQ1: Compilability. How many LLMs-generated test oracles are able to compile successfully? RQ2: Compeleteness. How complete are the LLMs generated test oracles in covering the key properties described in Javadocs? RQ3: Exception Handling. How complete are the LLMs generated test oracles in handling exceptions described in Javadocs? RQ4: Quality. What is the quality of the LLMs generated test oracles? Are they clear and understandable? RQ5: Prompt Ablation. How effective are our prompt techniques? What will happen if we drop some of them? 4.1 Experimental Setup To answer the research questions, we designed an experimental setup based on the following key components: 4.1.1 Large Language Models. We choose the state-of-the-art GPT-4 model, setting the temperature to 0.7 to balance creativity and consistency in the generated outputs. This configuration has been found to perform well for code generation tasks[ 9], allowing for a diverse but controlled generation of test oracles. We access the GPT-4 model via ChatGPT website. 4.1.2 Javadoc Dataset. We choose the most widely used java libraries as our dataset. Specifically, we choose 2 classes from java.lang (Object , and String ) and 3 interfaces from java.util (Map,Set, andList). These classes represent a broad spectrum of functionality and are commonly used across Java projects. By using this standardized dataset, we ensure that the generated oracles have strong generalization capabilities, as they can be applied across various Java projects that depend on these Java libraries. 4.2 RQ1: Compilability The first research question in evaluating the generated test oracles is their compilability, as non- compiling oracles cannot be used in testing. We compile the test oracle methods in a standard Java environment and manually review errors for non-compiling oracles to assess if they still represent the intended properties in the Javadocs. Such oracles are considered correct. Due to similar overload functions in the JDK Javadocs, LLMs may generate oracles with identical names and parameters, causing compilation errors. In these cases, we only adjust function names, preserving parameters and oracle content. We measure the percentage of oracles that compile without further modification. Table 1 presents the results of RQ1, with columns for the number of methods, generated oracles, compilable oracles, and correct oracles. We observe that: (1) Over 97% of test oracles generated by LLMs compile successfully, and (2) 423 out of 428 oracles (98.8%) are correct. Correctness differs from compilability, as some test oracles require helper functions or hypothetical classes. For J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 11]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:11 1boolean checkCloneIndependency(Object x) throws CloneNotSupportedException { 2 Object original = x.clone(); 3 Object clone = original.clone(); 4 5 // Assuming clone modifies a mutable field as a simple example 6 if (original instanceof CloneExample) { // CloneExample is a hypothetical class with mutable fields 7 ((CloneExample) clone).setMutableField(new Object()); 8 } 9 10 return !clone.equals(original); 11} Fig. 10. An example of correct but not compilable oracle instance, the test oracle in Figure 10 targets the Object.clone() method by checking if the cloned object remains independent of the original via field changes. Since Object lacks mutable fields, LLMs use a hypothetical class with fields for simulation, making the oracle correct but unable to compile due to the undefined class. Such oracles, though non-compilable, are easy to fix and provide a useful guideline for test oracle development. Incorrect oracles fall into two categories: (1) Type errors, such as int actual = set.stream().count(); , where the return type of count() islong, and implicit casting from long tointis not allowed; (2) Syntax errors, such as actualResult == mainStr.indexOf(subSeq.toString()) != -1; , where the intended behavior is to compare actualResult with the result of mainStr.indexOf(subSeq.toString()) != -1 . However, due to incorrect execution order, it first compares actualResult with mainStr.indexOf(subSeq.toString()) . Adding parentheses corrects this to make a valid oracle. Answer to RQ1: Compilability of Test Oracles Overall, our approach effectively generates compilable test oracles (97%) without additional compilation tools. Even non-compilable or incorrect oracles offer valuable guidance for manual test oracle development, as they contain only minor, easily correctable errors. 4.3 RQ2: Completeness This research question assesses whether generated test oracles cover all intended properties of each method as defined in the JDK Javadocs, including all assertion and exception cases. Note that exception oracles and assertion oracles may overlap, as LLMs can generate effective oracles that simultaneously verify functionality and expected exceptions, as shown in Figure 13. Table 2 provides the overall results: #Documented indicates the properties identified in Javadocs, #Gener- ated represents the properties covered by generated oracles, and #Checked shows the properties successfully checked by test oracles. Precision is calculated as #Checked / #Generated , and Recall as #Generated / #Documented . We also examine how LLMs manage varying levels of complexity in generating assertion oracles. (1)Explicitly defined properties. LLMs can generate simple assertion oracles directly from property descriptions in Javadocs (e.g., Figure 4). In our experiments, we found no instances where LLMs ignored explicitly defined properties. Here, LLMs leverage their training data to utilize JDK helper functions without needing additional prompt context. (2)Implicit conditions. For instance, in the java.util.List interface, the isEmpty() method is described as “Returns true if this list contains no elements.” As shown in Figure 11, although there is no explicit description of the size() method, LLMs can infer that isEmpty() should yield the J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 12]
111:12 Shan et al. 1/** 2*Test oracle for checking if isEmpty correctly identifies an empty list. 3* 4*@param list the list to check 5*@return true if isEmpty returns true for an empty list and false for a 6* non-empty list, false otherwise 7*/ 8boolean checkIsEmpty(List<?> list) { 9 boolean empty = list.isEmpty(); 10 if (list.size() == 0) { 11 return empty; // Should be true if the list is indeed empty 12 } else { 13 return !empty; // Should be false if the list is not empty 14 } 15} 16 17/** 18*Test oracle for checking if remove(Object o ) correctly removes the first occurrence of the element. 19* 20*@param list the list to be checked 21*@param o the element to be removed 22*@return true if the element is correctly removed and method return true false otherwise 23*/ 24<E> boolean checkElementRemoval(List<E> list, E o) { 25 int originalSize = list.size(); 26 boolean contains = list.contains(o); 27 boolean result = list.remove(o); 28 boolean newSizeCorrect = list.size() == ( contains ? originalSize - 1 : originalSize); 29 return result == contains && newSizeCorrect; 30} Fig. 11. An example of implicit conditions1// Oracle to verify that the wait is indefinite without notify 2boolean checkIndefiniteWait(Object obj) { 3 Thread notifyingThread = new Thread(() -> { 4 try { 5 Thread.sleep(100); // Delay to ensure main thread is waiting 6 synchronized (obj) { 7 obj.notify(); 8 } 9 } catch (InterruptedException e) { 10 Thread.currentThread().interrupt() ; 11 } 12 }); 13 14 long startTime = System.currentTimeMillis (); 15 synchronized (obj) { 16 try { 17 notifyingThread.start(); 18 obj.wait(); // This should wait until it is notified above 19 long waitTime = System. currentTimeMillis() - startTime; 20 return waitTime >= 100 && waitTime < 200; // Check that wait was indeed waiting until notified 21 } catch (InterruptedException e) { 22 return false; // If interrupted, not handling as indefinite wait 23 } 24 } 25} Fig. 12. An example of complex conditions Class #Documented #Generated #Checked Precision(%) Recall(%) java.lang.Object 33 30 29 96.7 90.9 java.lang.String 162 151 144 95.4 93.2 java.util.Set 44 39 36 92.3 88.6 java.util.List 71 63 62 98.4 88.7 java.util.Map 80 69 67 97.1 86.3 Total 390 352 338 96.0 90.3 Table 2. RQ2: Evaluation of Assertion Oracles same boolean result as size() == 0 . Leveraging a large training dataset, LLMs can call functions not directly mentioned and create a structure that accurately represents the intended Javadocs behavior. Remarkably, LLMs can generate oracles for implicit details, like reproducibility. For example, although reproducibility is not specified for the getClass() method in Object Javadocs, LLMs produced an oracle that repeatedly calls this method, verifying consistent results. This J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 13]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:13 capacity to link properties beyond explicit documentation demonstrates the model’s nuanced understanding from training. (3)Complex conditions. LLMs can simulate complex scenarios, such as multithreading (e.g., Figure 12), and detect correlations between methods. For interdependent methods like equals() and hashcode() , we evaluate whether generated oracles maintain consistency across methods, covering both individual behaviors and their interrelationships. The generated oracles respect these dependencies, demonstrating the model’s capability to uphold consistency across related functions, as shown in Figure 8. We observe that LLMs can generate test oracles for most properties in JDK Javadocs with high precision. Their effectiveness is due to extensive training data, which improves their ability to identify Javadoc features. Even when Javadocs lack details for a fully correct oracle, LLMs can suggest sample oracles that guide property verification. For instance, in Map subclasses where order matters, LLMs can use ordered sets like TreeSet to demonstrate ordering. These oracles offer developers helpful guidance. Answer to RQ2: Completeness of Assertion Oracles The results demonstrate LLMs’ effectiveness in generating assertion oracles that are both syntactically correct and semantically accurate, producing broadly applicable oracles. Over- all, LLMs cover 90.3% of the properties identified in JDK Javadocs, with 96.0% of the generated oracles successfully checking these properties. 4.4 RQ3: Exception Handling Exception handling is essential in testing, and this research question examines whether the gener- ated test oracles correctly check for expected exceptions under specified conditions. Our exception oracles return a boolean value to indicate correct exception handling. Specifically, the test oracles employ a try-catch structure to catch and filter for expected exceptions, returning false for any unexpected ones. Table 3 shows the results of exception oracles, with columns defined as in Table 2. The left side of Figure 13 shows an example of an exception oracle, where we use a try-catch structure to check if the codePointAt(int index) method correctly handles out-of-bounds indices. If an IndexOutOfBoundsException is thrown, the oracle verifies the index validity and returns the result, ensuring it can detect abnormal behavior. On the right is an incomplete exception oracle: a ConcurrentModificationException should be thrown when a modification occurs during iteration or if multiple threads modify the Setconcurrently. This oracle simulates only the first case, failing to cover concurrent modifications, so it incompletely checks the exception. However, the oracle itself remains functionally correct. Notably, even without ConcurrentModificationException being explicitly mentioned in the Javadocs, LLMs infer the exception through the fail-fast property. Although LLMs may lack full context for certain exceptions, their ability to generate even incomplete oracles demonstrates impressive potential and highlights their strength in managing complex, context-limited scenarios. Answer to RQ3: Completeness of Exception Oracles The results demonstrate LLMs’ effectiveness in generating exception oracles that are both comprehensive and accurate. Overall, LLMs can generate oracles for 98.9% exceptions defined in JDK Javadocs, and 97.2% generated oracles can catch corresponding exceptions correctly. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 14]
111:14 Shan et al. 1/** 2*Test oracle to check if codePointAt method correctly handles index validation. 3* 4*@param str the string to test 5*@param index the index of the code point to retrieve 6*@return true if the method correctly throws IndexOutOfBoundsException when necessary, false otherwise 7*/ 8boolean checkIndexValidation(String str, int index) { 9 try { 10 int result = str.codePointAt(index); 11 return true; // No exception means index is within valid range. 12 } catch (IndexOutOfBoundsException e) { 13 return index < 0 || index >= str. length(); 14 } catch (Exception e) { 15 return false; // Handle unexpected exceptions. 16 } 17}1boolean checkConcurrentModificationException( Map map, BiConsumer<? super K, ? super V> action) { 2 try { 3 Iterator<Map.Entry<K, V>> it = map. entrySet().iterator(); 4 if (it.hasNext()) { 5 map.remove(it.next().getKey()); // Modify map during iteration 6 } 7 map.forEach(action); // Attempt to perform action after modification 8 return false; // If it reaches here, no ConcurrentModificationException was thrown 9 } catch (ConcurrentModificationException e ) { 10 return true; // Correct behavior, exception was thrown 11 } 12} Fig. 13. An Example of generated exception oracles Class #Documented #Generated #Checked Precision(%) Recall(%) java.lang.Object 11 11 11 100 100 java.lang.String 28 28 27 96.4 100 java.util.Set 24 23 22 95.7 95.8 java.util.List 62 62 61 98.4 100 java.util.Map 57 56 54 96.4 98.2 Total 182 180 175 97.2 98.9 Table 3. RQ3: Evaluation of Exception Oracles 4.5 RQ4: Oracle Quality and Understandability A critical aspect of high-quality test oracles is adherence to software engineering best practices, including meaningful variable naming, logical clarity, and coding standards, which enhance com- prehensibility and maintainability for developers. This research question evaluated the generated oracles focused on several key qualities: 4.5.1 Naming and Documentation. The structured prompts used in this study consistently guided LLMs to create functionally and semantically appropriate names for test oracles based on the prop- erties being verified. For instance, in checkElementRemoval (Figure 11), when complex properties with multiple sub-properties were identified, LLMs effectively used distinct, meaningful variable names for each sub-property, enhancing clarity and maintainability. 4.5.2 Comments and Javadocs. LLMs demonstrated the capability to integrate relevant comments within test oracles, clarifying the properties under test. Additionally, they generated suitable Javadoc comments for each test oracle, as seen in Figure 11, detailing tested properties along with expected inputs and outputs, thus ensuring clarity in understanding and usage. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 15]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:15 4.5.3 Code Clarity and Structure. The generated test oracles showed clear and concise code struc- ture, following unit testing best practices. LLMs effectively used ternary expressions to encapsulate property checks and structured exception handling to capture potential runtime anomalies. Where feasible, LLMs also combined handling of two exceptions within a single oracle, ensuring correctness while improving oracle effectiveness. Answer to RQ4: Oracle Quality and Understandability Overall, LLMs generate oracles with human-readable variable names and a clear code structure, making them easy to understand and maintain. 4.6 RQ5: Prompt Ablation In designing our prompt, we experimented with various techniques and ultimately selected the aforementioned four. This research question evaluates the impact of specific prompt engineering strategies on the effectiveness and reliability of test oracle generation. 4.6.1 Assistant Creation. Excluding the assistant creation phase from our prompts significantly reduced the effectiveness of the generated oracles. In some instances, LLMs shifted from producing executable code to merely outlining test strategies in a descriptive manner. 4.6.2 Few-shot Learning. Without few-shot learning, LLMs struggled to grasp task requirements, often defaulting to test cases with hard-coded values rather than adaptable test oracles. This limitation was especially evident in features needing generalization, such as those in the Map class. 4.6.3 Chain of Thought (CoT).. The absence of CoT in prompts led to “lazy” responses, where not all Javadocs features were identified or were incorrectly merged into a single test oracle. This compromised effectiveness, especially when distinguishing between exceptions like IllegalArgu- mentException andIllegalMonitorStateException . 4.6.4 Javadocs Partitioning. Javadocs partitioning proved essential for managing LLM output token limitations, ensuring that LLMs did not merely describe features but generated actionable test oracles for specific methods. Answer to RQ5: Effectiveness of Prompt Techniques All of our prompt techniques are essential for enabling the LLMs to understand the task and generate accurate test oracles. Removing any of these techniques causes the LLMs to suffer from ablation, leading it to generate descriptions instead of oracles or simply repeat information from the prompts. 5 Threats to Validity External Threats. The primary external threats to our approach stem from the inherent random- ness of LLMs. This randomness is unavoidable and also contributes to the diversity of generated program variants and inputs. Our approach has certain limitations tied to the use of LLMs, including potential obsolescence due to the rapid evolution of AI technology. Despite potential variations, the LLMs demonstrated consistent performance in oracle generation, with test oracle effectiveness unaffected by differing comment styles. To mitigate this threat and enhance the representativeness of our results, we conducted experiments on widely used JDK classes from java.lang andjava.util . J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 16]
111:16 Shan et al. Internal Threats. Our evaluation relies on manual verification of test oracle correctness, intro- ducing a subjective element into our analysis. To migrate this, two authors independently performed the checks. Any discrepancies in our inspection results were further checked to ensure the cor- rectness of the manual inspection. Besides, the “black box” nature of ChatGPT and similar LLMs can obscure the rationale behind model outputs, making it challenging to interpret or replicate results precisely. To mitigate this, we use a Chain of Thought prompting approach, enhancing transparency and making it easier to trace from Javadocs inputs to the generated oracles. 6 Related work Test Oracle Generation is a critical component of automated software testing, as test oracles define the intended behavior of a software system. Numerous studies have focused on advancing methods for test oracle generation. Overall, these methods can be divided into two categories: specification mining method and neural method[ 10,17]. Specification mining methods rely on a restricted format of documentation and a set of handcrafted rules to infer exceptions and assertions. @TComment[ 38] defines natural language patterns along with heuristics to infer nullness properties. More recently, MeMo[ 4] uses equivalence phrases in Javadocs comments to infer metamorphic relations (e.g., sum(x,y) == sum(y,x) ). However, if developers do not adhere to a standardized documentation format or omit documentation entirely, these methods typically fail to extract meaningful oracles from most real-world software components. Recently, neural models have also been used to generate test oracles. NLP-based methods, such as ATLAS[ 43] and AthenaTest[ 39] have been shown to outperform the specification mining methods. However, these methods rely solely on the implementation and the System Under Test (SUT), overlooking valuable information in documentation. This poses a problem: if the implementation is incorrect and intended behavior is inferred from the code itself, testing effectiveness is compromised. More recently TOGA[ 7] established a new state of the art by a large margin. TOGA is not a generative model; it constrains the search space by manually defining oracle templates. For each assertion, it uses a ranking model to select the most relevant template. This approach, however, depends heavily on predefined templates and struggles to cover all possible scenarios. Large Language Models (LLMs) show effectiveness for various software development tasks recently, including program synthesis[ 15] and test generation[ 45]. LLMs can generate program code[ 15][40][36] from natural language prompts, by associating documentation text with code from a large training set[ 42]. TiCoder[ 21] adopts LLMs to generate the whole test cases to formalize the user intent, which include prefix and oracle. TOGLL[13] fine-tuned LLMs to generate test oracles from project implementations and documentations, ensuring that implementations align with their own documentation. However, fine-tuning demands high-performance hardware, making it expensive for developers with limited resources. The main difference between these methods and ours is that we do not rely on predefined rules or Java code. Our approach requires only JDK Javadocs as input, making our oracles applicable across any Java project. Our goal is to generate general test oracles from the JDK, ensuring conformance to universal rules (e.g., JDK contracts) rather than project-specific constraints. To our knowledge, we are the first to leverage LLMs to address this problem. 7 Conclusions In this paper, we introduced a novel approach for test oracle generation from JDK Javadocs using LLMs. Our method focuses on generating test oracles for clients of widely used Java libraries, e.g., java.lang andjava.util packages. The key insight is that Javadocs that provide a rich source of information can enable automated generation of test oracles without any code implementation. We use large language models as an enabling technology to embody our insight into a framework J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 17]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:17 for test oracle automation, and propose a prompt template which is flexible and allows generation of test oracles for a variety of JDK Javadocs. Our approach offers a promising method to validate that clients must conform to the subtle and easy to overlook Java language contracts given in JDK Javadocs rather than only adhering to their own project requirements. The experimental results show that LLMs can generate highly applicable and comprehensive test oracles, capable of accurately expressing expected behaviors and intended exceptions in JDK. References [1]Sheeva Afshan, Phil McMinn, and Mark Stevenson. 2013. Evolving readable string test inputs using a natural language model to reduce human oracle cost. In 2013 IEEE Sixth International Conference on Software Testing, Verification and Validation . IEEE, 352–361. [2]Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. 2024. Make Your LLM Fully Utilize the Context. arXiv preprint arXiv:2404.16811 (2024). [3]Roberto Baldoni, Emilio Coppa, Daniele Cono D’elia, Camil Demetrescu, and Irene Finocchi. 2018. A survey of symbolic execution techniques. ACM Computing Surveys (CSUR) 51, 3 (2018), 1–39. [4]Arianna Blasi, Alessandra Gorla, Michael D Ernst, Mauro Pezzè, and Antonio Carzaniga. 2021. MeMo: Automatically identifying metamorphic relations in Javadoc comments for test automation. Journal of Systems and Software 181 (2021), 111041. [5]Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. 2023. When do you need chain-of-thought prompting for chatgpt? arXiv preprint arXiv:2304.03262 (2023). [6]Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. 2023. Active prompting with chain-of- thought for large language models. arXiv preprint arXiv:2302.12246 (2023). [7]Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and Shuvendu K. Lahiri. 2022. TOGA: a neural method for test oracle generation. In Proceedings of the 44th International Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE ’22). Association for Computing Machinery, New York, NY, USA, 2130–2141. https://doi.org/10.1145/3510003.3510141 [8]Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu. 2023. Towards next-generation intelligent assistants leveraging llm techniques. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5792–5793. [9]Madeline Endres, Sarah Fakhoury, Saikat Chakraborty, and Shuvendu K Lahiri. 2024. Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions? Proceedings of the ACM on Software Engineering 1, FSE (2024), 1889–1912. [10] Afonso Fontes and Gregory Gay. 2021. Using machine learning to generate test oracles: A systematic literature review. InProceedings of the 1st International Workshop on Test Oracles . 1–10. [11] Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, and Dipendra Misra. 2024. Aligning llm agents by learning latent preference from user edits. arXiv preprint arXiv:2404.15269 (2024). [12] Mark Harman, Sung Gon Kim, Kiran Lakhotia, Phil McMinn, and Shin Yoo. 2010. Optimizing for the number of tests generated in search based test data generation with an application to the oracle cost problem. In 2010 Third International Conference on Software Testing, Verification, and Validation Workshops . IEEE, 182–191. [13] Soneya Binta Hossain and Matthew Dwyer. 2024. TOGLL: Correct and Strong Test Oracle Generation with LLMs. arXiv preprint arXiv:2405.03786 (2024). [14] Soneya Binta Hossain, Antonio Filieri, Matthew B Dwyer, Sebastian Elbaum, and Willem Visser. 2023. Neural-based test oracle generation: A large-scale evaluation and lessons learned. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 120–132. [15] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering . 1219–1231. [16] Taewoong Jeon, Hyon Woo Seung, and Sungyoung Lee. 2002. Embedding built-in tests in hot spots of an object-oriented framework. ACM Sigplan Notices 37, 8 (2002), 25–34. [17] K Kamaraj, B Lanitha, S Karthic, PN Prakash, and R Mahaveerakannan. 2023. A Hybridized Artificial Neural Network for Automated Software Test Oracle. Computer Systems Science & Engineering 45, 2 (2023). [18] Sarfraz Khurshid, Corina S Păsăreanu, and Willem Visser. 2003. Generalized symbolic execution for model checking and testing. In International Conference on Tools and Algorithms for the Construction and Analysis of Systems . Springer, 553–568. [19] Dong Jae Kim and Tse-Hsun Chen. 2024. Exploring the Impact of Inheritance on Test Code Maintainability. In Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings . 382–383. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 18]
111:18 Shan et al. [20] James C King. 1976. Symbolic execution and program testing. Commun. ACM 19, 7 (1976), 385–394. [21] Shuvendu K Lahiri, Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat Chakraborty, Madanlal Musuvathi, Piali Choudhury, Curtis von Veh, Jeevana Priya Inala, Chenglong Wang, et al .2022. Interactive code generation via test-driven user-intent formalization. arXiv preprint arXiv:2208.05950 (2022). [22] Choonghwan Lee, Dongyun Jin, Patrick O’Neil Meredith, and Grigore Rosu. 2012. Towards categorizing and formalizing the JDK API. (2012). [23] Andreas Leitner, Ilinca Ciupa, Manuel Oriol, Bertrand Meyer, and Arno Fiva. 2007. Contract driven development= test driven development-writing test cases. In Proceedings of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering . 425–434. [24] Marco Lewis, Sadegh Soudjani, and Paolo Zuliani. 2023. Formal verification of quantum programs: Theory, tools, and challenges. ACM Transactions on Quantum Computing 5, 1 (2023), 1–35. [25] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering and Methodology (2023). [26] Zongjie Li, Chaozheng Wang, Zhibo Liu, Haoxuan Wang, Dong Chen, Shuai Wang, and Cuiyun Gao. 2023. Cctest: Testing and repairing code completion systems. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 1238–1250. [27] Kaibo Liu, Yiyang Liu, Zhenpeng Chen, Jie M Zhang, Yudong Han, Yun Ma, Ge Li, and Gang Huang. 2024. LLM-Powered Test Case Generation for Detecting Tricky Bugs. arXiv preprint arXiv:2404.10304 (2024). [28] Shaoying Liu and Shin Nakajima. 2020. Automatic test case and test oracle generation based on functional scenarios in formal specifications for conformance testing. IEEE Transactions on Software Engineering 48, 2 (2020), 691–712. [29] Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, and Bingzhe Wu. 2023. Fairness-guided few-shot prompting for large language models. Advances in Neural Information Processing Systems 36 (2023), 43136–43155. [30] Chuize Meng, Shan Jiang, Mengning Wu, Xuan Xiao, Dan Tao, and Ruipeng Gao. 2022. BatMapper-Plus: Smartphone- Based Multi-level Indoor Floor Plan Construction via Acoustic Ranging and Inertial Sensing. In International Conference on Wireless Algorithms, Systems, and Applications . Springer, 155–167. [31] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1–13. [32] Shuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang. 2023. LLM is Like a Box of Chocolates: the Non- determinism of ChatGPT in Code Generation. arXiv preprint arXiv:2308.02828 (2023). [33] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2024. Are emergent abilities of large language models a mirage? Advances in Neural Information Processing Systems 36 (2024). [34] Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C Lipton, and J Zico Kolter. 2024. Rethinking llm memorization through the lens of adversarial compression. arXiv preprint arXiv:2404.15146 (2024). [35] Jiho Shin, Hadi Hemmati, Moshi Wei, and Song Wang. 2024. Assessing evaluation metrics for neural test oracle generation. IEEE Transactions on Software Engineering (2024). [36] Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md Rafiqul Islam Rabin, Susmit Jha, Prem Devanbu, and Toufique Ahmed. 2024. Quality and Trust in LLM-generated Code. arXiv preprint arXiv:2402.02047 (2024). [37] Gang Tan and Jason Croft. 2008. An Empirical Security Study of the Native Code in the JDK.. In Usenix Security Symposium . 365–378. [38] Shin Hwei Tan, Darko Marinov, Lin Tan, and Gary T Leavens. 2012. @ tcomment: Testing javadoc comments to detect comment-code inconsistencies. In 2012 IEEE Fifth International Conference on Software Testing, Verification and Validation . IEEE, 260–269. [39] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. 2020. Unit test case generation with transformers and focal context. arXiv preprint arXiv:2009.05617 (2020). [40] Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. 2024. Improving llm code generation with grammar augmentation. arXiv preprint arXiv:2403.01632 (2024). [41] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with large language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering (2024). [42] Xinyi Wang, Wanrong Zhu, and William Yang Wang. 2023. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916 (2023), 3. [43] Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys Poshyvanyk. 2020. On learning meaningful assert statements for unit test cases. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering . 1398–1409. [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al .2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 24824–24837. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

[Página 19]
Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs 111:19 [45] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. 2024. Fuzz4all: Univer- sal fuzzing with large language models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1–13. [46] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. 2023. Universal fuzzing via large language models. arXiv preprint arXiv:2308.04748 (2023). [47] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. 2022. Prompting decision transformer for few-shot policy generalization. In international conference on machine learning . PMLR, 24631–24645. [48] Jieyu Zhang, Ranjay Krishna, Ahmed H Awadallah, and Chi Wang. 2023. Ecoassistant: Using llm assistant more affordably and accurately. arXiv preprint arXiv:2310.03046 (2023). [49] Xiaogang Zhu, Sheng Wen, Seyit Camtepe, and Yang Xiang. 2022. Fuzzing: a survey for roadmap. ACM Computing Surveys (CSUR) 54, 11s (2022), 1–36. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.