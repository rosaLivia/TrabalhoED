
[Página 1]
Secret Breach Prevention in Software Issue Reports ZAHIN WAHAB∗,Bangladesh University of Engineering and Technology, Bangladesh SADIF AHMED∗,Bangladesh University of Engineering and Technology, Bangladesh MD NAFIU RAHMAN∗,Bangladesh University of Engineering and Technology, Bangladesh RIFAT SHAHRIYAR, Bangladesh University of Engineering and Technology, Bangladesh GIAS UDDIN, York University, Canada In the digital age, the exposure of sensitive information poses a significant threat to security. Leveraging the ubiquitous nature of code-sharing platforms like GitHub and BitBucket, developers often accidentally disclose credentials and API keys, granting unauthorized access to critical systems. Despite the availability of tools for detecting such breaches in source code, detecting secret breaches in software issue reports remains largely unexplored. This paper presents a novel technique for secret breach detection in software issue reports using a combination of language models and state-of-the-art regular expressions. We highlight the challenges posed by noise, such as log files, URLs, commit IDs, stack traces, and dummy passwords, which complicate the detection process. By employing relevant pre-processing techniques and leveraging the capabilities of advanced language models, we aim to mitigate potential breaches effectively. Drawing insights from existing research on secret detection tools and methodologies, we propose an approach combining the strengths of state-of-the-art regexes with the contextual understanding of language models. Our method aims to reduce false positives and improve the accuracy of secret breach detection in software issue reports. We have curated a benchmark dataset of 25000 instances with only 437 true positives. Although the data is highly skewed, our model performs well with a 0.6347 F1-score, whereas state-of-the-art regular expression hardly manages to get a 0.0341 F1-Score with a poor precision score. We have also developed a secret breach mitigator tool for GitHub, which will warn the user if there is any secret in the posted issue report. By addressing this critical gap in contemporary research, our work aims at enhancing the overall security posture of software development practices. CCS Concepts: •Security and privacy →Vulnerability management ;Software security engineering ;•Computing method- ologies→Language Models . Additional Key Words and Phrases: Secret breach detection, Software issue reports, API key leaks, Sensitive information exposure, Regular expressions, Language models 1 Introduction Secrets are one of the most critical components in software development to safeguard systems and are, in essence, a form of digital authentication system that secures access to sensitive information and services. Secrets come in many forms, from API keys and OAuth tokens to private encryption keys, especially the RSA keys, TLS and SSL certificates, ∗First three authors contributed equally to this research. Authors’ Contact Information: Zahin Wahab, zahinwahab@gmail.com, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Sadif Ahmed, ahmedsadif67@gmail.com, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Md Nafiu Rahman, nafiu.rahman@ gmail.com, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Rifat Shahriyar, rifat@cse.buet.ac.bd, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Gias Uddin, guddin@yorku.ca, York University, Toronto, Ontario, Canada. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ©2024 Copyright held by the owner/author(s). 1arXiv:2410.23657v1 [cs.SE] 31 Oct 2024

[Página 2]
2 Wahab et al. and credentials(i.e., usernames and passwords). However, secrets can quickly spread across modern and distributed applications relying on many services, thus easily increasing exposure risk [22]. Source code repositories like GitHub and Bitbucket have become common places for developers to share and merge their codes. However, while sharing codes, they can also mistakenly reveal secrets. According to GitGuardian’s yearly state of secrets sprawl report [ 13], the number of secret breaches has increased by leaps and bounds since 2020. In 2022, 10M new hard-coded secrets were detected, which is a 67% increase from 2021, and scanned commits increased from 860M in 2021 to 1.027B in 2022. Several news articles have reported that vicious users stole unsecured API keys embedded in source files from GitHub to get free web/software services [ 18]. Sophos report found out that compromised credentials comprised 50% of the root cause of all attacks recorded in the first half of 2023 [ 34]. Although web service providers like Amazon and Google have scraped code repositories and invoked leaked API keys, version control system (VCS) repositories do little to control the problem. Apart from source codes, configuration files, test code, documentation, package management files, scripts, and logs can also leak potential secrets [22]. Several tools [ 20] are available to find vulnerabilities in source code during commits, including TruffleHog [ 35], Gitrob [ 26], ggshield [ 12], and Gitleaks [ 16] key methods of secret detection are pattern-based matching and regex. Among them, Gitleaks and ggshield use predefined regex to find hard-coded secrets while TruffleHog finds high-entropy strings [ 32]. These technologies have high false positive rates and, hence, are effective only to a certain degree, especially in noisy environments. Meli et al. [ 25] addressed this by performing an in-depth study of the GitHub repositories using a pattern matching-entropy-based filtering with regex. They identified secret leaks in more than 100000 repositories with 99.29% accuracy. Adding to this, Sinha et al. [ 33] proposed advanced approaches involving keyword searches and program slicing, which increased both recall and precision for detecting API keys or similar secrets. Saha et al. [ 29] addressed the problem of false positives by training models on datasets gathered through the Git REST API [ 15]. Their Soft-Voting Classifier performed very well with an F1-score of 86.7%. Feng et al. [ 10] improved pattern recognition further by reducing false positives using deep neural networks over textual passwords. Sinha et al. [ 33] proposed several solutions to mitigate secret breaches in source by using tools like git-crypt [1]. Basak et al. [ 3] suggested 24 practices for secret management. While there is some progress being made in source code analysis, secret detection in areas like issue reports remains unexplored. Noise such as log files, URLs, commit IDs, stack traces, file paths, shell commands, UUIDs, GUIDs, and dummy passwords typically interfere with the process of secret detection in issue reports. In such cases, methods that leverage regex tend to throw up a high number of false positives, hence motivating the need for advanced techniques. Our research focuses on the automatic detection of secret breaches in issue reports. We propose a novel approach that employs pre-trained language models and regex detection to filter out noise and accurately identify sensitive information. Also, because there are no benchmark datasets for this problem, we created a specialized dataset of issue reports containing secret breaches to facilitate future studies in this area. We have curated a benchmark dataset of 25000 instances with only 437 true positives. Although the data is highly skewed, our model performs well with a 0.6347 F1-score, whereas state-of-the-art regular expression hardly manages to get a 0.0341 F1-Score with a poor precision score. We have also developed a secret breach mitigator tool for GitHub, which will warn the user if there is any secret in the posted issue report. In summary, we make the following contributions: (1) We curate a new benchmark dataset of issue reports with secret breaches

[Página 3]
Secret Breach Prevention in Software Issue Reports 3 (2)We assess existing tools based on regular expressions (regex) and enhance their accuracy through pre-processing and machine-learning techniques (3)We implement and compare various pre-trained language models, such as BERT and RoBERTa, to refine the process of filtering out false positives identified by regex-based detection (4)We create a browser extension (SBMBot) that assists in preventing secret breaches by giving real-time feedback when creating GitHub issues to mitigate secret breaches. (5)We release our models, implementation of browser extension, and data to help replicate the experiments in this paper https://github.com/100100guy/Secret-Breach-Prevention-In-Software-Issue-Reports. The rest of the paper is organized as follows: Section 2 discusses the motivation behind this research and presents an initial survey on secret breaches within issue reports to understand its severity and influencing factors by exploring some research questions. Section 3 provides a detailed introduction to the benchmark dataset that we curated. Section 4 shows preprocessing techniques to remove noises from issue reports. Section 5 evaluates the performance of state-of-the-art regexes and machine learning techniques, selects the best model based on various metrics by hyper-parameter tuning, discusses our model’s pipeline for secret detection and assesses its performance in real-world repositories. Section 6 showcases our browser extension that uses the trained model, SBMBOT, and evaluates its effectiveness based on user feedback from a follow-up survey. Section 7 outlines the threats to validity, followed by related works in Section 8, and finally, Section 9 concludes the paper with future directions for research. 2 Understanding secret breaches in issue reports GitHub issue reports serve as a feature that allows users to monitor issues, tasks, enhancements, and other activities in the project repository [ 14]. Users may post, comment on, and close issues, allowing for better collaboration in software development. Each issue usually has a title, a description, and occasionally milestones, assignments, etc., to assist in organizing the work. Often, users accidentally expose sensitive information, such as API keys, in GitHub issues. Figure 1 shows an example of such a scenario. In this example, the API key ( 12345-abcde-67890-fghij ) is sensitive information that should not be posted publicly. Disclosing such secrets in issue reports exposes them to potential exploitation, resulting in security flaws, including unauthorized access to services and data. Understanding the existence, severity, and consequence of secret breaches in issue reports is important to develop efficient techniques for mitigation and detection. To learn more, we surveyed 30 people to get their opinions on secret exposures in issue reports and their experiences with them. Based on the findings from this survey, we answer the following research questions: (1) Research Question (RQ) 1a) How likely are you to encounter a secret breach in the issue reports of GitHub? 1b) What do you think of the severity of secret sharing in issue reports? 1c) What scenarios could influence your sharing of secrets in issue reports? Answering RQ-1a: How likely you are to encounter a secret breach in the issue reports of GitHub? We asked the above research question to determine the likelihood of coming across a secret breach in issue reports on GitHub. Respondents were asked to rate their likelihood on a 5-point scale, where 1 indicated "least likely," and 5 indicated "most likely." A total of 30 participants responded to this question. The distribution of the responses is given in Figure 2.

[Página 4]
4 Wahab et al. Fig. 1. Example of secret breach in GitHub issue report Fig. 2. Likelihood of encountering a secret breach in issue reports The data indicates that the majority of participants (50%) ranked a 3, suggesting a moderate risk of encountering secret breaches. While a minority of participants assessed the possibility as very high (6.7%), a considerable portion (43.3%) inclined toward some likelihood (ratings of 3 or higher). This implies that many users are concerned about the possibility of secret breaches in issue reporting. For RQ-1b and RQ-1c, which were open-ended questions, we applied an open coding approach following the methodology outlined in [ 27]. This approach was used for qualitative analysis similar to the method used in [ 17] for systematically categorizing the textual responses based on recurring themes. Open coding involves labeling categories within the textual content, focusing on the properties and dimensions of the entities discussed in the responses. To structure this process, we used the card sorting method described in [ 11], where textual content is divided into discrete units or "cards," each representing a conceptually coherent quote. For instance, consider the following response: "Highly severe, but depends on situation." This sentence can be broken into two distinct categories of quotes: "Highly severe," which indicates a theme of high severity, and "depends on situation," which suggests that the severity depends on context. Two authors collaboratively coded the responses to extract relevant categories from each question. We discovered three

[Página 5]
Secret Breach Prevention in Software Issue Reports 5 category types for RQ-1b and four for RQ-1c through our open coding. Tables 1 and 2 show the number of quotes and corresponding survey respondents for each category. The categories observed during the open coding process are summarized in the tables, with #C representing the number of codes and #R representing the number of respondents for each category. R 𝑗denotes the response of the j-th participant. Table 1. Categories and Respondents for RQ-1b Category #C #R Highly severe 19 19 Moderately/Contextually severe 6 6 Potential Data Breach and Unauthorized Access 7 4 Total 32 29 Table 2. Categories and Respondents for RQ-1c Category #C #R Urgency/Time Pressure 7 7 Accidental Sharing Due to Lack of Attention 3 3 Lack of Awareness/Knowledge 15 15 Collaboration/Team Sharing 3 3 Total 28 28 Answering RQ-1b: What do you think of the severity of secret sharing in issue reports? Fig. 3. Severities associated with secret breaches in GitHub issue reports Figure 3 summarizes the three key themes identified in the survey responses of 25 people related to RQ2. These themes represent various severities associated with secret breaches in GitHub issue reports. (1)Highly severe (76%): Most respondents characterized secret breaches as highly severe, with potentially serious consequences for the organizations affected. One respondent (R 6) commented, "It’s really concerning. Can cause severe security breaches. " (2)Moderately/Contextually severe (24%): A portion of the participants stated that the impact of the secret breaches in issue reports is moderate or depends on the situation. As R 5stated, "depending on the nature of the secrets and the context in which they are shared."

[Página 6]
6 Wahab et al. (3)Potential Data Breach and Unauthorized Access (16%): A smaller portion of the respondents indicated that secret sharing in issue reports may lead to potential data breach or unauthorized access in a system. R 22mentioned, "it can lead to security breaches, unauthorized access, and data exploitation." Answering RQ-1c: What scenarios could influence your sharing of secrets in issue reports? Fig. 4. Factors influencing sharing of secrets in issue reports Figure 4 summarizes the four key themes identified in the survey responses of 24 people related to RQ3. These themes reflect different factors contributing to accidental or unintentional secret sharing on GitHub issue reports. (1)Lack of Awareness/Knowledge (62.5%): The theme that was brought up the most was the participants’ lack of understanding or awareness of security procedures and the consequences associated with them. For example, one respondent (R 10) noted, "maybe not knowing what could be potential secrets?" (2)Urgency/Time Pressure (29.167%): A few participants identified the stress of deadlines or the urgency to resolve issues quickly as contributing factors. As another respondent (R 1) remarked, "It could a be a time crunch/emergency situation/deadline that required hasty solution for a bug/problem" (3)Accidental Sharing Due to Lack of Attention (12.5%): A few participants mentioned that when handling sensitive data, it’s easy to overlook details and unintentionally reveal secrets. For example, R 9mentioned, "Copy-pasting error reports from the IDE." (4)Collaboration/Team Sharing (12.5%): A small portion of participants pointed to collaborative environments as a cause for accidental secret breaches, as team members often share sensitive information among themselves without realizing its consequences. R 11explained, "If it is an emergency and the repository is private and shared with trusted individuals only." In conclusion, the survey’s findings show that participants are concerned about the possibility of secret breaches in GitHub issue reports. The majority of respondents emphasize the severe consequences these breaches can have and rate their severity as high. Urgency, team collaboration, and a lack of awareness or knowledge about security risks are major contributors to inadvertent secret sharing in most cases. 3 A Secret Breach Dataset for Issue Reports To our knowledge, research has yet to deal with secret breach detection in software issue reports. So, we curate a benchmark dataset to compare different methods.

[Página 7]
Secret Breach Prevention in Software Issue Reports 7 Step 1: Selecting a primary dataset .For curating the benchmark dataset, we used the NLBSE’23 issue report dataset [ 19] as our primary dataset. This dataset is the benchmark for issue report classification competition which involves constructing and evaluating a multi-class classification model to categorize issue reports based on the type of information they convey. It has about more than 1.4 million labeled issue reports (as bugs, enhancements, questions, and documentation) extracted from real open-source projects. The attributes of this dataset are Label, ID, Title, Body, and Author Association. There are different types of labels: Bug, Feature, Question, and Documentation. However, we only deal with bug-labelled reports, and there are around 750𝐾of them. Finally, we use a raw unstructured issue report body for our benchmark dataset. Apply 761 regular expressions used to curate SecretBench dataset on 750K bug reports Randomly sample 1% candidate strings and label them manually Repeat the process until 500 TPs are reached External annotator labels 200 TPs and 200 FPs Calculate inter-rater agreementIf the agreement has a kappa of more than 0.7, then labeling is substantial and personal bias is not introduced.Produce the final benchmark by resolving the disagreements between annotators 1 and 2 Fig. 5. Steps for curating benchmark dataset Step 2: Enumerating regular expressions .TruffleHog [ 35], a widely used open-source secret-scanning tool, comprises a package of secret detectors. Basak et al. [ 5] extracted 751 regular expressions (regex) patterns from the source code of the detector package and incorporated them into their pattern set. Additionally, they also included 10 regex patterns from Meli et al. [ 25] to detect secrets in GitHub repositories that are not covered by the TruffleHog detector package [ 31]. In total, their pattern set consists of 761 regex patterns, which can be accessed online [ 4]. This pattern set has been used to curate SecretBench, a benchmark dataset for secret detection in source codes. Step 3: Applying regular expressions .After applying 761 regular expressions on 750,000 raw issue reports, 653,024 candidate secrets were captured. Step 4: Labeling the candidate secrets .Rater 1 labeled randomly sampled 1% candidate strings iteratively until 400-500 candidate secrets are reached. Finally, we have 437 secrets in 25,000 labeled data.

[Página 8]
8 Wahab et al. Step 5: Calculating inter-rater agreement .To calculate inter-rater agreement, 200 true positives and 200 false positive data are given to rater 2 for labeling. They labeled these 400 instances independently. The result is given in Table 3. Finally, Cohen’s kappa coefficient [8] is calculated. Table 3. Confusion matrix for two annotators Rater 1 positive Rater 1 negative Rater 2 positive 184 13 Rater 2 negative 16 187 Step 6: Checking substantiality of the labeling .Kappa (𝜅) values are commonly interpreted as follows: values ≤ 0 as indicating ‘no agreement’ and 0.01 – 0.20 as ‘none to slight’, 0.21 – 0.40 as ‘fair’, 0.41 – 0.60 as ‘moderate’, 0.61–0.80 as ‘substantial’, and 0.81–1.00 as ‘almost perfect agreement’. Since Cohen’s kappa is found to be 0.855 in step 5, it can be safely concluded that this value represents excellent agreement beyond chance, and personal bias is not introduced. Step 7: Producing final benchmark .Finally, we resolved the disagreements between the raters and produced the benchmark dataset. The full workflow for curating the benchmark dataset is shown in Figure 5. 4 Preprocessing Issue Report Secret breach detection is a widely addressed problem with respect to source codes. However, dealing with secrets in issue reports is not as straightforward as it seems, given the unstructured nature of the content. Issue reports are prone to noise like URLs, IDs, file paths, directories, stack traces, and other system-specific information; all of these might contain high-entropy strings that can confuse or mislead any detection algorithm since they just look like actual secrets. Moreover, some valid identifiers or tokens may contain patterns resembling sensitive information, and this increases the false positives problem. To reduce this risk, we crafted the regular expressions that filtered such noise from the issue report body as shown in Table 4. 5 Automatic Detection of Secret Breaches with Pre-trained Language Models This section outlines our methodology for using pre-trained language models to automatically identify security breaches in issue reports. Context Windows. Issue reports can be long (160 words on average), so it’s important that models accurately represent the context of any secrets that have been flagged. The number of tokens (words or characters) considered by a language model for a prediction task is referred to as a context window [ 21]. Context windows extend to the left and right of a token in models such as BERT and RoBERTa, which allow bidirectional dependencies to be captured. Although larger context windows incur higher computational costs, they improve the ability to detect long-range dependencies. To find the best range for our task, we experimented with context windows of different sizes: 75, 100, 125, 150, 200, and 250 characters. Evaluation Metrics. We used the following metrics to evaluate our models:

[Página 9]
Secret Breach Prevention in Software Issue Reports 9 Table 4. Regex patterns for text cleaning Description Regex Pattern Quotation Marks Removal [\’"\\|] Directory List Removal drwx[-\s]*\d+\s+ w+\s+ w+\s+ d+\s+w+\s+ d+\s+[0-9a-fA-F-]+.* Shell Code Removal ```shell([^ `]+)``` Shell Code Removal (another) ```shell \s*"([^"]*)" \s* ``` Remove Saved Game <details><summary> Saved game </summary>\n \n ```(.*?) ``` Packages Removal (\w+\.)+\w+ Java Exception Stack Trace Removal at\s[\w.$]+\.([\w]+)\(([^:]+:\d+)\) URL Removal (Pattern 1) https?://[^\s#]+#[A-Za-z0-9\-\=\+]+ URL Removal (Pattern 2) http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\).]|(?:%[0-9a- fA-F][0-9a-fA-F]))+ Commit ID Removal commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]40)\b File Path Removal (Pattern 1) /[\w/. :-]+ File Path Removal (Pattern 2) (/[^/\s]+)+ SHA256 Removal sha256\s*[:]?[=]?\s*[a-fA-F0-9]64 SHA1 Removal git-tree-sha1\s*=\s*[a-fA-F0-9]+ Build ID Removal build-id\s*[:]?[=]?\s*([a-fA-F0-9]+) UUID Removal ([0-9a-fA-F-]+\s*,\s*[0-9a-fA-F-]+\s*,\s*[0-9a-fA-F-]+) GUID Removal GUIDs:\s+([0-9a-fA-F-]+\s+[0-9a-fA-F-]+\s+[0-9a-fA-F-]+) Event ID Removal <([^>]+)> Different IDs Removal (?:UUID|GUID|version|id)[\\=: ¨\’\s]*\b[a-fA-F0-9]{8}-[a-fA-F0-9] {4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\b Hex Removal (?:data|address|id)[\\=:"\’\s]*\b0x[0-9a-fA-F]+\b Screenshots Removal Screenshot_(\d{4}[_]\d{2}[_]\d{2}[_]\d{2}[_]\d{2}) Precision: The precision measures how many positive observations were correctly predicted out of all the positive observations that were predicted. It is calculated as: Precision =True Positives True Positives+False Positives(1) Recall: The ratio of correctly predicted positive observations to all observations in the actual positive class is known as recall (sensitivity). It is computed as: Recall =True Positives True Positives+False Negatives(2) We can use precision and recall in disproportionate cases when dataset is imbalanced [30]. F1 Score: The𝐹1score is the harmonic mean of precision and recall. It is computed as: 𝐹1=2·Precision·Recall Precision+Recall(3) Minority Class F1 Score: This is specifically the 𝐹1score calculated for the minority class in an imbalanced dataset. It helps assess performance in the smaller class. The formula is the same as the general 𝐹1score, but precision and recall are calculated for the minority class: 𝐹Minority 1=2·Precision Minority·Recall Minority Precision Minority+Recall Minority(4)

[Página 10]
10 Wahab et al. F-beta Score: The𝐹𝛽score is a generalized form of the 𝐹1score, where the 𝛽parameter allows control over the trade-off between precision and recall. It is defined as: 𝐹𝛽=(1+𝛽2)·Precision·Recall (𝛽2·Precision)+Recall(5) Where𝛽>1emphasizes recall, and 𝛽<1emphasizes precision. .With these concepts and metrics in place, we proceeded in two steps: •Model Selection: In order to determine which language model performed the best for our task, we first experimented with several of them while adjusting the size of the context window. •Hyperparameter tuning: After the optimal model was chosen, we adjusted its context window size, learning rate, and optimizer to further improve performance. 5.1 Experimentation to select the best model We experimented with BERT-based language models like BERT ( bert-base-uncased ) [9], RoBERTa ( RoBERTa-base ) [23], SpanBERT ( SpanBERT-base-cased ) [23], and ELECTRA ( ELECTRA-base-discriminator ) [7]. These models utilize Transformer [ 36], an attention mechanism [ 24] that learns contextual relationships among words (or sub-words) within a text. For experimentation with the models, we used learning rate = 1𝑒−5and AdamW optimizer. Using Context Window of 100 Characters .With context window of 100 characters, we can clearly see from Figure 6, RoBERTa outperforms all the models on all metrics. And here, BERT performs the worst. Fig. 6. Evaluation of language models with context window = 100, learning rate = 1𝑒−5and AdamW optimizer Using Context Window of 125 Characters .Next, we experimented with a context window of 125 characters. We can clearly observe from Figure 7, RoBERTa outperforms all the models on all metrics. On the other hand, SpanBERT performs the worst among all models.

[Página 11]
Secret Breach Prevention in Software Issue Reports 11 Fig. 7. Evaluation of language models with context window = 125, learning rate = 1𝑒−5and AdamW optimizer Using Context Window of 200 Characters .Using a context window of 200 characters, referring to Figure 8, ELECTRA slightly outperforms RoBERTa and is the best among all models. SpanBERT still remains the worst-performing model. Fig. 8. Evaluation of language models with context window = 200, learning rate = 1𝑒−5and AdamW optimizer Using Context Window of 250 Characters .Finally, we assessed the models’ performance with a context window of 250 characters. We can clearly observe from Figure 9, RoBERTa outperforms all the models with respect to minority 𝐹1whereas ELECTRA has the best 𝐹1and𝐹𝛽score. On the contrary, BERT performs the worst among all models.

[Página 12]
12 Wahab et al. Fig. 9. Evaluation of language models with context window = 250, learning rate = 1𝑒−5and AdamW optimizer We can see that RoBERTa is the best-performing model in context windows 100 and 125, whereas it is a strong contender in the other two context windows (200 and 250) against ELECTRA. Notably, RoBERTa achieves the highest F1-score of 0.634 when employing a context window of 125 characters. So, we will now move forward with RoBERTa and tune other hyperparameters for conducting a comprehensive experimental analysis. 5.2 Hyper-parameter tuning of RoBERTa For our final experimentation, we used the pre-trained model RoBERTa ( RoBERTa-base ). We tuned hyperparameters like optimizer, learning rate, and context window. Varying Learning Rate .We varied learning rate from 1𝑒−1to1𝑒−9keeping optimizer and context window fixed. We used AdamW as the optimizer and 100 as the context window. As shown in Figure 10, we have observed 0 F1-score for learning rates 1𝑒−1,1𝑒−2,1𝑒−3,1𝑒−4,1𝑒−8, and 1𝑒−9which means 0 precision and 0 recall. That is, these models did not predict any true positives. Learning rate 1𝑒−5shows highest 𝐹1, minority-𝐹1and𝐹𝛽score. Fig. 10. learning rate vs 𝐹1, minority𝐹1,𝐹𝛽context window=100, AdamW optimizer Varying Optimizer .For optimizers, we experimented with AdamW, SGD, AdaGrad, and RMSProp [ 28]. We can clearly see from Figure 11, SGD performs worst with 0 true positives detected. AdamW and RMSProp are strong contenders here. AdamW has a 𝐹1-score of 0.589 which is slightly greater than that of RMSProp (0.559). However,

[Página 13]
Secret Breach Prevention in Software Issue Reports 13 RMSProp performs better than AdamW with respect to the minority 𝐹1score and𝐹𝛽score. Detailed results are shown in Table 5. Table 5. Optimizer Performance Optimizer 𝐹1Score Minority𝐹1Score𝐹beta AdamW 0.589 0.713 0.577 SGD 0 0 0 AdaGrad 0.425 0.53 0.401 RMSProp 0.559 0.788 0.586 Fig. 11. Optimizer vs 𝐹1, minority𝐹1,𝐹𝛽context window=100, lr= 1𝑒−5 Varying Length of Context Window .As we varied the length of context windows (shown in Figure 12) around the captured candidate secret, the highest 𝐹1, minority𝐹1, and𝐹𝛽were achieved with a context window of 125. Generally, as the context window decreases or increases from the optimal size (125), the 𝐹1Score tends to decrease. 𝐹1-score of the model falls below 0.4 when using the entire issue report body as the feature. However, introducing context windows of variable length ensures at least a 𝐹1score of 0.55. Overall, the context window size significantly impacts the performance of the model, with an optimal size around 125 yielding the best results. Fig. 12. Context Window vs 𝐹1, minority𝐹1,𝐹𝛽with AdamW optimizer and lr= 1𝑒−5

[Página 14]
14 Wahab et al. The training time is slightly lower when using a context window compared to when not using a context window (shown in Figure 15). However, the testing time is also slightly lower when using a context window (shown in Figure 14). Overall, there isn’t a significant difference in training and testing times between using and not using a context window. However, using a context window provides better performance, as indicated by the higher 𝐹1Score and Minority 𝐹1 Score in the previous paragraph. Fig. 13. Sensitivity analysis of evaluation metrics with respect to the size of context window Fig. 14. Comparing test time with and without context window (AdamW optimizer and lr= 1𝑒−5) Fig. 15. Comparing training time with and without context window (AdamW optimizer and lr= 1𝑒−5)

[Página 15]
Secret Breach Prevention in Software Issue Reports 15 5.3 Effectiveness of the pre-trained language model to detect secret breaches To evaluate our language model-based approach to detecting secrets, we answer the following research questions: (2) Research Question (RQ) 2a)How do the state-of-the-art (SOTA) regular expressions (regexes) perform to detect secret breaches in issue reports? 2b) Can preprocessing and cleaning of the issue reports improve the performance of the SOTA regexes? 2c) Can machine learning-based techniques outperform the SOTA regexes? Answering RQ-2a: How do the state-of-the-art (SOTA) regexes perform to detect secret breaches in issue reports? The confusion matrix of true labels vs. predicted labels of SOTA regexes run on the raw dataset is given in Figure 16. The evaluation metrics are analyzed as follows: Precision: Here, precision is 0.0174, indicating that only a small proportion of predicted positives were actually true positives. Recall (Sensitivity): In this case, recall is 0.8554, indicating a relatively high proportion of actual positives were correctly identified. F1 Score: The𝐹1score here is 0.0341, which is low, indicating the overall effectiveness of the model in terms of precision and recall is limited. Minority Class 𝐹1Score: The minority class 𝐹1score here is 0.9221, indicating that the model performs significantly better in correctly identifying instances of the minority class compared to the overall 𝐹1score. In summary, the model based on SOTA regexes performs poorly in terms of precision and overall 𝐹1score, indicating a high rate of false positives. Fig. 16. Confusion Matrix of true labels vs. predicted labels of SOTA regexes run on the raw dataset Answering RQ-2b: Can preprocessing and cleaning of the issue reports improve the performance of the SOTA regexes? The confusion matrix of true labels vs. predicted labels of SOTA regexes run on the preprocessed dataset is given in Figure 17. We discuss the evaluation metrics and our observations below. Precision: Precision here is 0.0196, indicating that there were a large number of false positives captured. Recall (Sensitivity): In this case, recall is 0.9398, indicating a high proportion of actual positives were correctly identified.

[Página 16]
16 Wahab et al. F1 Score: The𝐹1score here is 0.0385, which is low, but higher than the 𝐹1score of state-of-the-art regular expressions. Minority Class 𝐹1Score: The minority class 𝐹1score here is 0.9689, which means that the model performs much better in correctly identifying instances of the minority class. In summary, the model on SOTA regexes performs poorly in terms of precision and overall 𝐹1score, indicating that it has a high rate of false positives. However, the 𝐹1score is better than that of state-of-the-art regular expressions. Also, it performs well in terms of recall and the 𝐹1score for the minority class, indicating that it effectively identifies instances of the minority class. We can safely say, that running SOTA regexes on pre-processed issue reports yields slightly better performance than running them on raw dataset. Fig. 17. Confusion Matrix of true labels vs predicted labels of SOTA regexes run on the pre-processed dataset Answering RQ-2c: Can machine learning-based techniques outperform the SOTA regexes? The confusion matrix of true labels vs. predicted labels of the language model is given in Figure 18. We analyze the evaluation metrics and discuss our observations below. Precision: The precision here is 0.6309, indicating that approximately 63.09% of the predicted positive instances are actually true positives. Recall: Here, recall is 0.6385, meaning 63.85% of the actual positive data are correctly predicted. F1 Score: The𝐹1score here is 0.6347, which indicates the overall effectiveness of the model in terms of precision and recall. Minority Class 𝐹1Score: The minority class 𝐹1score here is 0.7794, indicating that the model performs significantly better in correctly identifying instances of the minority class compared to the overall 𝐹1score. To summarize, the model based on machine learning demonstrates relatively good precision and recall, with a balanced𝐹1score. Additionally, it performs particularly well in identifying instances of the minority class, as indicated by the higher minority class 𝐹1score. It suggests that machine learning techniques perform way better than the SOTA regexes on issue reports.

[Página 17]
Secret Breach Prevention in Software Issue Reports 17 Fig. 18. Confusion Matrix of true labels vs predicted labels of Language Model ( RoBERTa-base , AdamW optimizer, context window = 125, lr = 1𝑒−5) 5.4 Training and Inference Pipeline of Model The proposed pipeline for secret breach detection in issue reports contains a few stages. First, text preprocessing (Section 4) is carried out with the help of a custom cleaning function aimed at removing unnecessary noises like directory listings, shell commands, file paths, UUIDs, URLs, SHA hashes, and other irrelevant artifacts that might prevent further analysis from the report. The subsequent step involves identifying candidate strings in each issue report with the help of regex for matching patterns indicative of keys, tokens, hashes, or credentials. After preprocessing, a context window is defined around every candidate string to provide the local information relevant to the possible secret breach. The context window follows the rule of a fixed-size window, say ±200 characters, that retains useful information while excluding out-of-topic text. This pre-processed and contextualized information then serves as input for a pre-trained language model mentioned earlier, such as RoBERTa, in the classification of whether the report contains a secret breach or not. The pipeline is illustrated in Figure 19. 5.5 Effectiveness of Model on Real World Repositories We tested the model ( RoBERTa-base , AdamW optimizer, lr = 1𝑒−5, with a context window of 200characters) on a diverse set of real-world repositories to assess its performance in practical scenarios. These repositories include Microsoft’s API Guidelines, Google’s Node.js client for Google APIs, Keycloak for identity and access management, Kubernetes for container orchestration, Redis for in-memory data storage, Supabase for open-source Firebase alternatives, HashiCorp’s Terraform provider for AWS infrastructure, and Vercel’s platform for deploying serverless functions and static websites. The total number of crawled issue reports from these repositories is 120200 , and the total captured secret by the model with a context window of 200is309. This evaluation provides deep insight into the model’s effectiveness across various domains and how it adapts to different software development contexts. Table 6 highlights some sample secrets detected in these repositories. We have obfuscated some of the characters in our secrets to protect the privacy of the users. Discussion about detected secrets .Our detection model ( RoBERTa-base , AdamW optimizer, lr = 1𝑒−5, with a context window of 200characters) is designed to identify sensitive information such as tokens, API keys, master keys, access tokens, SHA-256 based secrets, and session keys within issue reports. It primarily scans for specific keywords

[Página 18]
18 Wahab et al. Start: Issue Report Input Regex Detection (Identify Candidate Strings) Text Preprocessing (Remove Noise) Context Window Extraction (±200 Characters) Pre-trained Model Secret Breach Detected? Secret Breach Reported No Secret Found End Fig. 19. Pipeline for Secret Breach Detection in Issue Reports. Table 6. Repositories, their total flagged secrets, and sample secrets for context window of 200 characters Repository Total Flagged Secrets (model)True Pos- itivesSample Secret Kubernetes 200 65–certificate-key 698b0866289********587f47e145f0111e4732874d51c9ee8f3994555eae405 kube key = AQDcffhZ1i3HMxAA5oB+S**********Dc5tKeQ== Terraform Provider AWS 62 15 client_request_token = “c9a0b34f-877c-4cee-9fa6-*********4fd" Keycloak 17 4client_secret=iXwJB01N5u0W**********vUWm5VA9HN client_secret=wDVOYAWe8R0LxkVGe1MOI5PcuVyWlZ14 Supabase 13 5 SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsI**********XVCJ9eyAgCiA**********SI6 ICJzZXJ2aWNsXJvbGUi LAogICAgImlzcyI6IC**********ZS1kZW1vIiwK ICAgICJpYXQiOiAxNjQxNzY5MjAwLAogIC**********IDE3OTk1MzU2MDAKfQ. DaYlN**********7tqibS-PHK5vgusbcbo7X36XVt4Q Google API Node.js Client 7 6 accessToken=‘ya29.nwI5Em6UnYGHvV**********-xzFl4_JG3_c- _t4FJ3owll_8i_rL5M17LFV6VlF7QE’; Vercel 5 2 Token abcdegjkdj%2tue**********sb%3D Microsoft API Guidelines 3 1 “storageAccessKey": “mDCN2IFsCkM250yvHFRJv4UZEPJ+CPAM8q/mWDNiENdX arhuSBRD8QoDh**********Ev61LQtD8oi75D4Uwng==", Redis 2 1 SSL-Session: Master-Key: 32A9B7E8DB11647956B5B4D23CC728 0B63CFD7xxx383A7C4**********D31AC2C72D61 Total = 309 Total = 99 like ‘password’, ‘token’, and ‘key’ to flag potential secrets. However, this method occasionally leads to false positives where non-sensitive data is mistakenly identified as a secret. For instance, strings such as placeholders or unrelated text may trigger the detection mechanism if they contain these keywords. Some of these examples are shown below:

[Página 19]
Secret Breach Prevention in Software Issue Reports 19 •“client_secret”: “5fS6xxxxxxxxxxxxxxxxxxf0w", In this scenario, the model identifies the word ’secret’ and marks it as a potentially sensitive item. But it is a dummy secret due to the masking of characters with ’x’. •session_state=78b4fb0f-781f-4686-91e8-58ead1f797ff The model identifies the keyword ’session’ and marks it as a potential secret, although it pertains to a session state rather than sensitive information. •grant_type=password The model identifies the keyword ’password’ and categorizes it as a potential secret, but in this context, it simply refers to a variable and not an actual sensitive piece of information. •auth_method=openid-connect The model flags "auth_method=openid-connect" as a potential secret due to the presence of the term "auth", but it’s actually an authentication method specification and not sensitive information. •kubeadm:default-node-token The string is flagged by a model as a potential secret due to its format resembling a token. However, in the context of Kubernetes (kubeadm), this is a standard identifier for a node token used for cluster management and should not be considered a sensitive secret. •token-ca-cert-hash The term "token-ca-cert-hash" might trigger a security model’s detection due to its resemblance to a potentially sensitive token or cryptographic hash related to certificate authority (CA) validation. However, this is likely a standard identifier used for referencing or managing CA certificate hashes, rather than an actual secret. •‘JWT_SECRET=7dxxx5F1-xxxx-xxxx-xxxx-f6xxx27aa398‘ The model flags this as potentially containing a sensitive keyword ("secret"), but it’s actually a secure JSON Web Token (JWT) secret obscured for privacy with placeholder characters ("xxx"). We contacted the repository maintainers to notify them about the potential presence of sensitive information or secrets in their issue reports. Despite our efforts, we received no response or acknowledgment from the repository maintainers. 6 SBMBot. A Context-Aware Warning Browser Extension to Mitigate Secret Breaches in Issue Reports Using the language model we developed earlier, we extended its functionality into a browser extension called SBMBot. This tool provides real-time, context-aware warnings to users while they create issue reports, aiding in the prevention of accidental secret breaches. 6.1 Workflow of SBMBot The workflow of the extension involves multiple components working together to detect potential exposure of secrets or sensitive information in issue reports. Here’s a breakdown of how the extension operates: Browser Extension Activation .Upon installation of the browser extension, it automatically activates when users navigate to the page for posting new issues. This activation does not require any manual intervention from the user, ensuring that the secret detection process begins as soon as the user starts interacting with the issue creation page.

[Página 20]
20 Wahab et al. Real-Time Content Monitoring .The extension continuously monitors the content in the description field of new issues. As the user types, the extension captures the content in real-time and sends it to our Flask server for processing. Secret Detection with RoBERTa Model .The Flask server utilizes the RoBERTa-based machine learning model to analyze the content for potential secrets. When the extension sends the description content, the server processes it in real-time and determines if any sensitive information is present. If a secret is detected, the server immediately sends a response back to the extension. Visual Feedback .Upon receiving the server’s response, the extension provides instant visual feedback by changing the color of a small button from green to red. This color change is prominently displayed within the issue creation interface, ensuring that the user is quickly alerted to the presence of any detected secrets without disrupting their workflow. Polling Mechanism .The extension uses a polling mechanism to automatically send the description content to the server at regular intervals. This ensures continuous monitoring for any new secrets as the user edits the description. The workflow of the secret mitigator extension is given in Figure 20. Fig. 20. Workflow of the Secret Mitigator Extension Server Specification .To speed up the process from payload delivery to instant detection of secrets and overall pipeline execution, a high-end server computer with the following specifications was used, as shown in Table 7.

[Página 21]
Secret Breach Prevention in Software Issue Reports 21 Table 7. System Specification Component Specification Processor Intel(R) Xeon(R) E-2176G CPU @ 3.70GHz RAM 64 GB System type 64-bit Operating System, x64 based processor Network speed 340 Mbps 6.2 Effectiveness Analysis of SBMBot This section assesses how well SBMBot performs and how users perceive it when it comes to locating secrets in GitHub issue reports. We gave each of the 30 participants in our follow-up survey 10 issue reports in order to evaluate this. Participants were tasked with identifying secrets within these reports—five using our extension and five using manual methods. Based on this additional survey, we address the following research questions: (3) Research Question (RQ) 3a) How likely is our bot able to identify secrets in issue reports of GitHub? 3b) What is the confidence level of users in posting secrets with and without using our bot/extension? 3c) How do you perceive the necessity of warning tools like our bot to prevent secret breaches? 3d) What are the future improvement opportunities for our bot? 3e)What are the other tool supports that you might need to assist you in issue report management while ensuring security? Answering RQ-3a: How likely is our bot able to identify secrets in issue reports of GitHub? Figure 21 shows that most of the participants (43.3%) rated a confidence level of 4, implying that the identification of secrets in issue reports by our bot is highly likely. A significant portion (26.7%) rated their confidence at a level of 5, indicating very high confidence in the bot’s abilities to detect secrets.13.3% of respondents gave a rating of 3, indicating moderate confidence. 10% of respondents selected a rating of 2, and only 6.7% selected a rating of 1, implying that lower confidence levels were not chosen frequently. These findings indicate that while the majority of respondents are confident in the bot’s capability (ratings of 4 or 5), a minority are skeptical, indicating that the bot’s performance or effectiveness could be improved. Answering RQ-3b: What is the confidence level of users in posting secrets with and without using our bot/extension? The comparative confidence levels of users in preventing secret breaches while posting GitHub issues, with and without using our extension, are shown in Figures 22a and 22b. Figure 22a illustrates how user confidence varies more when the extension is removed. In particular, only 30% of users indicated the highest confidence level (5), whereas 43.3% of users reported a high level of confidence (4 on the scale). Moreover, a significant segment of participants demonstrated reduced confidence levels; 10% of them rated their own level as 3, while 9.3% and 7.3% rated their level as 2 and 1, respectively. This suggests that without the extension, users are usually less confident in their ability to stop secret breaches. On the other hand, Figure 22b, which depicts the situation in which users used the extension, demonstrates a significant rise in confidence. The highest level on the scale, level 5, was rated by the majority of 54%, and level 4, which

[Página 22]
22 Wahab et al. Fig. 21. Likelihood of detecting a secret breach in issue reports by our bot (a) Confidence meter without using extension (b) Confidence meter using extension. Fig. 22. Comparison of user confidence in avoiding secret breaches. is rated by 28.67%, also showed high confidence. Just 4% of users at level 1 and 2.67% at level 2 reported having low confidence. This comparison analysis shows that users who use the extension feel much more confident in avoiding secret breaches; in particular, the percentage of users expressing the highest level of confidence (level 5) increases by 24% when compared to when the extension is not used. For the open-ended questions RQ-3c, RQ-3d, and RQ-3e, we adopted the same methodology as Section 2 to categorize the responses. The categories observed during the open coding process are summarized in the tables 8, 9, and 10 with #C representing the number of codes and #R representing the number of respondents for each category. R 𝑗denotes the response of the j-th participant. Answering RQ-3c: How do you perceive the necessity of warning tools like our bot to prevent secret breaches? Based on the responses of 26 participants, the factors that drive the necessity of the security tool are summarized in Figure 23. These themes show different factors that influence how the tool is used. (1)Generalized Necessity (50%): Half of the participants cited this theme, which indicates the tool’s usage for generalized security purposes. For instance, R 6mentioned, "It’s really necessary to have some tools like this. " (2)Secret Prevention/Detection (34.62%): A notable proportion of participants stated that tools like ours can help prevent or detect secrets. As R 4explained, "It may help reducing accidental posting of secrets"

[Página 23]
Secret Breach Prevention in Software Issue Reports 23 Table 8. Categories and Respondents of RQ-3c Category Count (#C) Responses (#R) Secret Prevention/Detection 9 9 Developer Assistance 3 3 Enforcing Security Practices 3 3 Concerns about Security Risks of the Tool Itself 1 1 Generalized Necessity 13 13 Total 26 26 Table 9. Categories and Respondents of RQ-3d Category Count (#C) Responses (#R) Highlighting and Locating Secrets 14 14 Improving Accuracy & Reducing False Positives 7 7 Performance Enhancements 3 3 Suggesting Fixes or Automated Solutions 3 3 Advanced Detection and Contextual Awareness 7 5 User Experience Improvements 3 3 Total 32 30 Table 10. Categories and Respondents of RQ-3e Category Count (#C) Responses (#R) Version Control and Access Management 1 1 Automated Issue Reporting & Compliance 2 2 Secure Encryption and Data Protection 3 3 Replacement or Redaction of Sensitive Data 1 1 No Additional Support/Unclear Need 8 8 Total 16 16 Fig. 23. Likelihood of detecting a secret breach in issue reports by our bot (3)Developer Assistance (11.54%): Some respondents noted that the tool assists developers by enabling automation for certain tasks or helping in flagging potential issues, R 4remarked, "An excellent helper. With cloud services that are

[Página 24]
24 Wahab et al. accessed through tokens & keys becoming the de-facto Internet infrastructure, such bots can put developers at ease while filing reports." (4)Enforcing Security Practices (11.54%): A portion of participants indicated that the tool plays a vital role in enforcing general security practices. R 3stated, "Warning tools are essential to prevent accidental secret exposure, ensuring security practices are followed." (5)Concerns about Security Risks of the Tool Itself (3.84%): A small number of participants expressed concerns regarding the potential risks introduced by the tool itself These findings state that while the tool is widely viewed as highly necessary for maintaining security practices by preventing breaches, concerns about the tool’s security risks, although minimal, are also present. Answering RQ-3d: What are the future improvement opportunities of our bot? Fig. 24. Future improvement opportunities of bot Figure 24 summarizes the future improvement opportunities for the bot based on feedback from 30 participants: (1)Highlighting and Locating Secrets (46.67%): The most common suggestion was to add highlighting and locating secrets in the reporting functionality to our bot. As R 1noted, "This is not related to the bot ability, but perhaps highlighting the secret may help in long reports." (2)Improving Accuracy (23.33%): A portion of the participants pointed out the need for greater accuracy in detecting security risks. R 10remarked, "The detection string can be more accurate" (3)Automated Solutions (16.67%): Respondents expressed that our bot should provide automated solutions. (4)Advanced Detection and Fix Suggestions (16.67%): A small portion of respondents desired the bot to offer more advanced detection methods and also recommended fixes in place of the detected strings. As R 12commented, " not only warning, suggesting changes should be nice" (5)Performance Enhancements (16.67%): A smaller portion of users desired for speed and performance enhance- ment of bot. One participant said, "Performance could be faster, especially when working with larger codebases." (6)User Experience Improvements (10%): Some users highlighted the need to enhance the user interface and overall interaction with the bot. One respondent stated, "A more intuitive user experience would make the bot easier to use for everyone." Answering RQ-3e: What are the other tool supports that you might need to assist you in issue report management while ensuring security? Figure 25 summarizes the additional tool supports that may be needed based on feedback from 16 participants:

[Página 25]
Secret Breach Prevention in Software Issue Reports 25 Fig. 25. Additional tool supports needed for issue report management (1)No Additional Support/Unclear Need (50%): Half of the participants indicated that further support was not needed, or they were unsure of any additional need for tools. As R 4mentioned, "This thing is fine" (2)Secure Encryption and Data Protection (18.75%): Some participants expressed the need for tools that securely encrypt sensitive data and thus protect it. As R 3stated, "Tool that supports secure encryption for sensitive data within issue reports." (3)Automated Issue Reporting & Compliance (12.5%): A few respondents suggested automating issue reporting and compliance as essential improvements. R 1said, "Some repositories have strict rules for issue reporting, which benefit both them and the users. Maybe a bot can check if an issue follows these rules of a repository." (4)Version Control and Access Management (6.25%): A small group emphasized the importance of tools to manage version control and restrict access to sensitive issue reports. (5)Replacement or Redaction of Sensitive Data (6.25%): Some users highlighted the need for tools to automatically redact or replace sensitive data in issue reports. R 12remarked, "Instead of pointing out if a secret breach might happen, a tool that would replace the secret with a placeholder." (6)Performance Enhancements (6.25%): A minor portion of respondents pointed out that improving the tool’s performance would be beneficial. 7 Threats to Validity In this section, we briefly discuss the limitations of our research. •VCS Selection: We did not consider other VCS (Version Control System) services such as GitLab [ 6] and Bitbucket [2]. In the future, we plan to expand our dataset by including repositories of other VCS services. •Manual Analysis Bias: The labeling of the secrets in our dataset is susceptible to bias. To mitigate the bias, a second rater labeled the secrets independently, and we resolved the disagreements. •Skewness of Benchmark Dataset: The benchmark dataset has 25000 instances with only 437 true positives, which makes it extremely skewed. Though we have managed to filter out false positives mostly, there is still scope for improvement. For the detection of true positives efficiently, more training examples with label 1 are needed. Oversampling techniques were not used to balance the dataset since we worked with real-world issue reports. Moreover, in textual data, there cannot be real text representation of new synthetic samples.

[Página 26]
26 Wahab et al. •Presence of False Negatives: Additionally, we recognize the potential for false negatives in the software issue reports. However, manually scrutinizing issue reports to identify overlooked passwords/credentials falls beyond the scope of our research. •Exploitability of the Secrets: We lack definitive confirmation regarding the exploitability of the secrets we uncover. Despite our attempts to filter out non-sensitive, outdated, or invalid secrets, we cannot guarantee their exploitability without conducting actual tests. 8 Related Work Secrets in Source Code. Meli et al. [ 25] carried out extensive research on secret leaks on GitHub by analyzing several billion files through real-time scans and snapshots. While doing so, they observed that still a great amount of secret leakage is there; over 100,000 repositories are afflicted with thousands of secrets being exposed daily. This study applied regular expressions, entropy-based filtering, and pattern detection in order to reduce false positives and achieved 99.29% accuracy. Enhanced Secret Detection Using Various Techniques. Sinha et al. [ 33] extended this work by using program slicing, keyword, and pattern-based searches. They achieved high precision and recall in detecting API keys and highlighted the challenges of false positives and proximity-based detection methods. Both studies emphasize the risks of secret leakage on GitHub. Machine Learning Approaches for Secret Detection. Saha et al. [ 29] address the challenges to reduce the number of false positives in secret detection through different machine learning techniques. They compiled a dataset of secrets and false positives by collecting files via the Git API and scanning them using regex-based detection. Binary features such as character patterns, entropy levels, and file types were used to train various machine-learning models. Soft Voting Classifier, with its F1 score of 86.7%, which greatly reduced false positives when put to test in a comparative study involving six algorithms including an ensemble Voting Classifier. Deep Neural Networks for Secret Detection. Feng et al. [ 10] employed deep neural networks for the textual passwords to understand their inherent patterns and also enhance the detection of actual passwords reducing false positives in the process. Mitigation Strategies for Secret Breaches. Sinha et al. [ 33] suggest solutions to reduce secret breaches in source code, such as keeping secrets in separate, untracked files or encrypting them with tools like git-crypt instead of incorporating them in the code directly. If secrets are accidentally leaked, tools such as git filter-branch can be used to remove them from version history, although this may cause synchronization problems for other collaborators. They also recommend automating leak protection by integrating a key detection and mitigation plugin into version control systems, which warns users and provides auto-refactoring when potential leaks are found during code commit. Best Practices for Secret Management. Basak et al. [ 5] identify 24 practices for secret management, classified into six categories. These include keeping secrets out of source code (e.g., using environment variables or external configuration files), securely storing secrets (e.g., using secret management systems like HashiCorp Vault), limiting exposure by using short-lived secrets and sanitizing VCS history, and avoiding accidental secret commits by using VCS scan tools and commit hooks. For deployment, they recommend that hard-coded secrets be removed from CI/CD scripts

[Página 27]
Secret Breach Prevention in Software Issue Reports 27 and that secret variables be disabled in pull requests. Organizations can enforce security policies through developer permissions, two-factor authentication, and commit signing. While these works primarily focus on detecting secrets embedded in source code, the detection of sensitive information in other areas, such as issue reports, remains underexplored. Detecting secrets in issue reports and other textual content using pre-trained language models is a promising but largely unexplored area. Pre-trained language models could enhance secret detection across a broader spectrum of software artifacts, offering a new way for mitigating potential breaches outside the realm of source code analysis. 9 Conclusion In conclusion, the evaluation of state-of-the-art regular expressions (SOTA regexes) revealed a significant disparity between precision and recall, with a huge amount of false positives. However, preprocessing of issue reports marginally improved the performance metrics, albeit still displaying limitations in precision and overall effectiveness. Again, employing machine learning-based techniques, particularly the language model ( roberta-base ), demonstrated superior precision and recall, with a balanced F1 score. Notably, the model exhibited very good performance in identifying instances of the minority class while eliminating false positives. These findings suggest that while SOTA regexes may have utility, machine learning techniques offer a more robust approach for detecting secret breaches in issue reports, particularly in contexts where precision and minority class identification are paramount. Our study has certain constraints. Initially, we only have 25000 instances where there are only 437 true positives, and in machine learning, a greater number of instances typically enhance learning and boost performance. Although our model is working fine while eradicating false positives, it might miss some of the true positives. In the future, we can incorporate more instances, preferably true positives, to increase the model’s efficiency. So, we will focus on upgrading our model to better understand context and intent, reducing false alarms while still detecting genuine security risks. This involves using more advanced algorithms to analyze text and patterns and integrating feedback mechanisms for continuous improvement. As an extension, we will also craft some manual features in addition to the context of the issue report for better evaluation of the candidate string. References [1] AGWA. 2012. git-crypt . Retrieved 2024-03-26 from https://github.com/AGWA/git-crypt [2] Atlassian. 2008. Bitbucket . Retrieved 2024-04-16 from https://bitbucket.org/product [3]Setu Kumar Basak, Lorenzo Neil, Bradley Reaves, and Laurie Williams. 2022. What are the practices for secret management in software artifacts?. In 2022 IEEE Secure Development Conference (SecDev) . IEEE, 69–76. [4]Setu Kumar Basak, Lorenzo Neil, Bradley Reaves, and Laurie Williams. 2023. Regular Expressions used in SecretBench dataset . Retrieved 2024-03-27 from https://zenodo.org/records/7571266 [5]Setu Kumar Basak, Lorenzo Neil, Bradley Reaves, and Laurie Williams. 2023. SecretBench: A Dataset of Software Secrets. arXiv preprint arXiv:2303.06729 (2023). [6] GitLab B.V. 2011. GitLab . Retrieved 2024-04-16 from https://about.gitlab.com/ [7]Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 (2020). [8] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 1 (1960), 37–46. [9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [10] Runhan Feng, Ziyang Yan, Shiyan Peng, and Yuanyuan Zhang. 2022. Automated detection of password leakage from public github repositories. In Proceedings of the 44th International Conference on Software Engineering . 175–186. [11] Sally Fincher and Josh Tenenberg. 2005. Making sense of card sorting data. Expert Systems 22, 3 (2005), 89–93. [12] GitGuardian. 2017. ggshield . Retrieved 2024-02-02 from https://www.gitguardian.com/ggshield [13] GitGuardian. 2024. State of Secrets Sprawl Report 2023 . Retrieved 2024-03-12 from https://www.gitguardian.com/state-of-secrets-sprawl-report-2023

[Página 28]
28 Wahab et al. [14] GitHub. 2009. GitHub Issues Documentation . Retrieved 2024-10-16 from https://docs.github.com/en/issues [15] GitHub. 2024. Getting Started with the REST API . Retrieved 2024-03-26 from https://docs.github.com/en/rest/using-the-rest-api/getting-started- with-the-rest-api?apiVersion=2022-11-28 [16] Gitleaks. 2018. Gitleaks . Retrieved 2024-02-02 from https://github.com/gitleaks/gitleaks [17] Khalid Hasan, Partho Chakraborty, Rifat Shahriyar, Anindya Iqbal, and Gias Uddin. 2021. A Survey-Based Qualitative Study to Characterize Expectations of Software Developers from Five Stakeholders. In Proceedings of the 15th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM) . 1–11. [18] Connor Jones. 2023. Cryptojackers steal AWS credentials from GitHub in 5 minutes . Retrieved 2024-02-02 from https://www.theregister.com/2023/10/ 30/cryptojackers_steal_aws_credentials_github/ [19] Rafael Kallis, Maliheh Izadi, Luca Pascarella, Oscar Chaparro, and Pooja Rani. 2023. NLBSE’23 issue report dataset . Retrieved 2024-02-02 from https://github.com/nlbse2023/issue-report-classification [20] Ramakrishnan Kandasamy. 2020. Secret Detection Tools for Source Codes . Retrieved 2024-02-02 from https://github.com/rmkanda/tools [21] Venus Kohli. 2023. Context Window . Retrieved 2024-04-13 from https://www.techtarget.com/whatis/definition/context-window#:~:text=A% 20context%20window%20is%20a,time%20the%20information%20is%20generated. [22] Igal Kreichman. 2021. The secrets about exposed secrets in code . Retrieved 2024-02-02 from https://apiiro.com/blog/the-secrets-about-secrets-in-code/ [23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [24] Navaneeth Malingan. 2024. Attention Mechanism in Deep Learning . Retrieved 2024-06-24 from https://www.scaler.com/topics/deep-learning/ attention-mechanism-deep-learning/ [25] Michael Meli, Matthew R McNiece, and Bradley Reaves. 2019. How bad can it git? characterizing secret leakage in public github repositories.. In NDSS . [26] Michenriksen. 2014. gitrob . Retrieved 2024-03-26 from https://github.com/michenriksen/gitrob [27] Matthew B Miles. 1994. Qualitative data analysis: An expanded sourcebook. Thousand Oaks (1994). [28] Sebastian Ruder. 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747 (2016). [29] Aakanksha Saha, Tamara Denning, Vivek Srikumar, and Sneha Kumar Kasera. 2020. Secrets in source code: Reducing false positives using machine learning. In 2020 International Conference on COMmunication Systems & NETworkS (COMSNETS) . IEEE, 168–175. [30] Takaya Saito and Marc Rehmsmeier. 2015. The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PloS one 10, 3 (2015), e0118432. [31] Truffle Security. 2016. Regular Expressions used in TruffleHog . Retrieved 2024-03-27 from https://github.com/trufflesecurity/trufflehog/tree/main/ pkg/detectors [32] Claude Elwood Shannon. 1948. A mathematical theory of communication. The Bell system technical journal 27, 3 (1948), 379–423. [33] Vibha Singhal Sinha, Diptikalyan Saha, Pankaj Dhoolia, Rohan Padhye, and Senthil Mani. 2015. Detecting and mitigating secret-key leaks in source code repositories. In 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories . IEEE, 396–400. [34] Sophos. 2023. Sophos 2023 Threat Report . Retrieved 2024-03-21 from https://www.sophos.com/en-us/content/security-threat-report [35] TruffleSecurity. 2016. TruffleHog . Retrieved 2024-02-02 from https://github.com/trufflesecurity/trufflehog [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).