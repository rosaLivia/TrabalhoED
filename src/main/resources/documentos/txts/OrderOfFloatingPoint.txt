
[P√°gina 1]
FPRev: Revealing the Order of Floating-Point Summation by Numerical Testing PEICHEN XIE, YANJIE GAO, and JILONG XUE, Microsoft Research The order of floating-point summation is a key factor in numerical reproducibility. However, this critical information is generally unspecified and unknown for most summation-based functions in numerical libraries, making it challenging to migrate them to new environments reproducibly. This paper presents novel, non- intrusive, testing-based algorithms that can reveal the order of floating-point summation by treating functions as callable black boxes. By constructing well-designed input that can cause the swamping phenomenon of floating-point addition, we can infer the order of summation from the output. We introduce FPRev, a tool that implements these algorithms, and validate its efficiency through extensive experiments with popular numerical libraries on various CPUs and GPUs (including those with Tensor Cores). FPRev reveals the varying summation orders across different libraries and devices, and outperforms other methods in terms of time complexity. The source code of FPRev is at https://github.com/microsoft/RepDL/tree/main/tools/FPRev. 1 INTRODUCTION With the rapid evolution of heterogeneous hardware and diverse software stacks, the lack of reproducibility in numerical computing has become a recognized problem [ 2,4,13,21,29,30]. The same numeric function can produce varying results when software is migrated to new hardware or when numerical libraries are updated. Non-reproducible results pose significant challenges in scientific research, software engineering, deep learning, and applications that rely on numerical models for decision making. These challenges undermine the credibility of findings, hinder progress by obscuring errors in programs, and can lead to incorrect conclusions or suboptimal decisions, ultimately affecting the reliability and trustworthiness. A primary cause of numerical non-reproducibility is discrepancies in the order of floating- point summation [ 1,6,8,9,27]. The result of floating-point summation depends on the order of computation due to the non-associative nature of floating-point addition. For example, as shown in Table 1, the sum of 0.1, 0.2 and 0.3 is order-dependent, because (0.1+0.2)+0.3‚â†0.1+(0.2+0.3)in IEEE-754 [ 16] binary64 (also known as float64). There is no general specification that stipulates the order of floating-point summation. Consequently, without well-defined specifications, numerical libraries usually compute floating-point summation in various orders in different environments, leading to inconsistent numerical output. Table 1. Examples of the non-associative nature of float64 addition. (0.1+0.2)+0.3 0 .1+(0.2+0.3) (‚àí 260+260)+1‚àí260+(260+1) Decimal 0.60000000000000008882 0.59999999999999997780 1 0 Hexadecimal 0x1.3333333333334p-1 0x1.3333333333333p-1 0x1p0 0x0p0 Knowing the order of summation is critical for reproducibility. Consider a function based on floating-point summation (e.g., matrix multiplication) that produces inconsistent output in new environments, which is undesirable. To fix the issue, the order of summation must be known. This information can serve as a valuable guide and constraint in determining the appropriate order of summation when migrating the function to the new environments. However, the information is virtually unknown. Existing numerical libraries, such as Intel MKL [ 17], OpenBLAS [ 7], and NVIDIA cuBLAS [ 24], do not specify this information in their documentation, and there is no specialized tool to reveal the information.arXiv:2411.00442v1 [math.NA] 1 Nov 2024

[P√°gina 2]
Revealing the order of summation is a challenging task. For example, people can manually deter- mine the order by analyzing the static source code, but many libraries or hardware implementations are black-box, which limits the static approach. Even if the function‚Äôs trace is obtained and analyzed, no tool can automatically generate the computational graph of the summation. We build a non-intrusive, testing-based tool called FPRev to reveal the order of summation. FPRev treats the summation-based function as a callable black box, generates specialized test cases, and infers the order of summation from the function‚Äôs output. FPRev provides two versions of algorithms: FPRev-basic and FPRev-advanced. Both leverage the swamping phenomenon in floating-point addition to generate well-designed numerical input. When two floating-point numbers differing by many orders of magnitude are added, the smaller number is swamped and does not contribute to the sum. For example, 260+1equals 260when using float64. Based on the phenomenon, we can utilize large numbers as masks to hide certain summands from the sum. In FPRev-basic, we first generate several ‚Äúmasked all-one arrays‚Äù. Each array is predominantly composed of the floating-point number 1.0, with exactly two non-one elements: ùëÄand‚àíùëÄ. Here, ùëÄrepresents a large positive number that can cause the swamping phenomenon in floating-point arithmetic. Specifically, let ùëõdenote the number of summands. ùëÄsatisfies¬±ùëÄ+ùúá=¬±ùëÄfor all non-negative number ùúá<ùëõ. Next, we call the tested function multiple times with different masked all-one arrays. Each output reveals how many summands are swamped by ¬±ùëÄduring summation and how many are not. This information relates to the structure of the computational graph. The graph is a full binary tree that accurately depicts the order of operations. Each output equals the number of leaf nodes out of the subtree rooted at the lowest common ancestor of the nodes corresponding to the indexes of ùëÄ and‚àíùëÄ. For example, Table 2 demonstrate the information for Algorithm 1, whose computational graph is Figure 1. Finally, we use the output information to construct the summation tree. This involves a tree- algorithm problem: how to construct a full binary tree given {(ùëñ,ùëó,ùëôùëñ,ùëó): 0‚â§ùëñ<ùëó<ùëõ}, where ùëôùëñ,ùëódenotes how many leaf nodes are in the subtree rooted at the lowest common ancestor of the ùëñ-th andùëó-th leaf nodes. We construct the tree in a bottom-up (leaf-to-root) way. We begin by constructing subtrees with two leaf nodes (corresponding to ùëôùëñ,ùëó=2). Subsequently, larger subtrees (corresponding to next larger ùëôùëñ,ùëó) are built from existing subtrees or isolate nodes, and the process is repeated until the entire tree is generated. Algorithm 1 An example of summation ( ùëõ=8). float sum = 0; for (int i=0; i<8; i+=2) sum += a[i] + a[i+1]; 0+ 1 2+ 34+ 56+ 7 +++ Fig. 1. The summation tree of Algorithm 1. The num- bers in the leaf nodes denote the indexes. 2

[P√°gina 3]
Table 2. The outputs and order-related information for Algorithm 1 with different masked all-one arrays. ùëñ ùëó ùê¥ùëñ,ùëósum(ùê¥ùëñ,ùëó)ùëôùëñ,ùëó 0 1(ùëÄ,‚àíùëÄ,1,1,1,1,1,1) 6 2 0 2(ùëÄ,1,‚àíùëÄ,1,1,1,1,1) 4 4 0 3(ùëÄ,1,1,‚àíùëÄ,1,1,1,1) 4 4 0 4(ùëÄ,1,1,1,‚àíùëÄ,1,1,1) 2 6 0 5(ùëÄ,1,1,1,1,‚àíùëÄ,1,1) 2 6 0 6(ùëÄ,1,1,1,1,1,‚àíùëÄ,1) 0 8 0 7(ùëÄ,1,1,1,1,1,1,‚àíùëÄ) 0 8 ... 2 3(1,1,ùëÄ,‚àíùëÄ,1,1,1,1) 6 2 2 4(1,1,ùëÄ,1,‚àíùëÄ,1,1,1) 2 6 ... FPRev-basic has a time complexity of Œò(ùëõ2ùë°(ùëõ)), whereùë°(ùëõ)is the time complexity of the tested function. As a contrast, the naive brute-force method has a time complexity of ùëÇ(4ùëõ/ùëõ3/2¬∑ùë°(ùëõ)). Building on FPRev-basic, we propose FPRev-advanced, which has a time complexity of Œ©(ùëõùë°(ùëõ)) andùëÇ(ùëõ2ùë°(ùëõ))and supports multi-term fused summation [ 10] used by matrix accelerators like NVIDIA GPU‚Äôs Tensor Cores [22]. We evaluate efficiency of FPRev by comprehensive experiments. We test FPRev with three popular numerical libraries across six different CPUs and GPUs. Experimental results show that FPRev-advanced is significantly faster than FPRev-basic, demonstrating its lower time complexity. We also showcase the discrepancies in the revealed orders across different libraries and devices. In summary, the contributions of this paper include the following: (1)We propose novel testing-based algorithms to reveal the order of summation for functions based on floating-point summation. The time complexity of the algorithms is polynomial, in contrast to the exponential time complexity of the naive approach. (2)We develop FPRev, a tool that enables automatic revelation of the order of floating-point summation. This tool is significantly helpful in debugging non-reproducible programs, and provides useful information for reproducing the program. (3) We demonstrate the practical efficiency of FPRev with extensive experiments. (4)We reveal the order of summation for common numerical libraries like cuBLAS for the first time. 2 PROBLEM STATEMENT 2.1 Definition of the problem We formulate the ordinary summation algorithm in Algorithm 2. To calculate the sum of ùëõfloating- point numbers, the floating-point addition is performed ùëõ‚àí1times in a predetermined order. We assume that the order of summation is unknown but unique to the given function, computational environment, and ùëõ(the number of floating-point numbers in the input).1 The order of summation can be represented as a computational graph in the form of a rooted full binary tree with ùëõleaf nodes and ùëõ‚àí1inner nodes. Each addition operation corresponds to an inner node, which represents the sum. The two children of the node represent the two summands of 1Therefore, randomized algorithms and algorithms where the order depends on the values of the summands are out of scope. 3

[P√°gina 4]
Algorithm 2 The ordinary summation algorithm. Require: Sequenceùê¥=(ùëé0,ùëé1,...,ùëéùëõ‚àí1) Ensure: Floating-point sum√çùëõ‚àí1 ùëò=0ùëéùëò function sum(ùê¥) ùê¥‚ÜêMultiSet(ùê¥) forùëò‚Üê0toùëõ‚àí1do Chooseùëé‚ààùê¥andùëè‚ààùê¥‚àí{ùëé}deterministically ùëê‚Üêfl(ùëé+ùëè) ‚ä≤floating-point addition ùê¥‚Üêùê¥‚àí{ùëé,ùëè}+{ùëê} returnùëê ‚ä≤nowùê¥={ùëê} this addition operation. For example, consider the program shown in Algorithm 1. The summation tree shown in Figure 1 depicts the order of summation of the program. The problem is how to reveal the order of summation for a black-box summation function sum. Specifically, we aim to design an algorithm whose input is the summation function (as a callback), and the number of summands ùëõ. The output of the algorithm is the summation tree. 2.2 Inefficiency of the naive solution Here we introduce a naive testing-based solution to the problem. The naive solution is based on brute-force search. We design a recursive function to enumerate the order of summation, as shown in Algorithm 3. For each order, we verify its correctness by random testing. Specifically, in Verify , we generate random input, compute the sum in that order, and compare the output with the output of the summation function. If they are identical for ùúàtrials, we accept the order. Algorithm 3 The brute-force algorithm. Require: Summation function sum, number of summands ùëõ, and number of trials ùúà Ensure: Summation tree ùëá function BruteForce (sum,ùëõ,ùúà) function Verify (ùëá) forùúàtimes do ùê¥‚ÜêRandom(ùëõ) ‚ä≤random array of size ùëõ ùë†‚Üêsum(ùê¥) ùë°‚ÜêComputeSum(ùê¥,order =ùëá) ifùë†‚â†ùë°then return False return True foreach possible summation tree ùëádo ifVerify (ùëá)then returnùëá The time complexity of the naive solution is ùëÇ(4ùëõ/ùëõ3/2¬∑ùë°(ùëõ)), because the number of all possible orders is the(ùëõ‚àí1)-th Catalan number ùê∂ùëõ‚àí1=(2ùëõ‚àí2)! ùëõ!(ùëõ‚àí1)!=ùëÇ(4ùëõ/ùëõ3/2). Here,ùë°(ùëõ)is the time com- plexity of the tested function. In addition to being inefficient, the naive solution is not fully reliable, because different orders can lead to the same output for some input. Although the probability is low and the reliability can be improved by increasing ùúà, a deterministic solution with full reliability is preferable. 4

[P√°gina 5]
2.3 Extension of the problem We also wish to reveal the order of summation for functions that can be abstracted as calls to the summation function with intermediate results as input. We call them ‚Äúsummation-based functions‚Äù. For example, dot product x¬∑ycan be treated as sum((ùë•0ùë¶0,ùë•1ùë¶1,...,ùë•ùëõ‚àí1ùë¶ùëõ‚àí1)). Thus, solutions to the original problem can be naturally extended to the problem for summation-based functions. 3 FPREV-BASIC This section demonstrates the basic algorithm FPRev-basic of our testing-based tool FPRev for revealing the order of floating-point summation. FPRev-basic has three stages. 3.1 Constructing masked all-one arrays The first stage is to construct different ‚Äúmasked all-one arrays", which are explained below. Let ùëõbe the number of summands, and let sum be an implementation of Algorithm 2 with a predetermined but unknown order of summation. Let ùëÄbe a floating-point number with the largest exponent, for example, ùëÄ=2127for float32 or ùëÄ=21023for float64. We then define a masked all-one array superscriptùëñ,ùëóasùê¥ùëñ,ùëó=(ùëéùëñ,ùëó 0,ùëéùëñ,ùëó 1,...,ùëéùëñ,ùëó ùëõ‚àí1)such that ùëéùëñ,ùëó ùëò= ùëÄ ifùëò=ùëñ ‚àíùëÄ ifùëò=ùëó 1.0 otherwise whereùëñandùëódenote the indexes of ùëÄand‚àíùëÄ, respectively. In ùê¥ùëñ,ùëó, there exist exactly one ùëÄand one‚àíùëÄ, with all other elements being 1.0. Insum(ùê¥ùëñ,ùëó), adding any summand or intermediate sum (except ùëÄand‚àíùëÄthemselves) to¬±ùëÄ results in¬±ùëÄ. Specifically, ùëÄ+ùúá=ùëÄand‚àíùëÄ+ùúá=‚àíùëÄhold for 0‚â§ùúá‚â§ùëõ‚àí2in floating-point arithmetic, if ùëõ‚â™ùëÄ. This demonstrates the swamping phenomenon of floating-point addition [15]. Therefore, ùëÄand‚àíùëÄserve as masks, swamping the summands or intermediate sums added to them. As a result, the sum of a masked all-one array ùê¥ùëñ,ùëódepends on the order of summation. For example, given ùëõ=4andùê¥0,2=(ùëÄ,1,‚àíùëÄ,1), the resulting sum can be 0, 1, or 2, depending on the order of summation. Pairwise summation (ùëÄ+1)+(‚àíùëÄ+1)yields 0, sequential summation ùëÄ+1+(‚àíùëÄ)+1yields 1, and stride summation ùëÄ+(‚àíùëÄ)+(1+1)yields 2. In this stage, we construct different masked all-one arrays by enumerating the locations of the masks, i.e.,ùëñandùëó. There areùëõ(ùëõ‚àí1)/2different arrays, which are {ùê¥ùëñ,ùëó: 0‚â§ùëñ<ùëó<ùëõ}. 3.2 Inferring order-related information The second stage is to identify the order-related information from the numerical output obtained using the constructed arrays as the input. The purpose of constructing masked all-one arrays is to leverage the masks to reveal information related to the order of summation. Because the masks swamp the summands or intermediate sums added to them, these numbers have no contribution to the sum. As a result, these summands are hidden by the masks. In contrast, only those summands not hidden by the masks contribute to the sum. Therefore, the output equals the sum of the summands that are not hidden by the masks. Since each of the summands equals 1.0, the output equals the number of the summands not hidden by the masks: ùëõùëñ,ùëó unhidden=sum(ùê¥ùëñ,ùëó). 5

[P√°gina 6]
Then, we can also obtain the number of the summands hidden by the masks: ùëõùëñ,ùëó hidden=ùëõ‚àí2‚àíùëõùëñ,ùëó unhidden. Take Algorithm 1 as an example. If ùëñ=2andùëó=4, then the array ùê¥2,4is(1,1,ùëÄ,1,‚àíùëÄ,1,1,1). Computing the sum in the order shown in Figure 1, the 3rd summand and the intermediate sum of the 0th and 1st summands are swamped by ùëÄ, so the 0th, 1st and 3rd summands are hidden byùëÄ; the 5th summand is swamped by ‚àíùëÄ, so in total, ùëõ2,4 hidden=4. In contrast, the 6th and 7th summands and their intermediate sum are not added to ùëÄor‚àíùëÄ, sosum(ùê¥2,4)=ùëõ2,4 unhidden=2. How does this information relate to the order, or specifically, the summation tree? Recall that ùëñand ùëódenote the locations of the masks, represented by the ùëñ-th andùëó-th leaf nodes in the summation tree. We note that the neutralization of the two masks (i.e., the addition operation ùëÄ+(‚àíùëÄ)=0) is represented by the lowest common ancestor of the ùëñ-th andùëó-th leaf nodes . Then, observing the subtree rooted at the lowest common ancestor, we find that all the summands hidden by the masks are in the subtree, and all the summands not hidden by the masks are out of the subtree. Therefore, the size of the subtree, defined as the number of leaf nodes in the subtree, equals ùëõ‚àíùëõùëñ,ùëó unhidden. Then we have ùëõLCA(ùëñ,ùëó) leaves=ùëõ‚àíùëõùëñ,ùëó unhidden=ùëõ‚àísum(ùê¥ùëñ,ùëó). For simplicity, we use ùëôùëñ,ùëóto denoteùëõLCA(ùëñ,ùëó) leaves, ‚Äúthe number of leaf nodes in the subtree rooted at the lowest common ancestor of the ùëñ-th andùëó-th leaf nodes", in the rest of the paper. In the previous example where ùëñ=2andùëó=4, the lowest common ancestor of the 2nd and 4th leaf nodes is the parent node of the 4th leaf node, corresponding to the neutralization of the 2nd summandùëÄand the 4th summand ‚àíùëÄ. Within the subtree rooted there, there are the 0th, 1st, 2nd, 3rd, 4th, and 5th leaf nodes, corresponding to the two masks and the summands hidden by the masks. In contrast, the 6th and 7th leaf nodes, corresponding to the unhidden summands, are out of the subtree. Therefore, ùëô2,4=ùëõLCA(2,4) leaves=8‚àí2=6. Table 2 shows more examples of the summation results and the order-related information for Algorithm 1. In summary, in this stage, we call the function with different masked all-one arrays, obtain the output of sum(ùê¥ùëñ,ùëó), and infer the order-related information ùêø={(ùëôùëñ,ùëó,ùëñ,ùëó)}from the output by calculating ùëôùëñ,ùëó=ùëõ‚àísum(ùê¥ùëñ,ùëó). (1) 3.3 Generating the summation tree The third stage is to generate the summation tree from the order-related information obtained. Now, generating the tree from ùêøis an algorithmic problem about trees. The basic idea of our solution is to construct the tree using a bottom-up approach. We first sort ùêøin ascending order. Then, according to each ùëôùëñ,ùëó, we find the existing roots of the trees containing theùëñ-th andùëó-th leaf nodes respectively, and merge them by constructing a new parent node for them. Repeating this process, we generate the tree from small subtrees to large subtrees. For example, consider the order-related information shown in Table 2. To generate the summation tree, we start by initializing the tree with eight disjoint nodes labeled 0 to 7. Then, examining the smallest value in ùêø, we haveùëô0,1=2. This signifies that the 0th and 1st leaf nodes should be in a subtree with 2 leaf nodes. Since the summation tree is a full binary tree, the 0th and 1st leaf nodes are exactly the two children of the root of the subtree. Therefore, we add a new node to the tree, label it with ùëõplus the label of its left child (i.e., ùëõ+0in this example), and add two edges from the two leaf nodes to the new node. 6

[P√°gina 7]
Forùëô2,3=ùëô4,5=ùëô6,7=2, similarly, we can construct the subtrees with the 2nd and 3rd, 4th and 5th, and 6th and 7th leaf nodes as their leaf nodes. Then, we have four subtrees of size 2, where the size of a subtree is defined as the number of leaf nodes in it. Next, the smallest unexamined value in ùêøisùëô0,2=4. This signifies that the 0th and 2nd leaf nodes should be in a subtree with 4 leaf nodes. We note that the 0th and 2nd leaf nodes are currently in two subtrees of size 2, so we should merge the two subtrees. Therefore, we find the current roots of the trees containing the 0th and 2nd leaf nodes respectively, denoted by ùëñ‚Ä≤=ùëõ+0andùëó‚Ä≤=ùëõ+2. Then, we add a new node as the parent of ùëñ‚Ä≤andùëó‚Ä≤, label it with ùëõplus the label of its left child (i.e., ùëõ+ùëñ‚Ä≤=2ùëõ+0in this example), and add two edges from ùëñ‚Ä≤andùëó‚Ä≤to the new node. Forùëô0,3=4, we find that the 0th and 3rd leaf nodes are already in the same subtree of size 4, so we just skip it. The same process is applied to ùëô1,2=ùëô1,3=4. Then, we have a subtree of size 4 and two subtrees of size 2. The next smallest unexamined value in ùêøisùëô0,4=6, which signifies that the 0th and 4th leaf nodes should be in a subtree with 6 leaf nodes. Similarly, the 0th and 4th leaf nodes are currently in two subtrees of size 4 and 2, respectively, so we should merge the two subtrees. Therefore, following the similar process, we find the current roots of the trees containing the 0th and 4th leaf nodes respectively (i.e., ùëñ‚Ä≤=2ùëõ+0,ùëó‚Ä≤=ùëõ+4). Then, we construct a new node as their parent (labelled as ùëõ+ùëñ‚Ä≤=3ùëõ+0), and add two edges from ùëñ‚Ä≤andùëó‚Ä≤to it. Forùëô0,5=6, we find that the 0th and 5th leaf nodes are already in the same subtree of size 6, so we skip it. The same process is applied to ùëñ‚àà{1,2,3}andùëó‚àà{4,5}. Then, we have a subtree of size 6 and a subtree of size 2. Finally, in the similar way, the next smallest unexamined value in ùêøisùëô0,6=8, which signifies that the 0th and 6th leaf nodes should be in a subtree with 8 leaf nodes. Since the two leaf nodes are currently in two subtrees of sizes 6 and 2, respectively, we should merge the two subtrees accordingly. After we add the parent node of the current roots of the two subtrees and add the corresponding edges, the entire summation tree is generated. To generalize and formulate the algorithm, we present Algorithm 4, which combines the three stages. The GenerateTree part encapsulates the third stage. For each ùëôùëñ,ùëó, we find the existing roots of the trees containing the ùëñ-th andùëó-th leaf nodes, denoted by ùëñ‚Ä≤andùëó‚Ä≤. If they are identical, then theùëñ-th andùëó-th leaf nodes are already in the same subtree. Otherwise, we combine them. The FindRoot function can be implemented by the disjoint-set data structure, resulting in an amortized time complexity of ùëÇ(ùõº(ùëõ))[28], where ùõº(ùëõ)is the inverse Ackermann function. 3.4 Time complexity and correctness The time complexity of the input construction is Œò(ùëõ2). Letùë°(ùëõ)be the time complexity of sum, the computation of ùêøhas a time complexity of Œò(ùëõ2ùë°(ùëõ)). In the tree generation stage, the time complexity of sorting ùëõ(ùëõ‚àí1)/2elements is Œò(ùëõ2logùëõ2)=Œò(ùëõ2logùëõ). Therefore, the time complexity of tree generation stage is Œò(ùëõ2logùëõ+ùëõ2ùõº(ùëõ))=Œò(ùëõ2logùëõ). Thus, the overall time complexity of FPRev-basic is Œò(ùëõ2)+Œò(ùëõ2ùë°(ùëõ))+Œò(ùëõ2logùëõ)=Œò(ùëõ2ùë°(ùëõ)). The correctness of FPRev-basic is guaranteed by design and can be proved as follows. For a given function sumandùëõ, we useùëáto denote the real summation tree and define ùëá‚Ä≤=FPRevBasic(sum,ùëõ). Assumingùëá‚â†ùëá‚Ä≤, there must exist ùëñandùëósuch thatùëôùëñ,ùëó ùëá‚â†ùëôùëñ,ùëó ùëá‚Ä≤. Now we construct ùê¥ùëñ,ùëóand compute its sum in the order ùëáandùëá‚Ä≤respectively, resulting in ùë†andùë†‚Ä≤. Then,ùë†=ùëõ‚àíùëôùëñ,ùëó ùëá‚â†ùë†‚Ä≤=ùëõ‚àíùëôùëñ,ùëó ùëá‚Ä≤. However, since ùë†=sum(ùê¥ùëñ,ùëó), thenùëôùëñ,ùëó ùëá‚Ä≤=ùëõ‚àíùë†‚Ä≤‚â†ùëõ‚àíùë†=ùëõ‚àísum(ùê¥ùëñ,ùëó). This contradicts the statementùëôùëñ,ùëó‚Üêùëõ‚àísum(ùê¥ùëñ,ùëó)in Algorithm 4. Therefore, the assumption ùëá‚â†ùëá‚Ä≤is false, soùëá=ùëá‚Ä≤. 7

[P√°gina 8]
Algorithm 4 FPRev-basic. Require: Summation function sum and number of summands ùëõ Ensure: Summation tree ùëá function FPRevBasic (sum,ùëõ) ùêø‚Üê‚àÖ forùëñ‚Üê0toùëõ‚àí1do forùëó‚Üêùëñ+1toùëõ‚àí1do Constructùê¥ùëñ,ùëó ùëôùëñ,ùëó‚Üêùëõ‚àísum(ùê¥ùëñ,ùëó) ùêø‚Üêùêø‚à™{(ùëôùëñ,ùëó,ùëñ,ùëó)} function GenerateTree (ùêø) ùëá‚Üê‚àÖ for(ùëôùëñ,ùëó,ùëñ,ùëó)‚ààùêøin ascending order do ùëñ‚Ä≤‚Üêùëá.FindRoot(ùëñ) ùëó‚Ä≤‚Üêùëá.FindRoot(ùëó) ifùëñ‚Ä≤‚â†ùëó‚Ä≤then ùëò‚Üêùëñ‚Ä≤+ùëõ ‚ä≤assign a new label ùëá‚Üêùëá‚à™{(ùëñ‚Ä≤,ùëò),(ùëó‚Ä≤,ùëò)} returnùëá return GenerateTree (ùêø) 4 FPREV-ADVANCED This section demonstrates the advanced algorithm FPRev-advanced of our tool FPRev. We first modify FPRev-basic for optimized efficiency. Then, based on the optimized version, we add support for multi-term fused summation used by matrix accelerators like NVIDIA GPU‚Äôs Tensor Cores. 4.1 Optimization 4.1.1 Recursive process. Observing FPRev-basic, we note that Algorithm 4 requires ùëõ(ùëõ‚àí1)/2 elements of ùêøas input, and many values of ùëôùëñ,ùëóare the same. However, only ùëõ‚àí1new nodes and 2(ùëõ‚àí1)new edges are constructed. Since obtaining multiple ùëôùëñ,ùëóby calling sum constitutes the primary cost of the method, reducing redundancy in ùëôùëñ,ùëó(i.e., the cases ùëñ‚Ä≤=ùëó‚Ä≤in Algorithm 4) can significantly improve efficiency. To achieve this, we will calculate ùëôùëñ,ùëóon demand. Specifically, we do not calculate all ùëôùëñ,ùëóahead of the tree generation stage. Instead, we directly enter the tree generation stage, and calculate ùëôùëñ,ùëó when needed. Following the bottom-up idea, we still construct subtrees from leaf to root. First step. We use the set ùêº={0,1,...,ùëõ‚àí1}to denote the indexes of the leaf nodes. Let ùëñrepresent the leaf node with the smallest index in ùêº. The sibling node of ùëñis either a leaf node or an inner node. If it is a leaf node, there must be a unique ùëósuch thatùëôùëñ,ùëó=2. Otherwise, if it is an inner node, thenùëôùëñ,ùëó>2for allùëósuch thatùëó‚â†ùëñ. Therefore, to distinguish the two cases, we need to calculate ùëôùëñ,ùëófor all other leaf nodes, denoted by the set ùêøùëñ={ùëôùëñ,ùëó:ùëó‚ààùêº‚àí{ùëñ}}. We examine the minimum value inùêøùëñ, which is denoted by ùëô=min(ùêøùëñ). Ifùëôequals 2, letùëóbe the one that satisfies ùëôùëñ,ùëó=2. Then, theùëó-th leaf node is the sibling node of ùëñ, so we add a new node to the tree, and add two edges from the ùëñ-th andùëó-th leaf node to the new node. The size of the currently constructed subtree is 2now. Otherwise, if ùëôis larger than 2, the sibling node of ùëñmust be an inner node. The subtree rooted at this inner node must have ùëô‚àí1leaf nodes. Let ùêΩùëô={ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó=ùëô}. Then, the number of 8

[P√°gina 9]
members of ùêΩùëômust beùëô‚àí1, and the members of ùêΩùëômust be the leaf nodes of this subtree. This can be proved by contradiction. Now, constructing this subtree is a subproblem for the leaf nodes ùêΩùëô. Suppose that we have constructed this subtree by a recursive algorithm. We shall add a new node to the tree, and add edges from ùëñand the root node of this subtree to the new node. Now, the size of the currently constructed subtree is ùëô. Summarizing the two cases, we can treat both cases as the same recursion process: finding ùêΩùëô={ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó=ùëô}and solving the subproblem for ùêΩùëô. The first case (where |ùêΩùëô|=1) just leads to the stop condition of the recursion (where |ùêº|=1). Second step. Now we have constructed a subtree of size ùëô. Letùëübe the root of this subtree. Similarly, to find the sibling node of ùëü, we examine the minimum value in the rest of ùêøùëñ, which is denoted by ùëô‚Ä≤here. Then, we solve the subproblem for ùêΩùëô‚Ä≤={ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó=ùëô‚Ä≤}, and get a subtree with leaf nodesùêΩùëô‚Ä≤. The root of the subtree, whether a leaf node or an inner node, is the sibling node of ùëü. Therefore, we shall add a new node to the tree, and add edges from ùëüand the root node of the subtree to the new node. Now, the size of the currently constructed subtree is ùëô‚Ä≤. Repetition. We then repeat the above step, until all values in ùêøùëñare examined and the whole tree is constructed. We implement this method with a recursive algorithm, as shown in Algorithm 5. Algorithm 5 Optimized version of FPRev-basic. Require: Summation function sum and number of summands ùëõ Ensure: Summation tree ùëá function FPRevOptimized (sum,ùëõ) function BuildSubtree (ùêº) ùëñ‚Üêmin(ùêº) ùëá‚Üê‚àÖ if|ùêº|=1then ‚ä≤stop condition returnùëá ùêøùëñ‚Üê‚àÖ forùëó‚ààùêº‚àí{ùëñ}do ‚ä≤calculateùëôùëñ,ùëóon demand Constructùê¥ùëñ,ùëó ùëôùëñ,ùëó‚Üêùëõ‚àísum(ùê¥ùëñ,ùëó) ùêøùëñ‚Üêùêøùëñ‚à™{ùëôùëñ,ùëó} ùëü‚Üêùëñ ‚ä≤current root of the subtree forùëô‚ààùêøùëñin ascending order do ùêΩùëô‚Üê{ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó=ùëô} ùëá‚Ä≤‚ÜêBuildSubtree(ùêΩùëô) ùëá‚Üêùëá‚à™ùëá‚Ä≤ ùëá‚Üêùëá‚à™{(ùëü,ùëü+ùëõ),(GetRoot(ùëá‚Ä≤),ùëü+ùëõ)} ùëü‚Üêùëü+ùëõ returnùëá return BuildSubtree ({0,1,...,ùëõ‚àí1}) 4.1.2 Demonstration with example. Consider the summation tree in Figure 1. We construct the tree with Algorithm 5. First, the set of leaf nodes is ùêº={0,1,...,7}, where the smallest index is ùëñ=0. Then, the set ùêøùëñ={ùëôùëñ,ùëó:ùëó‚ààùêº‚àí{ùëñ}}={2,4,4,6,6,8,8}={2,4,6,8}is calculated. Examining the smallest value in ùêøùëñ, we haveùëô=2andùêΩùëô={ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó=ùëô}={1}. Therefore, 9

[P√°gina 10]
BuildSubtree ({1}) is called, reaching the stop condition. Then, the subtree with the 0th and 1st leaf nodes is constructed. The root of this subtree is denoted by ùëü. Next, examining the smallest value in the rest of ùêøùëñ, we haveùëô=4andùêΩùëô={ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó= ùëô}={2,3}. Therefore, BuildSubtree ({2,3}) is called, where we have ùêº={2,3},ùëñ=2, andùêøùëñ={2}, andBuildSubtree ({3}) is called there. BuildSubtree ({2,3}) returns the subtree with the 2nd and 3rd leaf nodes as its leaf nodes. We then take its root as the sibling node of ùëü, and construct the parent node of this root and ùëü. Then, the subtree with the 0th, 1st, 2nd, and 3rd leaf nodes is constructed. ùëüis updated by the root of this subtree. The next smallest value is ùëô=6. We haveùêΩùëô={4,5}. Similarly, BuildSubtree ({4,5}) is called, and it returns the subtree with the 4th and 5th leaf nodes. We merge its root with ùëü, and construct the subtree with the 0th, 1st, 2nd, 3rd, 4th, and 5th leaf nodes. ùëüis updated by the root of this subtree. Finally,ùëô=8andùêΩùëô={6,7}.BuildSubtree ({6,7}) is called, and it returns the subtree with the 6th and 7th leaf nodes. We merge its root with ùëü. Then the whole tree is constructed. 4.1.3 Time complexity. The time complexity of Algorithm 5 is ùëÇ(ùëõ2ùë°(ùëõ))andŒ©(ùëõùë°(ùëõ)). The worst- case scenario occurs when adding ùëõsummands in the right-to-left order. In this case, BuildSubtree will be invoked with all suffixes of {0,1,...,ùëõ‚àí1}, andùëôùëñ,ùëófor all 0‚â§ùëñ<ùëó<ùëõwill be calculated. The worst-case time complexity is Œò(ùëõ2ùë°(ùëõ)). In practice, this order is cache-unfriendly, and thus no library uses it. The best-case scenario corresponds to the sequential summation, where the summation tree will be constructed in one pass, and only ùëô0,ùëófor all 0<ùëó<ùëõwill be calculated. The best-case time complexity is Œò(ùëõùë°(ùëõ)). In practice, many libraries use similar orders, because these orders are cache-friendly and efficient. 4.2 Extension 4.2.1 Multi-term fused summation. Matrix accelerators like NVIDIA Tensor Cores are specialized hardware components in recent GPUs. Matrix accelerators enable assembly instructions that take a matrixùê¥=(ùëéùëñùëó)ùëÄ√óùêæ, a matrixùêµ=(ùëèùëñùëó)ùêæ√óùëÅ, and a matrix ùê∂=(ùëêùëñùëó)ùëÄ√óùëÅas input, and produce a matrixùê∑=(ùëëùëñùëó)ùëÄ√óùëÅsuch thatùê∑=ùê¥√óùêµ+ùê∂. The data types of ùê¥andùêµare identical. The data types ofùê∂andùê∑are also identical, and their precision is no lower than the precision of ùê¥andùêµ. The numerical behavior of matrix accelerators has not been disclosed. Specifically, the compu- tation ofùëëùëñùëó=ùëêùëñùëó+√çùêæ‚àí1 ùëò=0ùëéùëñùëòùëèùëòùëóis executed in an undocumented way. Prior work has proposed different guesses, such as chain of fused multiply‚Äìadd operations (FMAs), pairwise summation, or multi-term fused summation [ 10,14,20,26]. By conducting numerical experiments, we have reproduced the findings of [ 10,20], which suggest that for low-precision input (specifically, when the precision of ùê¥andùêµis lower than float32), the computation of ùëëùëñùëó=ùëêùëñùëó+√çùêæ‚àí1 ùëò=0ùëéùëñùëòùëèùëòùëóis executed in multi-term fused summation: ‚Ä¢The products are computed exactly, and the results are maintained in full precision without rounding after the multiplication. ‚Ä¢The summation of the ùêæ+1summands is performed in a fixed-point manner. Speficically, the significands are aligned to the largest exponent of the summands, and then truncated to 24+ bits (i.e., no less than the precision of float32). The number of bits and the truncation method depend on the GPU architecture. ‚Ä¢Then, the sum is converted to the floating-point number. The normalization method depends on the GPU architecture and the output data type of the instruction. For higher-precision input, the computation is executed in a chain of standard FMAs. Our proposed methods, i.e., FPRev-basic and FPRev-optimized, can work for standard FMAs. However, 10

[P√°gina 11]
multi-term fused summation requires a new method, because it is executed in a non-standard, IEEE-754-incompliant way. Specifically, as shown in Algorithm 6, the ùêæ+1summands ( ùë•0=ùëê, and ùë•ùëñ+1=ùëéùëñùëèùëñfor0‚â§ùëñ<ùêæ) are summed in a fixed-point manner, thus making the result independent of the summation order. To represent this operation in the summation tree, we should use a node withùêæ+1children instead of a node with two children. Therefore, the summation tree should be an(ùêæ+1)-way tree. Algorithm 6 Multi-term fused summation Require: Sequenceùëã=(ùë•0,ùë•1,...,ùë•ùêæ) Ensure: Floating-point sum of ùëã function FusedSum (ùëã) ifthere exists NaN or infinity in ùëãthen return NaN or infinity according to IEEE-754 for all 0‚â§ùëñ‚â§ùêædo ‚ä≤can be parallel ùë†ùëñ‚ÜêSignificand(ùë•ùëñ) ‚ä≤1‚â§|ùë†ùëñ|<2 ùëíùëñ‚ÜêExponent(ùë•ùëñ) ‚ä≤ùë†ùëñ√ó2ùëíùëñ=ùë•ùëñ ùê∏‚Üêmax{ùëíùëñ} for all 0‚â§ùëñ‚â§ùêædo ‚ä≤can be parallel ùëßùëñ‚ÜêRShift(ùë†ùëñ,ùê∏‚àíùëíùëñ) ‚ä≤alignment and truncation ùëÜ‚Üê√çùêæ ùëñ=0ùëßùëñ ‚ä≤fixed-point summation return fl(ùëÜ√ó2ùê∏) ‚ä≤conversion 4.2.2 Constructing the multiway tree. To accommodate the multiway tree, we revisit our approach in Section 3. The first two stages keep working because we find that the key equation ùëôùëñ,ùëó= ùëõLCA(ùëñ,ùëó) leaves=ùëõ‚àísum(ùê¥ùëñ,ùëó)remains valid in multi-term fused summation. Thus, the values of ùëôùëñ,ùëócan be obtained in the same way, and we only need to redesign the tree construction algorithm in the third stage. Then, we revisit the tree construction algorithm in Algorithm 5. In BuildSubtree (ùêº), we calculate ùëôùëñ,ùëófor a fixedùëñand allùëó‚ààùêº‚àí{ùëñ}, enumerate them in ascending order, and maintain ùëüas the root of the largest constructed subtree containing the ùëñ-th leaf node. For some ùëô‚ààùêøùëñ={ùëôùëñ,ùëó:ùëó‚ààùêº‚àí{ùëñ}} andùêΩùëô={ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó=ùëô}, the return value of BuildSubtree (ùêΩùëô) is the subtree with ùêΩùëôas its leaf nodes. The root of this subtree must be the sibling node of ùëü, so we can add a new node as their parent node and update ùëü. However, this relation is not always true for the multiway tree. In addition to being sibling node, the root of the subtree may also be the parent node of ùëüin the multiway tree. For example, suppose a 5-way tree with leaf nodes ùêº={0,1,2,3,4}as the children of the root. Then, when ùëü=0,ùëô=5, andùêΩùëô={1,2,3,4}, solving the subproblem for ùêΩùëôshould return a partial subtree with ùêΩùëôas its leaves. The root node of the subtree is the parent node of ùëü. To distinguish the two cases, we observe the return value of BuildSubtree (ùêΩùëô), denoted by ùëá‚Ä≤, and the complete subtree rooted at the root of ùëá‚Ä≤, denoted by ùëáùëê. In the first case, the root of ùëá‚Ä≤ should be the sibling of ùëü, andùëá‚Ä≤=ùëáùëê. In the second case, the root of ùëá‚Ä≤should be the parent of ùëü, andùëá‚Ä≤‚äÇùëáùëê. Therefore, we can compare the size of ùëá‚Ä≤(denoted by ùëõùëá‚Ä≤ leaves) with the size of ùëáùëê (denoted by ùëõùëáùëê leaves). We note that ùëõùëá‚Ä≤ leaves=|ùêΩùëô|, andùëõùëáùëê leaves=max{ùëôùëó,ùëò:ùëó,ùëò‚ààùêΩùëô}=max(ùêømin(ùêΩùëô)). Therefore, if max(ùêømin(ùêΩùëô))=|ùêΩùëô|, then the root of ùëá‚Ä≤should be the sibling of ùëü, so we should add a new node as their parent node and update ùëüwith the index of this new node. Otherwise, max(ùêømin(ùêΩùëô))>|ùêΩùëô|, so the root of ùëá‚Ä≤should be the parent of ùëü, and thus we should add an edge fromùëüto the root of ùëá‚Ä≤, and update ùëüwith the root. 11

[P√°gina 12]
Through this modification, the multiway tree can be correctly constructed. We elaborate on the above process in Algorithm 7. We call it FPRev-advanced. FPRev-advanced has the same time complexity as Algorithm 5 (note that Algorithm 5 just corresponds to the case where max(ùêømin(ùêΩùëô))= |ùêΩùëô|) and supports multi-term fused summation. Algorithm 7 FPRev-advanced Require: Summation function sum and number of summands ùëõ Ensure: Summation tree ùëá function FPRevAdvanced (sum,ùëõ) function BuildSubtreeAdvanced (ùêº) ùëñ‚Üêmin(ùêº) ùëá‚Üê‚àÖ if|ùêº|=1then ‚ä≤stop condition returnùëá ùêøùëñ‚Üê‚àÖ forùëó‚ààùêº‚àí{ùëñ}do ‚ä≤calculateùëôùëñ,ùëóon demand Constructùê¥ùëñ,ùëó ùëôùëñ,ùëó‚Üêùëõ‚àísum(ùê¥ùëñ,ùëó) ùêøùëñ‚Üêùêøùëñ‚à™{ùëôùëñ,ùëó} ùëü‚Üêùëñ ‚ä≤current root of the subtree forùëô‚ààùêøùëñin ascending order do ùêΩùëô‚Üê{ùëó:ùëó‚ààùêº‚àí{ùëñ}‚àßùëôùëñ,ùëó=ùëô} (ùëá‚Ä≤,ùëõùëáùëê leaves)‚ÜêBuildSubtreeAdvanced (ùêΩ) ùëá‚Üêùëá‚à™ùëá‚Ä≤ if|ùêΩùëô|=ùëõùëáùëê leavesthen ‚ä≤ùëá‚Ä≤=ùëáùëê ùëá‚Üêùëá‚à™{(ùëü,ùëü+ùëõ),(GetRoot(ùëá‚Ä≤),ùëü+ùëõ)} ùëü‚Üêùëü+ùëõ else ‚ä≤ùëá‚Ä≤‚äÇùëáùëê ùëá‚Üêùëá‚à™{(ùëü,GetRoot(ùëá‚Ä≤))} ùëü‚ÜêGetRoot(ùëá‚Ä≤) return(ùëá,max(ùêøùëñ)) (ùëá,ùëõùëáùëê leaves)‚ÜêBuildSubtreeAdvanced ({0,1,...,ùëõ‚àí1}) returnùëá 4.3 Time complexity and correctness The time complexity of FPRev-advanced is ùëÇ(ùëõ2ùë°(ùëõ))andŒ©(ùëõùë°(ùëõ)), following the same analysis in Section 4.1.3. The correctness of it is also guaranteed by design and can be proved following the same process in Section 3.4. 5 EVALUATION 5.1 Experiment design In this section, we evaluate the efficiency of FPRev. Specifically, we aim to answer the following research questions: ‚Ä¢RQ1: How efficient is FPRev in revealing the order of summation for summation functions? ‚Ä¢RQ2: How efficient is FPRev in revealing the order of summation for different summation- based functions? 12

[P√°gina 13]
‚Ä¢RQ3: How efficient is FPRev on different CPUs and GPUs? We implement the naive brute-force algorithm (Algorithm 3), FPRev-basic (Algorithm 4), and FPRev-advanced (Algorithm 7) with Python (version 3.11). We refer to these algorithms as ‚Äúnaive‚Äù, ‚Äúbasic‚Äù, and ‚Äúadvanced‚Äù. For RQ1, we measure the run time (wall-clock time) of the three algorithms for summation functions in three libraries: NumPy (version 1.26) [ 12], PyTorch (version 2.3) [ 25], and JAX (version 0.4) [3]. For RQ2, we omit the naive algorithm, which has been proven to be too inefficient in the experiments for RQ1. We measure the run time of FPRev-basic and FPRev-advanced for the dot product, matrix‚Äìvector multiplication, and matrix‚Äìmatrix multiplication functions in three libraries. The time complexity of these functions is ùëÇ(ùëõ),ùëÇ(ùëõ2), andùëÇ(ùëõ3), respectively. For RQ3, we measure the run time of FPRev-basic and FPRev-advanced for matrix multiplication functions in PyTorch2on different devices. Specifically, we conduct these experiments on three platforms (denoted by A, B, and C) with different CPUs and GPUs: ‚Ä¢A-CPU: Intel Xeon E5-2690 v4 (24 v-cores, 24 threads) ‚Ä¢A-GPU: NVIDIA V100 (80 SMs, 5120 CUDA cores) ‚Ä¢B-CPU: AMD EPYC 7V13 (24 v-cores, 24 threads) ‚Ä¢B-GPU: NVIDIA A100 (108 SMs, 6912 CUDA cores) ‚Ä¢C-CPU: Intel Xeon Platinum 8480C (96 v-cores, 96 threads) ‚Ä¢C-GPU: NVIDIA H100 (132 SMs, 16896 CUDA cores) 5.2 RQ1: How efficient is FPRev in revealing the order of summation for summation functions? The experiments for RQ1 are conducted on A-CPU using the float32 data type. We start with the number of summands ùëõ=4, and increase ùëõuntil the run time exceeds one second. The results are shown in Figure 2. Each experiment represents a specific combination of revelation algorithm, library, and number of summands ùëõ. Each experiment is carried out 10 times, and the arithmetic mean of the 10 results is reported in Figure 2. The orange curves show that the run time of the naive algorithm grows exponentially as ùëõgrows. The results substantiate the ùëÇ(4ùëõ/ùëõ3/2¬∑ùë°(ùëõ))time complexity of the naive algorithm. The green and blue lines show that the run time of FPRev-basic and that of FPRev-advanced grows polynomially. The results also show that the run time of FPRev-basic is longer than that of FPRev-advanced, and grows faster as ùëõincreases. This is because the time complexity of FPRev-basic is Œò(ùëõ2ùë°(ùëõ)), while the time complexity of FPRev-advanced is Œ©(ùëõùë°(ùëõ))andùëÇ(ùëõ2ùë°(ùëõ)). These trends suggest that the scalability of FPRev-basic is much better than that of the naive algorithm, and the scalability of FPRev-advanced is even better. For example, if ùëõ=16, the naive algorithm can take more than 24 hours to produce an output, but FPRev-basic and FPRev-advanced only take less than 0.01 seconds. If ùëõ=8192 , FPRev-basic will take more than 100 seconds to produce an output, but FPRev-advanced only takes about one second. 5.3 RQ2: How efficient is FPRev in revealing the order of summation for different summation-based functions? The experiments for RQ2 are conducted on A-CPU using the float32 data type. Similarly, we start withùëõ=4, and increase ùëõuntil the run time exceeds one second. Each experiment is carried out 10 times, and the arithmetic mean of the 10 results is reported in Table 3. (The results for the 2NumPy does not support GPU. The backend library of both PyTorch and JAX is the same (i.e., cuBLAS) on NVIDIA GPUs. 13

[P√°gina 14]
0.00010.00100.01000.10001.000010.0000 4 16 64 256 1024 4096 16384Run time (s) Number of summandsNumpySum naive basic advanced 0.00010.00100.01000.10001.000010.0000 4 16 64 256 1024 4096 16384Run time (s) Number of summandsT orchSum naive basic advanced 0.00010.00100.01000.10001.000010.0000 4 16 64 256 1024 4096Run time (s) Number of summandsJaxSum naive basic advancedFig. 2. Run time for revealing the order of summation in NumPy, PyTorch, and JAX with the naive, basic, and advanced methods. The vertical axis is run time in seconds. The horizontal axis is the number of summands ùëõ. matrix-matrix multiplication functions are omitted because of space limit, and will be available online.) Empty cells represent that the run time significantly exceeds one second. The results show that the run time of FPRev-basic is longer than the run time of FPRev-advanced, and grows faster as ùëõincreases. In addition, as the time complexity of the workload increases, the growth speed of the runtime with regard to ùëõaccelerates. Therefore, the speedup of FPRev-advanced versus FPRev-basic is more pronounced as the workload is more complex. For example, for the dot product functions in the three libraries, the geometric mean of the speedups of FPRev-advanced versus FPRev-basic is 2.62√óforùëõ=16, and 15.38√óforùëõ=128. For the matrix‚Äìvector multiplication functions, the geometric mean of speedups is 3.42√óforùëõ=16, and35.67√óforùëõ=128. For the matrix‚Äìmatrix multiplication functions, the geometric mean of speedups is 4.12√óforùëõ=16, and 53.70√óforùëõ=128. 5.4 RQ3: How efficient is FPRev on different CPUs and GPUs? The experiments for RQ3 are conducted on A-CPU, A-GPU, B-CPU, B-GPU, C-CPU, and C-GPU using the float32 data type. Similarly, we start with ùëõ=4, and increase ùëõuntil the run time exceeds one second. Each experiment is carried out 10 times, and the arithmetic mean of the 10 results is reported in Table 4. The results show consistent improvements in the run time of FPRev-advanced versus FPRev-basic. Therefore, FPRev-advanced is more efficient than FPRev-basic on different CPUs and GPUs. 6 REVEALED SUMMATION ORDERS In this section, we aim to answer the following research questions by showcasing some orders of summation revealed by FPRev. ‚Ä¢RQ4: Are different orders of summation observed across different libraries? 14

[P√°gina 15]
Table 3. Run time in seconds for revealing the order of operations in the dot product and matrix-vector multiplication functions in NumPy, PyTorch and JAX using FPRev-basic and FPRev-advanced. Empty cells represent timeout. NumpyDot TorchDot JaxDot NumpyGEMV TorchGEMV JaxGEMV ùëõ basic advanced basic advanced basic advanced basic advanced basic advanced basic advanced 4 0.00014 0.00012 0.00035 0.00030 0.00034 0.00024 0.00015 0.00013 0.00043 0.00039 0.00122 0.00067 8 0.00026 0.00020 0.00088 0.00065 0.00104 0.00044 0.00029 0.00023 0.00117 0.00069 0.00555 0.00149 16 0.00068 0.00037 0.00290 0.00138 0.00368 0.00080 0.00077 0.00043 0.00406 0.00135 0.02245 0.00303 32 0.00224 0.00100 0.01079 0.00304 0.01523 0.00154 0.00239 0.00087 0.01556 0.00243 0.08805 0.00610 64 0.00763 0.00202 0.04254 0.00662 0.05791 0.00307 0.00873 0.00176 0.06280 0.00459 0.36301 0.01233 128 0.03630 0.00401 0.15306 0.01450 0.22875 0.00601 0.16700 0.00750 0.34565 0.01004 1.53696 0.02595 256 0.10465 0.00807 0.61788 0.02977 0.92213 0.01210 0.71664 0.02215 1.70408 0.02072 0.05123 512 0.42698 0.01546 2.51626 0.06142 3.78066 0.02445 5.02031 0.05548 0.05648 0.11312 1024 1.84475 0.03224 0.12324 0.04925 0.23766 0.27945 0.32024 2048 0.06431 0.25404 0.10267 0.67724 0.31079 1.46030 4096 0.13521 0.52728 0.21915 19.27085 4.18472 8192 0.28186 1.38307 0.49702 16384 0.69205 1.24560 32768 1.87398 Table 4. Run time in seconds for revealing the order of operations in the matrix-matrix multiplication function in PyTorch with FPRev-basic and FPRev-advanced on different devices. Empty cells represent timeout. A.CPU A.GPU B.CPU B.GPU C.CPU C.GPU ùëõ basic advanced basic advanced basic advanced basic advanced basic advanced basic advanced 4 0.00045 0.00040 0.00114 0.00078 0.00022 0.00019 0.00065 0.00046 0.00016 0.00017 0.00065 0.00041 8 0.00125 0.00079 0.00431 0.00209 0.00062 0.00036 0.00223 0.00121 0.00044 0.00031 0.00164 0.00098 16 0.00466 0.00159 0.01638 0.00542 0.00212 0.00071 0.00895 0.00195 0.00172 0.00060 0.00636 0.00228 32 0.01810 0.00343 0.06556 0.00643 0.01461 0.00179 0.03238 0.00584 0.00679 0.00124 0.02240 0.00700 64 0.11473 0.00810 0.25203 0.02205 0.05977 0.00357 0.12834 0.01196 0.27901 0.00867 0.08552 0.01500 128 0.73907 0.01467 0.99224 0.04196 0.28069 0.00772 0.52889 0.02241 0.90428 0.01775 0.32772 0.02881 256 3.51068 0.03431 4.11688 0.08044 1.84963 0.02857 2.21939 0.05613 4.96277 0.17905 1.50927 0.05948 512 0.35281 0.18761 0.19230 0.10890 0.14732 0.12560 1024 3.59237 0.63623 2.52039 0.39472 1.92971 0.31785 2048 5.85436 2.21231 0.93580 4096 6.93602 15

[P√°gina 16]
‚Ä¢RQ5: Are different orders of summation observed on different devices? 6.1 RQ4: Are different orders of summation observed across different libraries? We run FPRev-advanced for the sumation functions in NumPy, PyTorch, and JAX on A-CPU using the float32 data type. We examine the revealed summation trees for ùëõfrom 4 to 2048. The results show that on NumPy uses multi-lane strided summation and reduce them with pairwise summation. PyTorch uses multi-level strided summation and reduce them with sequential summation. JAX uses multi-lane strided summation and reduce them with pairwise summation, but the number of lanes and the reduction order are different from NumPy. For example, Figure 3 shows the summation trees of the libraries for ùëõ=64. 0 +8 +16 +24 +32 +40 +48 +56 +1 +9 +17 +25 +33 +41 +49 +57 +2 +10 +18 +26 +34 +42 +50 +58 +3 +11 +19 +27 +35 +43 +51 +59 +4 +12 +20 +28 +36 +44 +52 +60 +5 +13 +21 +29 +37 +45 +53 +61 +6 +14 +22 +30 +38 +46 +54 +62 +7 +15 +23 +31 +39 +47 +55 +63 (a) NumPy 0 +32 +8 +40 +16 +48 +24 +56 +1 +33 +9 +41 +17 +49 +25 +57 +2 +34 +10 +42 +18 +50 +26 +58 +3 +35 +11 +43 +19 +51 +27 +59 +4 +36 +12 +44 +20 +52 +28 +60 +5 +37 +13 +45 +21 +53 +29 +61 +6 +38 +14 +46 +22 +54 +30 +62 +7 +39 +15 +47 +23 +55 +31 +63 (b) PyTorch 0 +8 +16 +24 +4 +12 +20 +28 +2 +10 +18 +26 +6 +14 +22 +30 +1 +9 +17 +25 +5 +13 +21 +29 +3 +11 +19 +27 +7 +15 +23 +31 +32 +40 +48 +56 +36 +44 +52 +60 +34 +42 +50 +58 +38 +46 +54 +62 +33 +41 +49 +57 +37 +45 +53 +61 +35 +43 +51 +59 +39 +47 +55 +63 (c) JAX Fig. 3. The summation trees of different libraries for summation of 64 numbers. 6.2 RQ5: Are different orders of summation observed on different devices? We run FPRev-advanced for the matrix multiplication function in PyTorch on A-GPU, B-GPU, and C-GPU, using the float16 data type to enable Tensor Core computation. We examine the revealed summation trees for ùëõfrom 16 to 2048. The results show that on A-GPU (NVIDIA V100), the summation tree is a 5-way tree; on B-GPU (NVIDIA A100), the summation tree is a 9-way tree; on C-GPU (NVIDIA H100), the summation tree is a 17-way tree. For example, Figure 4 shows the summation tree for ùëõ=32on these devices. Our results corroborate the conclusion in [ 10,20], which states that the Tensor Cores on NVIDIA Volta, Ampere, and Hopper architectures use (4+1), (8+1), and (16+1)-term fused summation re- spectively. 16

[P√°gina 17]
0 +1 2 3 +4 5 6 7 +8 9 10 11 +12 13 14 15 +16 17 18 19 +20 21 22 23 +24 25 26 27 +28 29 30 31(a) NVIDIA V100 0 +1 2 3 4 5 6 7 +8 9 10 11 12 13 14 15 +16 17 18 19 20 21 22 23 +24 25 26 27 28 29 30 31 (b) NVIDIA A100 0 +1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 +16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 (c) NVIDIA H100 Fig. 4. The summation trees for 32x32x32 matrix multiplication on Tensor Cores 7 DISCUSSION 7.1 Threats to validity 7.1.1 Internal. The precision of the floating-point data type can limit the validity of our tool. For float32, whose precision is 24 bits, the maximum number of summands ( ùëõ) that our tool supports is 224+1=16777217 . For larger numbers, the sum of ùëõ‚àí2ones cannot be represented precisely by float32, so the sum of masked all-one arrays may be incorrect. This problem could be solved by replacing the multiple ones corresponding to a constructed subtree in the masked all-one arrays with a single one and multiple zeros. This solution does not affect efficiency. For low-precision data types, there is an additional issue on the floating-point swamping because ùëÄ‚â™ùëõmay not hold. For example, the maximum exponent of float16 is 15. If ùëÄ=215andùúá=256, ¬±ùëÄ+ùúá‚â†¬±ùëÄ. To solve this problem, we can replace the ones of the masked all-one arrays by smaller numbers (e.g., 2‚àí24), and scale the sum back to an integer between 0 and ùëõ‚àí2when calculatingùëôùëñ,ùëó. This solution does not affect efficiency, either. 7.1.2 External. If the tested function has functional defects, our tool may not yield a correct result. Nonetheless, we can still identify some causes of non-reproducibility from the output of our tool. For example, our tool is effective in detecting the decrease in precision. If there is a conversion from float64 to float32 during summation, our tool will detect overflows, indicating the presence of the conversion. For non-reproducibility caused by randomization, it is easy to tell whether the function is randomized or deterministic by repeated testing. For non-reproducibility caused by switching instruction fusion, our tool can still yield the correct result, as it supports fused multiply-add instructions and fused multi-term addition instructions. 7.2 Extensibility The scope of our tool has been detailed in Section 2. In addition, our tool has been extended to support Algorithm 6 (the fused multi-term summation algorithm). As a result, our tool supports summation-based functions in most popular numerical libraries. Our tool also works for collective communication primitives if their order of summation is predetermined. To further extend our tool 17

[P√°gina 18]
for other functions based on special summation algorithms, the algorithms must have the property ùëõLCA(ùëñ,ùëó) leaves=ùëõ‚àísum(ùê¥ùëñ,ùëó). Thus, our methods can be abstracted as a template to reveal the order of summation. 8 RELATED WORK 8.1 Debugging of non-reproducibility The debugging of floating-point non-reproducibility is a significant challenge. Diverse factors, including hardware, compilers, algorithms, etc., can influence reproducibility [ 1]. They may intro- duce mathematical transformations that are semantically equivalent in real numbers but differ in floating-point arithmetic. To identify the root causes of numerical non-reproducibility, we need to figure out how the calculation is transformed. Previous work has used differential testing to identify the non-reproducible parts of a program [ 11,19,23]. However, this is insufficient to identify the root cause of non-reproducibility; people still need to scrutinize the trace or the code manually. 8.2 Floating-point summation Summation functions underpin high-level operations in numerical computing, such as dot products, vector norms, convolutions, matrix multiplications, and stencils. Many numerical computing applications frequently invoke these functions, so ensuring their numerical reproducibility is crucial for the overall reproducibility of the applications. Previous work [ 27,30] has identified the primary cause of non-reproducibility of the summation- based functions: varying orders of summation. However, the previous work cannot provide detailed information about how the order of summation is transformed, making it difficult to examine and reproduce existing functions. There are special summation algorithms that are not based on pure floating-point addition, such as Kahan‚Äôs summation algorithm [ 18]. Notably, order-independent summation algorithms have been proposed [6, 8, 9]. These algorithms ensure that the result of summation remains consistent regardless of the order. However, they are inefficient and are therefore not widely used. 8.3 Revelation of numerical behaviors Previous work has employed testing-based approaches to detect numerical errors for software [ 5,31] or analyze numerical behaviors for hardware [ 10,14,20]. The authors run numerical experiments with well-designed input, and analyze the numerical behaviors of the tested software or hardware based on the output. However, to our knowledge, we are the first to employ testing-based approaches to reveal the order of floating-point summation. 9 CONCLUSION In this paper, we introduce FPRev, a testing-based tool for revealing the order of floating-point summation. We demonstrate its efficiency through experiments that cover various workloads and devices, and show the different orders of summation for common numerical libraries. Our source code and test cases are available online, encouraging further investigation and improvement by the research community. REFERENCES [1]Andrea Arteaga, Oliver Fuhrer, and Torsten Hoefler. Designing Bit-Reproducible Portable High-Performance Ap- plications. In IEEE International Parallel and Distributed Processing Symposium (IPDPS) , pages 1235‚Äì1244, 2014. doi:10.1109/IPDPS.2014.127 . 18

[P√°gina 19]
[2]David H Bailey, Jonathan M Borwein, and Victoria Stodden. Facilitating reproducibility in scientific computing: Principles and practice. In Reproducibility: Principles, Problems, Practices, and Prospects , pages 205‚Äì231. Wiley Online Library, 2016. Publisher: Wiley Online Library. [3]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL: http://github.com/google/jax. [4]Boyuan Chen, Mingzhi Wen, Yong Shi, Dayi Lin, Gopi Krishnan Rajbahadur, and Zhen Ming Jiang. Towards Training Reproducible Deep Learning Models. In International Conference on Software Engineering (ICSE) , pages 2202‚Äì2214, 2022. doi:10.1145/3510003.3510163 . [5]Wei-Fan Chiang, Ganesh Gopalakrishnan, Zvonimir Rakamaric, and Alexey Solovyev. Efficient search for inputs causing high floating-point errors. In ACM Symposium on Principles and Practice of Parallel Programming (PPoPP) , pages 43‚Äì52, 2014. doi:10.1145/2555243.2555265 . [6]Sylvain Collange, David Defour, Stef Graillat, and Roman Iakymchuk. Numerical Reproducibility for the Parallel Reduction on Multi- and Many-Core Architectures. Parallel Computing , 49:83‚Äì97, 2015. doi:10.1016/j.parco.2015. 09.001 . [7] OpenBLAS Contributors. Openblas: An optimized blas library. URL: https://www.openblas.net/. [8]James Demmel and Hong Diep Nguyen. Fast Reproducible Floating-Point Summation. In IEEE Symposium on Computer Arithmetic (ARITH) , pages 163‚Äì172, 2013. doi:10.1109/ARITH.2013.9 . [9]James Demmel and Hong Diep Nguyen. Parallel Reproducible Summation. IEEE Transactions on Computers , 64(7):2060‚Äì 2070, 2015. doi:10.1109/TC.2014.2345391 . [10] Massimiliano Fasi, Nicholas J. Higham, Mantas Mikaitis, and Srikara Pranesh. Numerical behavior of NVIDIA tensor cores. PeerJ Computer Science , 7:e330, 2021. doi:10.7717/peerj-cs.330 . [11] Hui Guo, Ignacio Laguna, and Cindy Rubio-Gonz√°lez. pLiner: isolating lines of floating-point code for compiler-induced variability. In International Conference for High Performance Computing, Networking, Storage and Analysis (SC) , page 49, 2020. doi:10.1109/SC41405.2020.00053 . [12] Charles R. Harris, K. Jarrod Millman, St√©fan van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern√°ndez del R√≠o, Mark Wiebe, Pearu Peterson, Pierre G√©rard- Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature , 585:357‚Äì362, 2020. URL: https://doi.org/10.1038/s41586-020-2649-2, doi:10.1038/S41586-020-2649-2 . [13] Yun He and Chris H. Q. Ding. Using Accurate Arithmetics to Improve Numerical Reproducibility and Stability in Parallel Applications. The Journal of Supercomputing , 18(3):259‚Äì277, 2001. doi:10.1023/A:1008153532043 . [14] Brian J. Hickmann and Dennis Bradford. Experimental Analysis of Matrix Multiplication Functional Units. In IEEE Symposium on Computer Arithmetic (ARITH) , pages 116‚Äì119, 2019. doi:10.1109/ARITH.2019.00031 . [15] Nicholas J. Higham. The Accuracy of Floating Point Summation. SIAM Journal on Scientific Computing , 14(4):783‚Äì799, 1993. doi:10.1137/0914050 . [16] IEEE. IEEE Standard for Floating-Point Arithmetic , 2019. doi:10.1109/IEEESTD.2019.8766229 . [17] Intel Corporation. Intel Math Kernel Library . URL: https://www.intel.com/content/www/us/en/developer/tools/oneapi/ onemkl.html. [18] William Kahan. Further remarks on reducing truncation errors. Communications of the ACM , 8(1):40, 1965. doi: 10.1145/363707.363723 . [19] Ignacio Laguna. Varity: Quantifying Floating-Point Variations in HPC Systems Through Randomized Testing. In IEEE International Parallel and Distributed Processing Symposium (IPDPS) , pages 622‚Äì633, 2020. doi:10.1109/IPDPS47924. 2020.00070 . [20] Xinyi Li, Ang Li, Bo Fang, Katarzyna Swirydowicz, Ignacio Laguna, and Ganesh Gopalakrishnan. FTTN: Feature- Targeted Testing for Numerical Properties of NVIDIA & AMD Matrix Accelerators, 2024. arXiv: 2403.00232. URL: https://doi.org/10.48550/arXiv.2403.00232, doi:10.48550/ARXIV.2403.00232 . [21] Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John C. Grundy, and Xiaohu Yang. On the Reproducibility and Replicability of Deep Learning in Software Engineering. ACM Transactions on Software Engineering and Methodology , 31(1):15:1‚Äì15:46, 2022. doi:10.1145/3477535 . [22] Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, and Jeffrey S. Vetter. NVIDIA Tensor Core Programmability, Performance & Precision. In IEEE International Parallel and Distributed Processing Symposium (IPDPS) Workshops , pages 522‚Äì531. IEEE Computer Society, 2018. doi:10.1109/IPDPSW.2018.00091 . [23] Dolores Miao, Ignacio Laguna, and Cindy Rubio-Gonz√°lez. Expression Isolation of Compiler-Induced Numerical Inconsistencies in Heterogeneous Code. In ISC High Performance , pages 381‚Äì401, 2023. URL: https://doi.org/10.1007/978- 3-031-32041-5_20, doi:10.1007/978-3-031-32041-5_20 . 19

[P√°gina 20]
[24] NVIDIA Corporation. cuBLAS: Basic Linear Algebra on NVIDIA GPUs . URL: https://developer.nvidia.com/cublas/. [25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K√∂pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Impera- tive Style, High-Performance Deep Learning Library. In Conference on Neural Information Processing Systems (NeurIPS) , pages 8024‚Äì8035, 2019. URL: https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740- Abstract.html. [26] Md Aamir Raihan, Negar Goli, and Tor M. Aamodt. Modeling Deep Learning Accelerator Enabled GPUs. In IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS) , pages 79‚Äì92, 2019. doi: 10.1109/ISPASS.2019.00016 . [27] Robert W. Robey, Jonathan M. Robey, and Rob Aulwes. In search of numerical consistency in parallel programming. Parallel Computing , 37(4-5):217‚Äì229, 2011. URL: https://doi.org/10.1016/j.parco.2011.02.009, doi:10.1016/J.PARCO. 2011.02.009 . [28] Robert Endre Tarjan and Jan van Leeuwen. Worst-case Analysis of Set Union Algorithms. Journal of the ACM , 31(2):245‚Äì281, 1984. doi:10.1145/62.2160 . [29] Michela Taufer, Omar Padron, Philip Saponaro, and Sandeep Patel. Improving numerical reproducibility and stability in large-scale numerical simulations on GPUs. In IEEE International Parallel and Distributed Processing Symposium (IPDPS) , pages 1‚Äì9, 2010. doi:10.1109/IPDPS.2010.5470481 . [30] Oreste Villa, Daniel Chavarria-Miranda, Vidhya Gurumoorthi, Andr√©s M√°rquez, and Sriram Krishnamoorthy. Effects of floating-point non-associativity on numerical computations on massively multithreaded systems. In Cray User Group Meeting (CUG) , volume 3, 2009. [31] Daming Zou, Ran Wang, Yingfei Xiong, Lu Zhang, Zhendong Su, and Hong Mei. A Genetic Algorithm for Detecting Significant Floating-Point Inaccuracies. In International Conference on Software Engineering (ICSE) , pages 529‚Äì539, 2015. doi:10.1109/ICSE.2015.70 . 20